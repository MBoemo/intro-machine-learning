<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>5 Nearest neighbours | An Introduction to Machine Learning</title>
  <meta name="description" content="Course materials for An Introduction to Machine Learning" />
  <meta name="generator" content="bookdown 0.31 and GitBook 2.6.7" />

  <meta property="og:title" content="5 Nearest neighbours | An Introduction to Machine Learning" />
  <meta property="og:type" content="book" />
  <meta property="og:image" content="/figures/cover_image.png" />
  <meta property="og:description" content="Course materials for An Introduction to Machine Learning" />
  <meta name="github-repo" content="bioinformatics-training/intro-machine-learning-2019" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="5 Nearest neighbours | An Introduction to Machine Learning" />
  
  <meta name="twitter:description" content="Course materials for An Introduction to Machine Learning" />
  <meta name="twitter:image" content="/figures/cover_image.png" />

<meta name="author" content="Sudhakaran Prabakaran, Matt Wayland and Chris Penfold" />


<meta name="date" content="2023-01-15" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="clustering.html"/>
<link rel="next" href="svm.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> About the course</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#overview"><i class="fa fa-check"></i><b>1.1</b> Overview</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#registration"><i class="fa fa-check"></i><b>1.2</b> Registration</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#prerequisites"><i class="fa fa-check"></i><b>1.3</b> Prerequisites</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#github"><i class="fa fa-check"></i><b>1.4</b> Github</a></li>
<li class="chapter" data-level="1.5" data-path="index.html"><a href="index.html#license"><i class="fa fa-check"></i><b>1.5</b> License</a></li>
<li class="chapter" data-level="1.6" data-path="index.html"><a href="index.html#contact"><i class="fa fa-check"></i><b>1.6</b> Contact</a></li>
<li class="chapter" data-level="1.7" data-path="index.html"><a href="index.html#colophon"><i class="fa fa-check"></i><b>1.7</b> Colophon</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>2</b> Introduction</a><ul>
<li class="chapter" data-level="2.1" data-path="intro.html"><a href="intro.html#what-is-machine-learning"><i class="fa fa-check"></i><b>2.1</b> What is machine learning?</a></li>
<li class="chapter" data-level="2.2" data-path="intro.html"><a href="intro.html#aspects-of-ml"><i class="fa fa-check"></i><b>2.2</b> Aspects of ML</a></li>
<li class="chapter" data-level="2.3" data-path="intro.html"><a href="intro.html#what-actually-happened-under-the-hood"><i class="fa fa-check"></i><b>2.3</b> What actually happened under the hood</a></li>
<li class="chapter" data-level="2.4" data-path="intro.html"><a href="intro.html#introduction-to-caret"><i class="fa fa-check"></i><b>2.4</b> Introduction to CARET</a><ul>
<li class="chapter" data-level="2.4.1" data-path="intro.html"><a href="intro.html#preprocessing-with-the-iris-dataset"><i class="fa fa-check"></i><b>2.4.1</b> Preprocessing with the Iris dataset</a></li>
<li class="chapter" data-level="2.4.2" data-path="intro.html"><a href="intro.html#training-different-types-of-models"><i class="fa fa-check"></i><b>2.4.2</b> Training different types of models</a></li>
<li class="chapter" data-level="2.4.3" data-path="intro.html"><a href="intro.html#cross-validation"><i class="fa fa-check"></i><b>2.4.3</b> Cross-validation</a></li>
<li class="chapter" data-level="2.4.4" data-path="intro.html"><a href="intro.html#optimising-hyperparameters"><i class="fa fa-check"></i><b>2.4.4</b> Optimising hyperparameters</a></li>
<li class="chapter" data-level="2.4.5" data-path="intro.html"><a href="intro.html#using-dummy-variables-with-the-sacramento-dataset"><i class="fa fa-check"></i><b>2.4.5</b> Using dummy variables with the Sacramento dataset</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html"><i class="fa fa-check"></i><b>3</b> Dimensionality reduction</a><ul>
<li class="chapter" data-level="3.1" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html#linear-dimensionality-reduction"><i class="fa fa-check"></i><b>3.1</b> Linear Dimensionality Reduction</a><ul>
<li class="chapter" data-level="3.1.1" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html#interpreting-the-principle-component-axes"><i class="fa fa-check"></i><b>3.1.1</b> Interpreting the Principle Component Axes</a></li>
<li class="chapter" data-level="3.1.2" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html#horseshoe-effect"><i class="fa fa-check"></i><b>3.1.2</b> Horseshoe effect</a></li>
<li class="chapter" data-level="3.1.3" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html#pca-analysis-of-mammalian-development"><i class="fa fa-check"></i><b>3.1.3</b> PCA analysis of mammalian development</a></li>
<li class="chapter" data-level="3.1.4" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html#biological-interpretation"><i class="fa fa-check"></i><b>3.1.4</b> Biological interpretation</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html#nonlinear-dimensionality-reduction"><i class="fa fa-check"></i><b>3.2</b> Nonlinear Dimensionality Reduction</a><ul>
<li class="chapter" data-level="3.2.1" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html#nonlinear-warping"><i class="fa fa-check"></i><b>3.2.1</b> Nonlinear warping</a></li>
<li class="chapter" data-level="3.2.2" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html#stochasticity"><i class="fa fa-check"></i><b>3.2.2</b> Stochasticity</a></li>
<li class="chapter" data-level="3.2.3" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html#analysis-of-mammalian-development"><i class="fa fa-check"></i><b>3.2.3</b> Analysis of mammalian development</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html#other-dimensionality-reduction-techniques"><i class="fa fa-check"></i><b>3.3</b> Other dimensionality reduction techniques</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="clustering.html"><a href="clustering.html"><i class="fa fa-check"></i><b>4</b> Clustering</a><ul>
<li class="chapter" data-level="4.1" data-path="clustering.html"><a href="clustering.html#introduction"><i class="fa fa-check"></i><b>4.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2" data-path="clustering.html"><a href="clustering.html#distance-metrics"><i class="fa fa-check"></i><b>4.2</b> Distance metrics</a></li>
<li class="chapter" data-level="4.3" data-path="clustering.html"><a href="clustering.html#hierarchic-agglomerative"><i class="fa fa-check"></i><b>4.3</b> Hierarchic agglomerative</a><ul>
<li class="chapter" data-level="4.3.1" data-path="clustering.html"><a href="clustering.html#linkage-algorithms"><i class="fa fa-check"></i><b>4.3.1</b> Linkage algorithms</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="clustering.html"><a href="clustering.html#k-means"><i class="fa fa-check"></i><b>4.4</b> K-means</a><ul>
<li class="chapter" data-level="4.4.1" data-path="clustering.html"><a href="clustering.html#algorithm"><i class="fa fa-check"></i><b>4.4.1</b> Algorithm</a></li>
<li class="chapter" data-level="4.4.2" data-path="clustering.html"><a href="clustering.html#choosing-initial-cluster-centres"><i class="fa fa-check"></i><b>4.4.2</b> Choosing initial cluster centres</a></li>
<li class="chapter" data-level="4.4.3" data-path="clustering.html"><a href="clustering.html#choosingK"><i class="fa fa-check"></i><b>4.4.3</b> Choosing k</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="clustering.html"><a href="clustering.html#dbscan"><i class="fa fa-check"></i><b>4.5</b> DBSCAN</a><ul>
<li class="chapter" data-level="4.5.1" data-path="clustering.html"><a href="clustering.html#algorithm-1"><i class="fa fa-check"></i><b>4.5.1</b> Algorithm</a></li>
<li class="chapter" data-level="4.5.2" data-path="clustering.html"><a href="clustering.html#implementation-in-r"><i class="fa fa-check"></i><b>4.5.2</b> Implementation in R</a></li>
<li class="chapter" data-level="4.5.3" data-path="clustering.html"><a href="clustering.html#choosing-parameters"><i class="fa fa-check"></i><b>4.5.3</b> Choosing parameters</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="clustering.html"><a href="clustering.html#example-clustering-synthetic-data-sets"><i class="fa fa-check"></i><b>4.6</b> Example: clustering synthetic data sets</a><ul>
<li class="chapter" data-level="4.6.1" data-path="clustering.html"><a href="clustering.html#hierarchic-agglomerative-1"><i class="fa fa-check"></i><b>4.6.1</b> Hierarchic agglomerative</a></li>
<li class="chapter" data-level="4.6.2" data-path="clustering.html"><a href="clustering.html#k-means-1"><i class="fa fa-check"></i><b>4.6.2</b> K-means</a></li>
<li class="chapter" data-level="4.6.3" data-path="clustering.html"><a href="clustering.html#dbscan-1"><i class="fa fa-check"></i><b>4.6.3</b> DBSCAN</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="clustering.html"><a href="clustering.html#evaluating-cluster-quality"><i class="fa fa-check"></i><b>4.7</b> Evaluating cluster quality</a><ul>
<li class="chapter" data-level="4.7.1" data-path="clustering.html"><a href="clustering.html#silhouetteMethod"><i class="fa fa-check"></i><b>4.7.1</b> Silhouette method</a></li>
<li class="chapter" data-level="4.7.2" data-path="clustering.html"><a href="clustering.html#example---k-means-clustering-of-blobs-data-set"><i class="fa fa-check"></i><b>4.7.2</b> Example - k-means clustering of blobs data set</a></li>
<li class="chapter" data-level="4.7.3" data-path="clustering.html"><a href="clustering.html#example---dbscan-clustering-of-noisy-moons"><i class="fa fa-check"></i><b>4.7.3</b> Example - DBSCAN clustering of noisy moons</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="clustering.html"><a href="clustering.html#example-gene-expression-profiling-of-human-tissues"><i class="fa fa-check"></i><b>4.8</b> Example: gene expression profiling of human tissues</a><ul>
<li class="chapter" data-level="4.8.1" data-path="clustering.html"><a href="clustering.html#hierarchic-agglomerative-2"><i class="fa fa-check"></i><b>4.8.1</b> Hierarchic agglomerative</a></li>
<li class="chapter" data-level="4.8.2" data-path="clustering.html"><a href="clustering.html#k-means-2"><i class="fa fa-check"></i><b>4.8.2</b> K-means</a></li>
<li class="chapter" data-level="4.8.3" data-path="clustering.html"><a href="clustering.html#dbscan-2"><i class="fa fa-check"></i><b>4.8.3</b> DBSCAN</a></li>
</ul></li>
<li class="chapter" data-level="4.9" data-path="clustering.html"><a href="clustering.html#exercises"><i class="fa fa-check"></i><b>4.9</b> Exercises</a><ul>
<li class="chapter" data-level="4.9.1" data-path="clustering.html"><a href="clustering.html#clusteringEx1"><i class="fa fa-check"></i><b>4.9.1</b> Exercise 1</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html"><i class="fa fa-check"></i><b>5</b> Nearest neighbours</a><ul>
<li class="chapter" data-level="5.1" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#introduction-1"><i class="fa fa-check"></i><b>5.1</b> Introduction</a></li>
<li class="chapter" data-level="5.2" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#classification-simulated-data"><i class="fa fa-check"></i><b>5.2</b> Classification: simulated data</a><ul>
<li class="chapter" data-level="5.2.1" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#knn-function"><i class="fa fa-check"></i><b>5.2.1</b> knn function</a></li>
<li class="chapter" data-level="5.2.2" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#plotting-decision-boundaries"><i class="fa fa-check"></i><b>5.2.2</b> Plotting decision boundaries</a></li>
<li class="chapter" data-level="5.2.3" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#bias-variance-tradeoff"><i class="fa fa-check"></i><b>5.2.3</b> Bias-variance tradeoff</a></li>
<li class="chapter" data-level="5.2.4" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#choosing-k"><i class="fa fa-check"></i><b>5.2.4</b> Choosing <em>k</em></a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#example-on-the-iris-dataset"><i class="fa fa-check"></i><b>5.3</b> Example on the Iris dataset</a></li>
<li class="chapter" data-level="5.4" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#knn-cell-segmentation"><i class="fa fa-check"></i><b>5.4</b> Classification: cell segmentation</a><ul>
<li class="chapter" data-level="5.4.1" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#cell-segmentation-data-set"><i class="fa fa-check"></i><b>5.4.1</b> Cell segmentation data set</a></li>
<li class="chapter" data-level="5.4.2" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#data-splitting"><i class="fa fa-check"></i><b>5.4.2</b> Data splitting</a></li>
<li class="chapter" data-level="5.4.3" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#identification-of-data-quality-issues"><i class="fa fa-check"></i><b>5.4.3</b> Identification of data quality issues</a></li>
<li class="chapter" data-level="5.4.4" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#fit-model"><i class="fa fa-check"></i><b>5.4.4</b> Fit model</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#knn-regression"><i class="fa fa-check"></i><b>5.5</b> Regression</a><ul>
<li class="chapter" data-level="5.5.1" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#partition-data"><i class="fa fa-check"></i><b>5.5.1</b> Partition data</a></li>
<li class="chapter" data-level="5.5.2" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#data-pre-processing"><i class="fa fa-check"></i><b>5.5.2</b> Data pre-processing</a></li>
<li class="chapter" data-level="5.5.3" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#search-for-optimum-k"><i class="fa fa-check"></i><b>5.5.3</b> Search for optimum <em>k</em></a></li>
<li class="chapter" data-level="5.5.4" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#use-model-to-make-predictions"><i class="fa fa-check"></i><b>5.5.4</b> Use model to make predictions</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#exercises-1"><i class="fa fa-check"></i><b>5.6</b> Exercises</a><ul>
<li class="chapter" data-level="5.6.1" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#knnEx1"><i class="fa fa-check"></i><b>5.6.1</b> Exercise 1</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="svm.html"><a href="svm.html"><i class="fa fa-check"></i><b>6</b> Support vector machines</a><ul>
<li class="chapter" data-level="6.1" data-path="svm.html"><a href="svm.html#introduction-2"><i class="fa fa-check"></i><b>6.1</b> Introduction</a><ul>
<li class="chapter" data-level="6.1.1" data-path="svm.html"><a href="svm.html#maximum-margin-classifier"><i class="fa fa-check"></i><b>6.1.1</b> Maximum margin classifier</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="svm.html"><a href="svm.html#support-vector-classifier"><i class="fa fa-check"></i><b>6.2</b> Support vector classifier</a></li>
<li class="chapter" data-level="6.3" data-path="svm.html"><a href="svm.html#support-vector-machine"><i class="fa fa-check"></i><b>6.3</b> Support Vector Machine</a></li>
<li class="chapter" data-level="6.4" data-path="svm.html"><a href="svm.html#example---training-a-classifier"><i class="fa fa-check"></i><b>6.4</b> Example - training a classifier</a><ul>
<li class="chapter" data-level="6.4.1" data-path="svm.html"><a href="svm.html#setup-environment"><i class="fa fa-check"></i><b>6.4.1</b> Setup environment</a></li>
<li class="chapter" data-level="6.4.2" data-path="svm.html"><a href="svm.html#partition-data-1"><i class="fa fa-check"></i><b>6.4.2</b> Partition data</a></li>
<li class="chapter" data-level="6.4.3" data-path="svm.html"><a href="svm.html#visualize-training-data"><i class="fa fa-check"></i><b>6.4.3</b> Visualize training data</a></li>
<li class="chapter" data-level="6.4.4" data-path="svm.html"><a href="svm.html#prediction-performance-measures"><i class="fa fa-check"></i><b>6.4.4</b> Prediction performance measures</a></li>
<li class="chapter" data-level="6.4.5" data-path="svm.html"><a href="svm.html#plot-decision-boundary"><i class="fa fa-check"></i><b>6.4.5</b> Plot decision boundary</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="svm.html"><a href="svm.html#defining-your-own-model-type-to-use-in-caret"><i class="fa fa-check"></i><b>6.5</b> Defining your own model type to use in caret</a><ul>
<li class="chapter" data-level="6.5.1" data-path="svm.html"><a href="svm.html#model-cross-validation-and-tuning"><i class="fa fa-check"></i><b>6.5.1</b> Model cross-validation and tuning</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="svm.html"><a href="svm.html#iris-example"><i class="fa fa-check"></i><b>6.6</b> Iris example</a></li>
<li class="chapter" data-level="6.7" data-path="svm.html"><a href="svm.html#cell-segmentation-example"><i class="fa fa-check"></i><b>6.7</b> Cell segmentation example</a></li>
<li class="chapter" data-level="6.8" data-path="svm.html"><a href="svm.html#example---regression"><i class="fa fa-check"></i><b>6.8</b> Example - regression</a></li>
<li class="chapter" data-level="6.9" data-path="svm.html"><a href="svm.html#further-reading"><i class="fa fa-check"></i><b>6.9</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="decision-trees.html"><a href="decision-trees.html"><i class="fa fa-check"></i><b>7</b> Decision trees and random forests</a><ul>
<li class="chapter" data-level="7.1" data-path="decision-trees.html"><a href="decision-trees.html#decision-trees-1"><i class="fa fa-check"></i><b>7.1</b> Decision Trees</a></li>
<li class="chapter" data-level="7.2" data-path="decision-trees.html"><a href="decision-trees.html#example-code-with-categorical-data"><i class="fa fa-check"></i><b>7.2</b> Example code with categorical data</a><ul>
<li class="chapter" data-level="7.2.1" data-path="decision-trees.html"><a href="decision-trees.html#loading-packages-and-data"><i class="fa fa-check"></i><b>7.2.1</b> Loading packages and data</a></li>
<li class="chapter" data-level="7.2.2" data-path="decision-trees.html"><a href="decision-trees.html#data-slicing"><i class="fa fa-check"></i><b>7.2.2</b> Data Slicing</a></li>
<li class="chapter" data-level="7.2.3" data-path="decision-trees.html"><a href="decision-trees.html#data-preprocessing"><i class="fa fa-check"></i><b>7.2.3</b> Data Preprocessing</a></li>
<li class="chapter" data-level="7.2.4" data-path="decision-trees.html"><a href="decision-trees.html#training-decisions-trees"><i class="fa fa-check"></i><b>7.2.4</b> Training Decisions Trees</a></li>
<li class="chapter" data-level="7.2.5" data-path="decision-trees.html"><a href="decision-trees.html#plotting-the-decision-trees"><i class="fa fa-check"></i><b>7.2.5</b> Plotting the decision trees</a></li>
<li class="chapter" data-level="7.2.6" data-path="decision-trees.html"><a href="decision-trees.html#prediction"><i class="fa fa-check"></i><b>7.2.6</b> Prediction</a></li>
<li class="chapter" data-level="7.2.7" data-path="decision-trees.html"><a href="decision-trees.html#implementing-decision-trees-directly"><i class="fa fa-check"></i><b>7.2.7</b> Implementing Decision Trees directly</a></li>
<li class="chapter" data-level="7.2.8" data-path="decision-trees.html"><a href="decision-trees.html#iris-example-for-decision-trees"><i class="fa fa-check"></i><b>7.2.8</b> Iris example for Decision Trees</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="decision-trees.html"><a href="decision-trees.html#cell-segmentation-examples"><i class="fa fa-check"></i><b>7.3</b> Cell segmentation examples</a></li>
<li class="chapter" data-level="7.4" data-path="decision-trees.html"><a href="decision-trees.html#regression-example---blood-brain-barrier"><i class="fa fa-check"></i><b>7.4</b> Regression example - Blood Brain Barrier</a><ul>
<li class="chapter" data-level="7.4.1" data-path="decision-trees.html"><a href="decision-trees.html#decision-tree"><i class="fa fa-check"></i><b>7.4.1</b> Decision tree</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="decision-trees.html"><a href="decision-trees.html#random-forest"><i class="fa fa-check"></i><b>7.5</b> Random Forest</a><ul>
<li class="chapter" data-level="7.5.1" data-path="decision-trees.html"><a href="decision-trees.html#iris-example-for-random-forests"><i class="fa fa-check"></i><b>7.5.1</b> Iris example for Random Forests</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="decision-trees.html"><a href="decision-trees.html#cell-segmentation-examples-1"><i class="fa fa-check"></i><b>7.6</b> Cell segmentation examples</a></li>
<li class="chapter" data-level="7.7" data-path="decision-trees.html"><a href="decision-trees.html#regression-example---blood-brain-barrier-1"><i class="fa fa-check"></i><b>7.7</b> Regression example - Blood Brain Barrier</a><ul>
<li class="chapter" data-level="7.7.1" data-path="decision-trees.html"><a href="decision-trees.html#random-forest-1"><i class="fa fa-check"></i><b>7.7.1</b> Random forest</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="use-case-1.html"><a href="use-case-1.html"><i class="fa fa-check"></i><b>8</b> Use case 1</a><ul>
<li class="chapter" data-level="8.1" data-path="use-case-1.html"><a href="use-case-1.html#introduction-3"><i class="fa fa-check"></i><b>8.1</b> Introduction</a></li>
<li class="chapter" data-level="8.2" data-path="use-case-1.html"><a href="use-case-1.html#problem-automated-detection-of-malaria"><i class="fa fa-check"></i><b>8.2</b> Problem: automated detection of malaria</a></li>
<li class="chapter" data-level="8.3" data-path="use-case-1.html"><a href="use-case-1.html#challenges"><i class="fa fa-check"></i><b>8.3</b> Challenges</a></li>
<li class="chapter" data-level="8.4" data-path="use-case-1.html"><a href="use-case-1.html#getting-started"><i class="fa fa-check"></i><b>8.4</b> Getting started</a><ul>
<li class="chapter" data-level="8.4.1" data-path="use-case-1.html"><a href="use-case-1.html#load-data"><i class="fa fa-check"></i><b>8.4.1</b> Load data</a></li>
<li class="chapter" data-level="8.4.2" data-path="use-case-1.html"><a href="use-case-1.html#model-comparison"><i class="fa fa-check"></i><b>8.4.2</b> Model comparison</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="use-case-1.html"><a href="use-case-1.html#solutions"><i class="fa fa-check"></i><b>8.5</b> Solutions</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="linear-models.html"><a href="linear-models.html"><i class="fa fa-check"></i><b>9</b> Linear models and matrix algebra</a><ul>
<li class="chapter" data-level="9.1" data-path="linear-models.html"><a href="linear-models.html#linear-models-1"><i class="fa fa-check"></i><b>9.1</b> Linear models</a></li>
<li class="chapter" data-level="9.2" data-path="linear-models.html"><a href="linear-models.html#matrix-algebra"><i class="fa fa-check"></i><b>9.2</b> Matrix algebra</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="logistic-regression.html"><a href="logistic-regression.html"><i class="fa fa-check"></i><b>10</b> Linear regression and logistic regression</a><ul>
<li class="chapter" data-level="10.1" data-path="logistic-regression.html"><a href="logistic-regression.html#regression"><i class="fa fa-check"></i><b>10.1</b> Regression</a><ul>
<li class="chapter" data-level="10.1.1" data-path="logistic-regression.html"><a href="logistic-regression.html#linear-regression"><i class="fa fa-check"></i><b>10.1.1</b> Linear regression</a></li>
<li class="chapter" data-level="10.1.2" data-path="logistic-regression.html"><a href="logistic-regression.html#polynomial-regression"><i class="fa fa-check"></i><b>10.1.2</b> Polynomial regression</a></li>
<li class="chapter" data-level="10.1.3" data-path="logistic-regression.html"><a href="logistic-regression.html#distributions-of-fits"><i class="fa fa-check"></i><b>10.1.3</b> Distributions of fits</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="logistic-regression.html"><a href="logistic-regression.html#classification"><i class="fa fa-check"></i><b>10.2</b> Classification</a><ul>
<li class="chapter" data-level="10.2.1" data-path="logistic-regression.html"><a href="logistic-regression.html#logistic-regression"><i class="fa fa-check"></i><b>10.2.1</b> Logistic regression</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="logistic-regression.html"><a href="logistic-regression.html#resources"><i class="fa fa-check"></i><b>10.3</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="ann.html"><a href="ann.html"><i class="fa fa-check"></i><b>11</b> Artificial neural networks</a><ul>
<li class="chapter" data-level="11.1" data-path="ann.html"><a href="ann.html#neural-networks"><i class="fa fa-check"></i><b>11.1</b> Neural Networks</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="use-case-2.html"><a href="use-case-2.html"><i class="fa fa-check"></i><b>12</b> Use case 2</a></li>
<li class="chapter" data-level="13" data-path="mlnn.html"><a href="mlnn.html"><i class="fa fa-check"></i><b>13</b> Deep Learning</a><ul>
<li class="chapter" data-level="13.1" data-path="mlnn.html"><a href="mlnn.html#multilayer-neural-networks"><i class="fa fa-check"></i><b>13.1</b> Multilayer Neural Networks</a><ul>
<li class="chapter" data-level="13.1.1" data-path="mlnn.html"><a href="mlnn.html#reading-in-images"><i class="fa fa-check"></i><b>13.1.1</b> Reading in images</a></li>
<li class="chapter" data-level="13.1.2" data-path="mlnn.html"><a href="mlnn.html#constructing-layers-in-kerasr"><i class="fa fa-check"></i><b>13.1.2</b> Constructing layers in kerasR</a></li>
<li class="chapter" data-level="13.1.3" data-path="mlnn.html"><a href="mlnn.html#rick-and-morty-classifier-using-deep-learning"><i class="fa fa-check"></i><b>13.1.3</b> Rick and Morty classifier using Deep Learning</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="mlnn.html"><a href="mlnn.html#convolutional-neural-networks"><i class="fa fa-check"></i><b>13.2</b> Convolutional neural networks</a><ul>
<li class="chapter" data-level="13.2.1" data-path="mlnn.html"><a href="mlnn.html#checking-the-models"><i class="fa fa-check"></i><b>13.2.1</b> Checking the models</a></li>
<li class="chapter" data-level="13.2.2" data-path="mlnn.html"><a href="mlnn.html#asking-more-precise-questions"><i class="fa fa-check"></i><b>13.2.2</b> Asking more precise questions</a></li>
<li class="chapter" data-level="13.2.3" data-path="mlnn.html"><a href="mlnn.html#more-complex-networks"><i class="fa fa-check"></i><b>13.2.3</b> More complex networks</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="mlnn.html"><a href="mlnn.html#further-reading-1"><i class="fa fa-check"></i><b>13.3</b> Further reading</a></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="logistic-regression.html"><a href="logistic-regression.html#resources"><i class="fa fa-check"></i><b>A</b> Resources</a><ul>
<li class="chapter" data-level="A.1" data-path="resources.html"><a href="resources.html"><i class="fa fa-check"></i><b>A.1</b> Python</a></li>
<li class="chapter" data-level="A.2" data-path="resources.html"><a href="resources.html#machine-learning-data-set-repositories"><i class="fa fa-check"></i><b>A.2</b> Machine learning data set repositories</a><ul>
<li class="chapter" data-level="A.2.1" data-path="resources.html"><a href="resources.html#mldata"><i class="fa fa-check"></i><b>A.2.1</b> MLDATA</a></li>
<li class="chapter" data-level="A.2.2" data-path="resources.html"><a href="resources.html#uci-machine-learning-repository"><i class="fa fa-check"></i><b>A.2.2</b> UCI Machine Learning Repository</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="B" data-path="solutions-dimensionality-reduction.html"><a href="solutions-dimensionality-reduction.html"><i class="fa fa-check"></i><b>B</b> Solutions - Dimensionality reduction</a><ul>
<li class="chapter" data-level="B.1" data-path="solutions-dimensionality-reduction.html"><a href="solutions-dimensionality-reduction.html#exercise-1."><i class="fa fa-check"></i><b>B.1</b> Exercise 1.</a></li>
<li class="chapter" data-level="B.2" data-path="solutions-dimensionality-reduction.html"><a href="solutions-dimensionality-reduction.html#exercise-2."><i class="fa fa-check"></i><b>B.2</b> Exercise 2.</a></li>
</ul></li>
<li class="chapter" data-level="C" data-path="solutions-clustering.html"><a href="solutions-clustering.html"><i class="fa fa-check"></i><b>C</b> Solutions ch. 4 - Clustering</a><ul>
<li class="chapter" data-level="C.1" data-path="solutions-clustering.html"><a href="solutions-clustering.html#exercise-1"><i class="fa fa-check"></i><b>C.1</b> Exercise 1</a></li>
</ul></li>
<li class="chapter" data-level="D" data-path="solutions-nearest-neighbours.html"><a href="solutions-nearest-neighbours.html"><i class="fa fa-check"></i><b>D</b> Solutions ch. 7 - Nearest neighbours</a><ul>
<li class="chapter" data-level="D.1" data-path="solutions-nearest-neighbours.html"><a href="solutions-nearest-neighbours.html#exercise-1-1"><i class="fa fa-check"></i><b>D.1</b> Exercise 1</a></li>
</ul></li>
<li class="chapter" data-level="E" data-path="solutions-svm.html"><a href="solutions-svm.html"><i class="fa fa-check"></i><b>E</b> Solutions ch. 6 - Support vector machines</a><ul>
<li class="chapter" data-level="E.1" data-path="solutions-svm.html"><a href="solutions-svm.html#exercise-1-2"><i class="fa fa-check"></i><b>E.1</b> Exercise 1</a></li>
</ul></li>
<li class="chapter" data-level="F" data-path="solutions-decision-trees.html"><a href="solutions-decision-trees.html"><i class="fa fa-check"></i><b>F</b> Solutions ch. 9 - Decision trees and random forests</a><ul>
<li class="chapter" data-level="F.1" data-path="solutions-decision-trees.html"><a href="solutions-decision-trees.html#exercise-1-3"><i class="fa fa-check"></i><b>F.1</b> Exercise 1</a></li>
</ul></li>
<li class="chapter" data-level="G" data-path="use-case-1-solutions.html"><a href="use-case-1-solutions.html"><i class="fa fa-check"></i><b>G</b> Solutions chapter 8 - use case 1</a><ul>
<li class="chapter" data-level="G.1" data-path="use-case-1-solutions.html"><a href="use-case-1-solutions.html#preparation"><i class="fa fa-check"></i><b>G.1</b> Preparation</a><ul>
<li class="chapter" data-level="G.1.1" data-path="use-case-1-solutions.html"><a href="use-case-1-solutions.html#load-required-libraries"><i class="fa fa-check"></i><b>G.1.1</b> Load required libraries</a></li>
<li class="chapter" data-level="G.1.2" data-path="use-case-1-solutions.html"><a href="use-case-1-solutions.html#define-svm-model"><i class="fa fa-check"></i><b>G.1.2</b> Define SVM model</a></li>
<li class="chapter" data-level="G.1.3" data-path="use-case-1-solutions.html"><a href="use-case-1-solutions.html#setup-parallel-processing"><i class="fa fa-check"></i><b>G.1.3</b> Setup parallel processing</a></li>
<li class="chapter" data-level="G.1.4" data-path="use-case-1-solutions.html"><a href="use-case-1-solutions.html#load-data-1"><i class="fa fa-check"></i><b>G.1.4</b> Load data</a></li>
</ul></li>
<li class="chapter" data-level="G.2" data-path="use-case-1-solutions.html"><a href="use-case-1-solutions.html#assess-data-quality"><i class="fa fa-check"></i><b>G.2</b> Assess data quality</a><ul>
<li class="chapter" data-level="G.2.1" data-path="use-case-1-solutions.html"><a href="use-case-1-solutions.html#zero-and-near-zero-variance-predictors-1"><i class="fa fa-check"></i><b>G.2.1</b> Zero and near-zero variance predictors</a></li>
<li class="chapter" data-level="G.2.2" data-path="use-case-1-solutions.html"><a href="use-case-1-solutions.html#are-all-predictors-on-the-same-scale"><i class="fa fa-check"></i><b>G.2.2</b> Are all predictors on the same scale?</a></li>
<li class="chapter" data-level="G.2.3" data-path="use-case-1-solutions.html"><a href="use-case-1-solutions.html#redundancy-from-correlated-variables"><i class="fa fa-check"></i><b>G.2.3</b> Redundancy from correlated variables</a></li>
<li class="chapter" data-level="G.2.4" data-path="use-case-1-solutions.html"><a href="use-case-1-solutions.html#skewness-1"><i class="fa fa-check"></i><b>G.2.4</b> Skewness</a></li>
</ul></li>
<li class="chapter" data-level="G.3" data-path="use-case-1-solutions.html"><a href="use-case-1-solutions.html#infection-status-two-class-problem"><i class="fa fa-check"></i><b>G.3</b> Infection status (two-class problem)</a><ul>
<li class="chapter" data-level="G.3.1" data-path="use-case-1-solutions.html"><a href="use-case-1-solutions.html#model-training-and-parameter-tuning"><i class="fa fa-check"></i><b>G.3.1</b> Model training and parameter tuning</a></li>
<li class="chapter" data-level="G.3.2" data-path="use-case-1-solutions.html"><a href="use-case-1-solutions.html#knn"><i class="fa fa-check"></i><b>G.3.2</b> KNN</a></li>
<li class="chapter" data-level="G.3.3" data-path="use-case-1-solutions.html"><a href="use-case-1-solutions.html#svm-1"><i class="fa fa-check"></i><b>G.3.3</b> SVM</a></li>
<li class="chapter" data-level="G.3.4" data-path="use-case-1-solutions.html"><a href="use-case-1-solutions.html#decision-tree-1"><i class="fa fa-check"></i><b>G.3.4</b> Decision tree</a></li>
<li class="chapter" data-level="G.3.5" data-path="use-case-1-solutions.html"><a href="use-case-1-solutions.html#random-forest-2"><i class="fa fa-check"></i><b>G.3.5</b> Random forest</a></li>
<li class="chapter" data-level="G.3.6" data-path="use-case-1-solutions.html"><a href="use-case-1-solutions.html#compare-models"><i class="fa fa-check"></i><b>G.3.6</b> Compare models</a></li>
<li class="chapter" data-level="G.3.7" data-path="use-case-1-solutions.html"><a href="use-case-1-solutions.html#predict-test-set-using-our-best-model"><i class="fa fa-check"></i><b>G.3.7</b> Predict test set using our best model</a></li>
<li class="chapter" data-level="G.3.8" data-path="use-case-1-solutions.html"><a href="use-case-1-solutions.html#roc-curve"><i class="fa fa-check"></i><b>G.3.8</b> ROC curve</a></li>
</ul></li>
<li class="chapter" data-level="G.4" data-path="use-case-1-solutions.html"><a href="use-case-1-solutions.html#discrimination-of-infective-stages-multi-class-problem"><i class="fa fa-check"></i><b>G.4</b> Discrimination of infective stages (multi-class problem)</a><ul>
<li class="chapter" data-level="G.4.1" data-path="use-case-1-solutions.html"><a href="use-case-1-solutions.html#define-cross-validation-procedure"><i class="fa fa-check"></i><b>G.4.1</b> Define cross-validation procedure</a></li>
<li class="chapter" data-level="G.4.2" data-path="use-case-1-solutions.html"><a href="use-case-1-solutions.html#knn-1"><i class="fa fa-check"></i><b>G.4.2</b> KNN</a></li>
<li class="chapter" data-level="G.4.3" data-path="use-case-1-solutions.html"><a href="use-case-1-solutions.html#svm-2"><i class="fa fa-check"></i><b>G.4.3</b> SVM</a></li>
<li class="chapter" data-level="G.4.4" data-path="use-case-1-solutions.html"><a href="use-case-1-solutions.html#decision-tree-2"><i class="fa fa-check"></i><b>G.4.4</b> Decision tree</a></li>
<li class="chapter" data-level="G.4.5" data-path="use-case-1-solutions.html"><a href="use-case-1-solutions.html#random-forest-3"><i class="fa fa-check"></i><b>G.4.5</b> Random forest</a></li>
<li class="chapter" data-level="G.4.6" data-path="use-case-1-solutions.html"><a href="use-case-1-solutions.html#compare-models-1"><i class="fa fa-check"></i><b>G.4.6</b> Compare models</a></li>
<li class="chapter" data-level="G.4.7" data-path="use-case-1-solutions.html"><a href="use-case-1-solutions.html#predict-test-set-using-our-best-model-1"><i class="fa fa-check"></i><b>G.4.7</b> Predict test set using our best model</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="H" data-path="solutions-linear-models.html"><a href="solutions-linear-models.html"><i class="fa fa-check"></i><b>H</b> Solutions ch. 3 - Linear models and matrix algebra</a><ul>
<li class="chapter" data-level="H.1" data-path="solutions-linear-models.html"><a href="solutions-linear-models.html#example-2"><i class="fa fa-check"></i><b>H.1</b> Example 2</a></li>
<li class="chapter" data-level="H.2" data-path="solutions-linear-models.html"><a href="solutions-linear-models.html#example-2-1"><i class="fa fa-check"></i><b>H.2</b> Example 2</a></li>
</ul></li>
<li class="chapter" data-level="I" data-path="solutions-logistic-regression.html"><a href="solutions-logistic-regression.html"><i class="fa fa-check"></i><b>I</b> Solutions ch. 4 - Linear and non-linear (logistic) regression</a></li>
<li class="chapter" data-level="J" data-path="solutions-ann.html"><a href="solutions-ann.html"><i class="fa fa-check"></i><b>J</b> Solutions ch. 10 - Artificial neural networks</a><ul>
<li class="chapter" data-level="J.1" data-path="solutions-ann.html"><a href="solutions-ann.html#exercise-1-4"><i class="fa fa-check"></i><b>J.1</b> Exercise 1</a></li>
</ul></li>
<li class="chapter" data-level="K" data-path="use-case-2-solutions.html"><a href="use-case-2-solutions.html"><i class="fa fa-check"></i><b>K</b> Solutions for use case 2</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">An Introduction to Machine Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="nearest-neighbours" class="section level1 hasAnchor">
<h1><span class="header-section-number">5</span> Nearest neighbours<a href="nearest-neighbours.html#nearest-neighbours" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<!-- Matt -->
<!-- edited by Irina Mohorianu iim22@cam.ac.uk-->
<div id="introduction-1" class="section level2 hasAnchor">
<h2><span class="header-section-number">5.1</span> Introduction<a href="nearest-neighbours.html#introduction-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><em>k</em>-NN is by far the simplest method of supervised learning we will cover in this course. It is a non-parametric method that can be used for both classification (predicting class membership) and regression (estimating continuous variables). <em>k</em>-NN is categorized as instance based (memory based) learning, because all computation is deferred until classification. The most computationally demanding aspects of <em>k</em>-NN are finding neighbours and storing the entire learning set.</p>
<p>A simple <em>k</em>-NN classification rule (figure <a href="nearest-neighbours.html#fig:knnClassification">5.1</a>) would proceed as follows:</p>
<ol style="list-style-type: decimal">
<li>when presented with a new observation, find the <em>k</em> closest samples in the learning set</li>
<li>predict the class by majority vote</li>
</ol>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:knnClassification"></span>
<img src="images/knn_classification.svg" alt="Illustration of _k_-nn classification. In this example we have two classes: blue squares and red triangles. The green circle represents a test object. If k=3 (solid line circle) the test object is assigned to the red triangle class. If k=5 the test object is assigned to the blue square class.  By Antti Ajanki AnAj - Own work, CC BY-SA 3.0, https://commons.wikimedia.org/w/index.php?curid=2170282" width="75%" />
<p class="caption">
Figure 5.1: Illustration of <em>k</em>-nn classification. In this example we have two classes: blue squares and red triangles. The green circle represents a test object. If k=3 (solid line circle) the test object is assigned to the red triangle class. If k=5 the test object is assigned to the blue square class. By Antti Ajanki AnAj - Own work, CC BY-SA 3.0, <a href="https://commons.wikimedia.org/w/index.php?curid=2170282" class="uri">https://commons.wikimedia.org/w/index.php?curid=2170282</a>
</p>
</div>
<p>A basic implementation of <em>k</em>-NN regression would calculate a summary (e.g. a distance, a voting summary) of the numerical outcome of the <em>k</em> nearest neighbours.</p>
<p>The number of neighbours <em>k</em> has an impact on the predictive performance of <em>k</em>-NN in both classification and regression. The optimal value of <em>k</em> (<em>k</em> is considered a hyperparameter) should be chosen using cross-validation.</p>
<p><strong>How do we define and determine the similarity between observations?</strong>
We use distance (or dissimilarity) metrics to compute the pairwise differences between observations. The most common distances are the Euclidean and Manhattan metrics;</p>
<p>Euclidean distance measures the straight-line distance between two samples (i.e., how the crow flies); it is the most widely used distance metric in <em>k</em>-nn, and will be used in the examples and exercises in this chapter. Manhattan measures the point-to-point travel time (i.e., city block) and is commonly used for binary predictors (e.g., one-hot encoded 0/1 indicator variables).</p>
<p><strong>Euclidean distance:</strong>
<span class="math display">\[\begin{equation}
  distance\left(p,q\right)=\sqrt{\sum_{i=1}^{n} (p_i-q_i)^2}
\end{equation}\]</span></p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:euclideanDistanceDiagram"></span>
<img src="04-nearest-neighbours_files/figure-html/euclideanDistanceDiagram-1.png" alt="Euclidean distance." width="75%" />
<p class="caption">
Figure 4.2: Euclidean distance.
</p>
</div>
<p><strong>Manhattan distance:</strong>
<span class="math display">\[\begin{equation}
  distance\left(p,q\right)={\sum_{i=1}^{n} |p_i-q_i|}
\end{equation}\]</span></p>
<p>There are other metrics to measure the distance between observations. For example, the Minkowski distance is a generalization of the Euclidean and Manhattan distances and is defined as</p>
<p><strong>Minkowski distance:</strong>
<span class="math display">\[\begin{equation}
  distance\left(p,q\right)=\sqrt[p]{\sum_{i=1}^{n} (p_i-q_i)^p}

\end{equation}\]</span></p>
<p>where p&gt;0 (Han, Pei, and Kamber 2011). When p=2 the Minkowski distance is the Euclidean distance and when q=1 it is the Manhattan distance</p>
</div>
<div id="classification-simulated-data" class="section level2 hasAnchor">
<h2><span class="header-section-number">5.2</span> Classification: simulated data<a href="nearest-neighbours.html#classification-simulated-data" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>A simulated data set will be used to demonstrate:</p>
<ul>
<li>bias-variance trade-off</li>
<li>the knn function in R</li>
<li>plotting decision boundaries</li>
<li>choosing the optimum value of <em>k</em></li>
</ul>
<p>The dataset has been partitioned into training and test sets.</p>
<p>Load data</p>
<div class="sourceCode" id="cb232"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb232-1" data-line-number="1"><span class="kw">load</span>(<span class="st">&quot;data/example_binary_classification/bin_class_example.rda&quot;</span>)</a>
<a class="sourceLine" id="cb232-2" data-line-number="2"><span class="kw">str</span>(xtrain)</a></code></pre></div>
<pre><code>## &#39;data.frame&#39;:    400 obs. of  2 variables:
##  $ V1: num  -0.223 0.944 2.36 1.846 1.732 ...
##  $ V2: num  -1.153 -0.827 -0.128 2.014 -0.574 ...</code></pre>
<div class="sourceCode" id="cb234"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb234-1" data-line-number="1"><span class="kw">str</span>(xtest)</a></code></pre></div>
<pre><code>## &#39;data.frame&#39;:    400 obs. of  2 variables:
##  $ V1: num  2.09 2.3 2.07 1.65 1.18 ...
##  $ V2: num  -1.009 1.0947 0.1644 0.3243 -0.0277 ...</code></pre>
<div class="sourceCode" id="cb236"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb236-1" data-line-number="1"><span class="kw">summary</span>(<span class="kw">as.factor</span>(ytrain))</a></code></pre></div>
<pre><code>##   0   1 
## 200 200</code></pre>
<div class="sourceCode" id="cb238"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb238-1" data-line-number="1"><span class="kw">summary</span>(<span class="kw">as.factor</span>(ytest))</a></code></pre></div>
<pre><code>##   0   1 
## 200 200</code></pre>
<div class="sourceCode" id="cb240"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb240-1" data-line-number="1"><span class="kw">library</span>(ggplot2)</a>
<a class="sourceLine" id="cb240-2" data-line-number="2"><span class="kw">library</span>(GGally)</a>
<a class="sourceLine" id="cb240-3" data-line-number="3"><span class="kw">library</span>(RColorBrewer)</a>
<a class="sourceLine" id="cb240-4" data-line-number="4">point_shapes &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">15</span>,<span class="dv">17</span>)</a>
<a class="sourceLine" id="cb240-5" data-line-number="5">point_colours &lt;-<span class="st"> </span><span class="kw">brewer.pal</span>(<span class="dv">3</span>,<span class="st">&quot;Dark2&quot;</span>)</a>
<a class="sourceLine" id="cb240-6" data-line-number="6">point_size =<span class="st"> </span><span class="dv">2</span></a>
<a class="sourceLine" id="cb240-7" data-line-number="7"></a>
<a class="sourceLine" id="cb240-8" data-line-number="8"><span class="kw">ggplot</span>(xtrain, <span class="kw">aes</span>(V1,V2)) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb240-9" data-line-number="9"><span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">col=</span>point_colours[ytrain<span class="op">+</span><span class="dv">1</span>], <span class="dt">shape=</span>point_shapes[ytrain<span class="op">+</span><span class="dv">1</span>], </a>
<a class="sourceLine" id="cb240-10" data-line-number="10">             <span class="dt">size=</span>point_size) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb240-11" data-line-number="11"><span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;train&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb240-12" data-line-number="12"><span class="st">  </span><span class="kw">theme_bw</span>() <span class="op">+</span></a>
<a class="sourceLine" id="cb240-13" data-line-number="13"><span class="st">  </span><span class="kw">theme</span>(<span class="dt">plot.title =</span> <span class="kw">element_text</span>(<span class="dt">size=</span><span class="dv">25</span>, <span class="dt">face=</span><span class="st">&quot;bold&quot;</span>), <span class="dt">axis.text=</span><span class="kw">element_text</span>(<span class="dt">size=</span><span class="dv">15</span>),</a>
<a class="sourceLine" id="cb240-14" data-line-number="14">        <span class="dt">axis.title=</span><span class="kw">element_text</span>(<span class="dt">size=</span><span class="dv">20</span>,<span class="dt">face=</span><span class="st">&quot;bold&quot;</span>))</a>
<a class="sourceLine" id="cb240-15" data-line-number="15"></a>
<a class="sourceLine" id="cb240-16" data-line-number="16"><span class="kw">ggplot</span>(xtest, <span class="kw">aes</span>(V1,V2)) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb240-17" data-line-number="17"><span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">col=</span>point_colours[ytest<span class="op">+</span><span class="dv">1</span>], <span class="dt">shape=</span>point_shapes[ytest<span class="op">+</span><span class="dv">1</span>], </a>
<a class="sourceLine" id="cb240-18" data-line-number="18">             <span class="dt">size=</span>point_size) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb240-19" data-line-number="19"><span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;test&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb240-20" data-line-number="20"><span class="st">  </span><span class="kw">theme_bw</span>() <span class="op">+</span></a>
<a class="sourceLine" id="cb240-21" data-line-number="21"><span class="st">  </span><span class="kw">theme</span>(<span class="dt">plot.title =</span> <span class="kw">element_text</span>(<span class="dt">size=</span><span class="dv">25</span>, <span class="dt">face=</span><span class="st">&quot;bold&quot;</span>), <span class="dt">axis.text=</span><span class="kw">element_text</span>(<span class="dt">size=</span><span class="dv">15</span>),</a>
<a class="sourceLine" id="cb240-22" data-line-number="22">        <span class="dt">axis.title=</span><span class="kw">element_text</span>(<span class="dt">size=</span><span class="dv">20</span>,<span class="dt">face=</span><span class="st">&quot;bold&quot;</span>))</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:simDataBinClassTrainTest"></span>
<img src="04-nearest-neighbours_files/figure-html/simDataBinClassTrainTest-1.png" alt="Scatterplots of the simulated training and test data sets that will be used in the demonstration of binary classification using _k_-nn" width="50%" /><img src="04-nearest-neighbours_files/figure-html/simDataBinClassTrainTest-2.png" alt="Scatterplots of the simulated training and test data sets that will be used in the demonstration of binary classification using _k_-nn" width="50%" />
<p class="caption">
Figure 5.2: Scatterplots of the simulated training and test data sets that will be used in the demonstration of binary classification using <em>k</em>-nn
</p>
</div>
<div id="knn-function" class="section level3 hasAnchor">
<h3><span class="header-section-number">5.2.1</span> knn function<a href="nearest-neighbours.html#knn-function" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>For <em>k</em>-nn classification and regression we will use the <strong>knn</strong> function in the package <strong>class</strong>.</p>
<div class="sourceCode" id="cb241"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb241-1" data-line-number="1"><span class="kw">library</span>(class)</a></code></pre></div>
<p><strong>Arguments to knn</strong></p>
<ul>
<li><code>train</code> : matrix or data frame of training set cases.</li>
<li><code>test</code> : matrix or data frame of test set cases. A vector will be interpreted as a row vector for a single case.</li>
<li><code>cl</code> : factor of true classifications of training set</li>
<li><code>k</code> : number of neighbours considered.</li>
<li><code>l</code> : minimum vote for definite decision, otherwise doubt. (More precisely, less than k-l dissenting votes are allowed, even if k is increased by ties.)</li>
<li><code>prob</code> : If this is true, the proportion of the votes for the winning class are returned as attribute prob.</li>
<li><code>use.all</code> : controls handling of ties. If true, all distances equal to the kth largest are included. If false, a random selection of distances equal to the kth is chosen to use exactly k neighbours.</li>
</ul>
<p>Let us perform <em>k</em>-nn on the training set with <em>k</em>=1. We will use the <strong>confusionMatrix</strong> function from the <a href="http://cran.r-project.org/web/packages/caret/index.html">caret</a> package to summarize performance of the classifier.</p>
<div class="sourceCode" id="cb242"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb242-1" data-line-number="1"><span class="kw">library</span>(caret)</a>
<a class="sourceLine" id="cb242-2" data-line-number="2">knn1train &lt;-<span class="st"> </span>class<span class="op">::</span><span class="kw">knn</span>(<span class="dt">train=</span>xtrain, <span class="dt">test=</span>xtrain, <span class="dt">cl=</span>ytrain, <span class="dt">k=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb242-3" data-line-number="3"><span class="kw">confusionMatrix</span>(knn1train, <span class="kw">as.factor</span>(ytrain))</a></code></pre></div>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction   0   1
##          0 200   0
##          1   0 200
##                                      
##                Accuracy : 1          
##                  95% CI : (0.9908, 1)
##     No Information Rate : 0.5        
##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16  
##                                      
##                   Kappa : 1          
##                                      
##  Mcnemar&#39;s Test P-Value : NA         
##                                      
##             Sensitivity : 1.0        
##             Specificity : 1.0        
##          Pos Pred Value : 1.0        
##          Neg Pred Value : 1.0        
##              Prevalence : 0.5        
##          Detection Rate : 0.5        
##    Detection Prevalence : 0.5        
##       Balanced Accuracy : 1.0        
##                                      
##        &#39;Positive&#39; Class : 0          
## </code></pre>
<p>The classifier performs perfectly on the training set, because with <em>k</em>=1, each observation is being predicted by itself!
<!--
table(ytrain,knn1train)
cat("KNN prediction error for training set: ", 1-mean(as.numeric(as.vector(knn1train))==ytrain), "\n")
--></p>
<p>Now let use the training set to predict on the test set.</p>
<div class="sourceCode" id="cb244"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb244-1" data-line-number="1">knn1test &lt;-<span class="st"> </span>class<span class="op">::</span><span class="kw">knn</span>(<span class="dt">train=</span>xtrain, <span class="dt">test=</span>xtest, <span class="dt">cl=</span>ytrain, <span class="dt">k=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb244-2" data-line-number="2"><span class="kw">confusionMatrix</span>(knn1test, <span class="kw">as.factor</span>(ytest))</a></code></pre></div>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction   0   1
##          0 131  81
##          1  69 119
##                                           
##                Accuracy : 0.625           
##                  95% CI : (0.5755, 0.6726)
##     No Information Rate : 0.5             
##     P-Value [Acc &gt; NIR] : 3.266e-07       
##                                           
##                   Kappa : 0.25            
##                                           
##  Mcnemar&#39;s Test P-Value : 0.3691          
##                                           
##             Sensitivity : 0.6550          
##             Specificity : 0.5950          
##          Pos Pred Value : 0.6179          
##          Neg Pred Value : 0.6330          
##              Prevalence : 0.5000          
##          Detection Rate : 0.3275          
##    Detection Prevalence : 0.5300          
##       Balanced Accuracy : 0.6250          
##                                           
##        &#39;Positive&#39; Class : 0               
## </code></pre>
<p>Performance on the test set is not so good. This is an example of a classifier being over-fitted to the training set.
<!--
table(ytest, knn1test)
cat("KNN prediction error for test set: ", 1-mean(as.numeric(as.vector(knn1test))==ytest), "\n")
--></p>
</div>
<div id="plotting-decision-boundaries" class="section level3 hasAnchor">
<h3><span class="header-section-number">5.2.2</span> Plotting decision boundaries<a href="nearest-neighbours.html#plotting-decision-boundaries" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Since we have just two dimensions we can visualize the decision boundary generated by the <em>k</em>-nn classifier in a 2D scatterplot. Situations where your original data set contains only two variables will be rare, but it is not unusual to reduce a high-dimensional data set to just two dimensions using the methods that will be discussed in chapter <a href="dimensionality-reduction.html#dimensionality-reduction">3</a>. Therefore, knowing how to plot decision boundaries will potentially be helpful for many different datasets and classifiers.</p>
<p>Create a grid so we can predict across the full range of our variables V1 and V2.</p>
<div class="sourceCode" id="cb246"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb246-1" data-line-number="1">gridSize &lt;-<span class="st"> </span><span class="dv">150</span> </a>
<a class="sourceLine" id="cb246-2" data-line-number="2">v1limits &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="kw">min</span>(<span class="kw">c</span>(xtrain[,<span class="dv">1</span>],xtest[,<span class="dv">1</span>])),<span class="kw">max</span>(<span class="kw">c</span>(xtrain[,<span class="dv">1</span>],xtest[,<span class="dv">1</span>])))</a>
<a class="sourceLine" id="cb246-3" data-line-number="3">tmpV1 &lt;-<span class="st"> </span><span class="kw">seq</span>(v1limits[<span class="dv">1</span>],v1limits[<span class="dv">2</span>],<span class="dt">len=</span>gridSize)</a>
<a class="sourceLine" id="cb246-4" data-line-number="4">v2limits &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="kw">min</span>(<span class="kw">c</span>(xtrain[,<span class="dv">2</span>],xtest[,<span class="dv">2</span>])),<span class="kw">max</span>(<span class="kw">c</span>(xtrain[,<span class="dv">2</span>],xtest[,<span class="dv">2</span>])))</a>
<a class="sourceLine" id="cb246-5" data-line-number="5">tmpV2 &lt;-<span class="st"> </span><span class="kw">seq</span>(v2limits[<span class="dv">1</span>],v2limits[<span class="dv">2</span>],<span class="dt">len=</span>gridSize)</a>
<a class="sourceLine" id="cb246-6" data-line-number="6">xgrid &lt;-<span class="st"> </span><span class="kw">expand.grid</span>(tmpV1,tmpV2)</a>
<a class="sourceLine" id="cb246-7" data-line-number="7"><span class="kw">names</span>(xgrid) &lt;-<span class="st"> </span><span class="kw">names</span>(xtrain)</a></code></pre></div>
<p>Predict values of all elements of grid.</p>
<div class="sourceCode" id="cb247"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb247-1" data-line-number="1">knn1grid &lt;-<span class="st"> </span>class<span class="op">::</span><span class="kw">knn</span>(<span class="dt">train=</span>xtrain, <span class="dt">test=</span>xgrid, <span class="dt">cl=</span>ytrain, <span class="dt">k=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb247-2" data-line-number="2">V3 &lt;-<span class="st"> </span><span class="kw">as.numeric</span>(<span class="kw">as.vector</span>(knn1grid))</a>
<a class="sourceLine" id="cb247-3" data-line-number="3">xgrid &lt;-<span class="st"> </span><span class="kw">cbind</span>(xgrid, V3)</a></code></pre></div>
<p>Plot</p>
<div class="sourceCode" id="cb248"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb248-1" data-line-number="1">point_shapes &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">15</span>,<span class="dv">17</span>)</a>
<a class="sourceLine" id="cb248-2" data-line-number="2">point_colours &lt;-<span class="st"> </span><span class="kw">brewer.pal</span>(<span class="dv">3</span>,<span class="st">&quot;Dark2&quot;</span>)</a>
<a class="sourceLine" id="cb248-3" data-line-number="3">point_size =<span class="st"> </span><span class="dv">2</span></a>
<a class="sourceLine" id="cb248-4" data-line-number="4"></a>
<a class="sourceLine" id="cb248-5" data-line-number="5"><span class="kw">ggplot</span>(xgrid, <span class="kw">aes</span>(V1,V2)) <span class="op">+</span></a>
<a class="sourceLine" id="cb248-6" data-line-number="6"><span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">col=</span>point_colours[knn1grid], <span class="dt">shape=</span><span class="dv">16</span>, <span class="dt">size=</span><span class="fl">0.3</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb248-7" data-line-number="7"><span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">data=</span>xtrain, <span class="kw">aes</span>(V1,V2), <span class="dt">col=</span>point_colours[ytrain<span class="op">+</span><span class="dv">1</span>],</a>
<a class="sourceLine" id="cb248-8" data-line-number="8">             <span class="dt">shape=</span>point_shapes[ytrain<span class="op">+</span><span class="dv">1</span>], <span class="dt">size=</span>point_size) <span class="op">+</span></a>
<a class="sourceLine" id="cb248-9" data-line-number="9"><span class="st">  </span><span class="kw">geom_contour</span>(<span class="dt">data=</span>xgrid, <span class="kw">aes</span>(<span class="dt">x=</span>V1, <span class="dt">y=</span>V2, <span class="dt">z=</span>V3), <span class="dt">breaks=</span><span class="fl">0.5</span>, <span class="dt">col=</span><span class="st">&quot;grey30&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb248-10" data-line-number="10"><span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;train&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb248-11" data-line-number="11"><span class="st">  </span><span class="kw">theme_bw</span>() <span class="op">+</span></a>
<a class="sourceLine" id="cb248-12" data-line-number="12"><span class="st">  </span><span class="kw">theme</span>(<span class="dt">plot.title =</span> <span class="kw">element_text</span>(<span class="dt">size=</span><span class="dv">25</span>, <span class="dt">face=</span><span class="st">&quot;bold&quot;</span>), <span class="dt">axis.text=</span><span class="kw">element_text</span>(<span class="dt">size=</span><span class="dv">15</span>),</a>
<a class="sourceLine" id="cb248-13" data-line-number="13">        <span class="dt">axis.title=</span><span class="kw">element_text</span>(<span class="dt">size=</span><span class="dv">20</span>,<span class="dt">face=</span><span class="st">&quot;bold&quot;</span>))</a>
<a class="sourceLine" id="cb248-14" data-line-number="14"></a>
<a class="sourceLine" id="cb248-15" data-line-number="15"><span class="kw">ggplot</span>(xgrid, <span class="kw">aes</span>(V1,V2)) <span class="op">+</span></a>
<a class="sourceLine" id="cb248-16" data-line-number="16"><span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">col=</span>point_colours[knn1grid], <span class="dt">shape=</span><span class="dv">16</span>, <span class="dt">size=</span><span class="fl">0.3</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb248-17" data-line-number="17"><span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">data=</span>xtest, <span class="kw">aes</span>(V1,V2), <span class="dt">col=</span>point_colours[ytest<span class="op">+</span><span class="dv">1</span>],</a>
<a class="sourceLine" id="cb248-18" data-line-number="18">             <span class="dt">shape=</span>point_shapes[ytrain<span class="op">+</span><span class="dv">1</span>], <span class="dt">size=</span>point_size) <span class="op">+</span></a>
<a class="sourceLine" id="cb248-19" data-line-number="19"><span class="st">  </span><span class="kw">geom_contour</span>(<span class="dt">data=</span>xgrid, <span class="kw">aes</span>(<span class="dt">x=</span>V1, <span class="dt">y=</span>V2, <span class="dt">z=</span>V3), <span class="dt">breaks=</span><span class="fl">0.5</span>, <span class="dt">col=</span><span class="st">&quot;grey30&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb248-20" data-line-number="20"><span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;test&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb248-21" data-line-number="21"><span class="st">  </span><span class="kw">theme_bw</span>() <span class="op">+</span></a>
<a class="sourceLine" id="cb248-22" data-line-number="22"><span class="st">  </span><span class="kw">theme</span>(<span class="dt">plot.title =</span> <span class="kw">element_text</span>(<span class="dt">size=</span><span class="dv">25</span>, <span class="dt">face=</span><span class="st">&quot;bold&quot;</span>), <span class="dt">axis.text=</span><span class="kw">element_text</span>(<span class="dt">size=</span><span class="dv">15</span>),</a>
<a class="sourceLine" id="cb248-23" data-line-number="23">        <span class="dt">axis.title=</span><span class="kw">element_text</span>(<span class="dt">size=</span><span class="dv">20</span>,<span class="dt">face=</span><span class="st">&quot;bold&quot;</span>))</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:simDataBinClassDecisionBoundaryK1"></span>
<img src="04-nearest-neighbours_files/figure-html/simDataBinClassDecisionBoundaryK1-1.png" alt="Binary classification of the simulated training and test sets with _k_=1." width="50%" /><img src="04-nearest-neighbours_files/figure-html/simDataBinClassDecisionBoundaryK1-2.png" alt="Binary classification of the simulated training and test sets with _k_=1." width="50%" />
<p class="caption">
Figure 5.3: Binary classification of the simulated training and test sets with <em>k</em>=1.
</p>
</div>
</div>
<div id="bias-variance-tradeoff" class="section level3 hasAnchor">
<h3><span class="header-section-number">5.2.3</span> Bias-variance tradeoff<a href="nearest-neighbours.html#bias-variance-tradeoff" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The bias–variance tradeoff is the problem of simultaneously minimizing two sources of error that prevent supervised learning algorithms from generalizing beyond their training set:</p>
<ul>
<li>The bias is error from erroneous assumptions in the learning algorithm. High bias can cause an algorithm to miss the relevant relations between features and target outputs (underfitting).</li>
<li>The variance is error from sensitivity to small fluctuations in the training set. High variance can cause an algorithm to model the random noise in the training data, rather than the intended outputs (overfitting).</li>
</ul>
<p>To demonstrate this phenomenon, let us look at the performance of the <em>k</em>-nn classifier over a range of values of <em>k</em>. First we will define a function to create a sequence of log spaced values. This is the <strong>lseq</strong> function from the <a href="https://cran.r-project.org/package=emdbook">emdbook</a> package:</p>
<div class="sourceCode" id="cb249"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb249-1" data-line-number="1">lseq &lt;-<span class="st"> </span><span class="cf">function</span>(from, to, length.out) {</a>
<a class="sourceLine" id="cb249-2" data-line-number="2">  <span class="kw">exp</span>(<span class="kw">seq</span>(<span class="kw">log</span>(from), <span class="kw">log</span>(to), <span class="dt">length.out =</span> length.out))</a>
<a class="sourceLine" id="cb249-3" data-line-number="3">}</a></code></pre></div>
<p>Get log spaced sequence of length 20, round and then remove any duplicates resulting from rounding.</p>
<div class="sourceCode" id="cb250"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb250-1" data-line-number="1">s &lt;-<span class="st"> </span><span class="kw">unique</span>(<span class="kw">round</span>(<span class="kw">lseq</span>(<span class="dv">1</span>,<span class="dv">400</span>,<span class="dv">20</span>)))</a>
<a class="sourceLine" id="cb250-2" data-line-number="2"><span class="kw">length</span>(s)</a></code></pre></div>
<pre><code>## [1] 19</code></pre>
<div class="sourceCode" id="cb252"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb252-1" data-line-number="1">train_error &lt;-<span class="st"> </span><span class="kw">sapply</span>(s, <span class="cf">function</span>(i){</a>
<a class="sourceLine" id="cb252-2" data-line-number="2">  yhat &lt;-<span class="st"> </span><span class="kw">knn</span>(xtrain, xtrain, ytrain, i)</a>
<a class="sourceLine" id="cb252-3" data-line-number="3">  <span class="kw">return</span>(<span class="dv">1</span><span class="op">-</span><span class="kw">mean</span>(<span class="kw">as.numeric</span>(<span class="kw">as.vector</span>(yhat))<span class="op">==</span>ytrain))</a>
<a class="sourceLine" id="cb252-4" data-line-number="4">})</a>
<a class="sourceLine" id="cb252-5" data-line-number="5"></a>
<a class="sourceLine" id="cb252-6" data-line-number="6">test_error &lt;-<span class="st"> </span><span class="kw">sapply</span>(s, <span class="cf">function</span>(i){</a>
<a class="sourceLine" id="cb252-7" data-line-number="7">  yhat &lt;-<span class="st"> </span><span class="kw">knn</span>(xtrain, xtest, ytrain, i)</a>
<a class="sourceLine" id="cb252-8" data-line-number="8">  <span class="kw">return</span>(<span class="dv">1</span><span class="op">-</span><span class="kw">mean</span>(<span class="kw">as.numeric</span>(<span class="kw">as.vector</span>(yhat))<span class="op">==</span>ytest))</a>
<a class="sourceLine" id="cb252-9" data-line-number="9">})</a>
<a class="sourceLine" id="cb252-10" data-line-number="10"></a>
<a class="sourceLine" id="cb252-11" data-line-number="11">k &lt;-<span class="st"> </span><span class="kw">rep</span>(s, <span class="dv">2</span>)</a>
<a class="sourceLine" id="cb252-12" data-line-number="12">set &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="kw">rep</span>(<span class="st">&quot;train&quot;</span>, <span class="kw">length</span>(s)), <span class="kw">rep</span>(<span class="st">&quot;test&quot;</span>, <span class="kw">length</span>(s)))</a>
<a class="sourceLine" id="cb252-13" data-line-number="13">error &lt;-<span class="st"> </span><span class="kw">c</span>(train_error, test_error)</a>
<a class="sourceLine" id="cb252-14" data-line-number="14">misclass_errors &lt;-<span class="st"> </span><span class="kw">data.frame</span>(k, set, error)</a></code></pre></div>
<div class="sourceCode" id="cb253"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb253-1" data-line-number="1"><span class="kw">ggplot</span>(misclass_errors, <span class="kw">aes</span>(<span class="dt">x=</span>k, <span class="dt">y=</span>error, <span class="dt">group=</span>set)) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb253-2" data-line-number="2"><span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">colour=</span>set, <span class="dt">linetype=</span>set), <span class="dt">size=</span><span class="fl">1.5</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb253-3" data-line-number="3"><span class="st">  </span><span class="kw">scale_x_log10</span>() <span class="op">+</span></a>
<a class="sourceLine" id="cb253-4" data-line-number="4"><span class="st">  </span><span class="kw">ylab</span>(<span class="st">&quot;Misclassification Errors&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb253-5" data-line-number="5"><span class="st">  </span><span class="kw">theme_bw</span>() <span class="op">+</span></a>
<a class="sourceLine" id="cb253-6" data-line-number="6"><span class="st">  </span><span class="kw">theme</span>(<span class="dt">legend.position =</span> <span class="kw">c</span>(<span class="fl">0.5</span>, <span class="fl">0.25</span>), <span class="dt">legend.title=</span><span class="kw">element_blank</span>(),</a>
<a class="sourceLine" id="cb253-7" data-line-number="7">        <span class="dt">legend.text=</span><span class="kw">element_text</span>(<span class="dt">size=</span><span class="dv">12</span>), </a>
<a class="sourceLine" id="cb253-8" data-line-number="8">        <span class="dt">axis.title.x=</span><span class="kw">element_text</span>(<span class="dt">face=</span><span class="st">&quot;italic&quot;</span>, <span class="dt">size=</span><span class="dv">12</span>))</a></code></pre></div>
<pre><code>## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.
## ℹ Please use `linewidth` instead.</code></pre>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:misclassErrorsFunK"></span>
<img src="04-nearest-neighbours_files/figure-html/misclassErrorsFunK-1.png" alt="Misclassification errors as a function of neighbourhood size." width="100%" />
<p class="caption">
Figure 5.4: Misclassification errors as a function of neighbourhood size.
</p>
</div>
<p>We see excessive variance (overfitting) at low values of <em>k</em>, and bias (underfitting) at high values of <em>k</em>.</p>
</div>
<div id="choosing-k" class="section level3 hasAnchor">
<h3><span class="header-section-number">5.2.4</span> Choosing <em>k</em><a href="nearest-neighbours.html#choosing-k" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We will use the caret library. Caret provides a unified interface to a huge range of supervised learning packages in R. The design of its tools encourages best practice, especially in relation to cross-validation and testing. Additionally, it has automatic parallel processing built in, which is a significant advantage when dealing with large data sets.</p>
<div class="sourceCode" id="cb255"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb255-1" data-line-number="1"><span class="kw">library</span>(caret)</a></code></pre></div>
<p>To take advantage of Caret’s parallel processing functionality, we simply need to load the <a href="http://cran.r-project.org/web/packages/doMC/index.html">doMC</a> package and register workers:</p>
<div class="sourceCode" id="cb256"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb256-1" data-line-number="1"><span class="kw">library</span>(doMC)</a></code></pre></div>
<pre><code>## Loading required package: foreach</code></pre>
<pre><code>## Loading required package: iterators</code></pre>
<pre><code>## Loading required package: parallel</code></pre>
<div class="sourceCode" id="cb260"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb260-1" data-line-number="1"><span class="kw">registerDoMC</span>(<span class="kw">detectCores</span>())</a></code></pre></div>
<p>To find out how many cores we have registered we can use:</p>
<div class="sourceCode" id="cb261"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb261-1" data-line-number="1"><span class="kw">getDoParWorkers</span>()</a></code></pre></div>
<pre><code>## [1] 76</code></pre>
<p>The <a href="http://cran.r-project.org/web/packages/caret/index.html">caret</a> function <strong>train</strong> is used to fit predictive models over different values of <em>k</em>. The function <strong>trainControl</strong> is used to specify a list of computational and resampling options, which will be passed to <strong>train</strong>. We will start by configuring our cross-validation procedure using <strong>trainControl</strong>.</p>
<p>We would like to make this demonstration reproducible and because we will be running the models in parallel, using the <strong>set.seed</strong> function alone is not sufficient. In addition to using <strong>set.seed</strong> we have to make use of the optional <strong>seeds</strong> argument to <strong>trainControl</strong>. We need to supply <strong>seeds</strong> with a list of integers that will be used to set the seed at each sampling iteration. The list is required to have a length of B+1, where B is the number of resamples. We will be repeating 10-fold cross-validation a total of ten times and so our list must have a length of 101. The first B elements of the list are required to be vectors of integers of length M, where M is the number of models being evaluated (in this case 19). The last element of the list only needs to be a single integer, which will be used for the final model.</p>
<p>First we generate our list of seeds.</p>
<div class="sourceCode" id="cb263"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb263-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">42</span>)</a>
<a class="sourceLine" id="cb263-2" data-line-number="2">seeds &lt;-<span class="st"> </span><span class="kw">vector</span>(<span class="dt">mode =</span> <span class="st">&quot;list&quot;</span>, <span class="dt">length =</span> <span class="dv">101</span>)</a>
<a class="sourceLine" id="cb263-3" data-line-number="3"><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">100</span>) seeds[[i]] &lt;-<span class="st"> </span><span class="kw">sample.int</span>(<span class="dv">1000</span>, <span class="dv">19</span>)</a>
<a class="sourceLine" id="cb263-4" data-line-number="4">seeds[[<span class="dv">101</span>]] &lt;-<span class="st"> </span><span class="kw">sample.int</span>(<span class="dv">1000</span>,<span class="dv">1</span>)</a></code></pre></div>
<p>We can now use <strong>trainControl</strong> to create a list of computational options for resampling.</p>
<div class="sourceCode" id="cb264"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb264-1" data-line-number="1">tc &lt;-<span class="st"> </span><span class="kw">trainControl</span>(<span class="dt">method=</span><span class="st">&quot;repeatedcv&quot;</span>,</a>
<a class="sourceLine" id="cb264-2" data-line-number="2">                   <span class="dt">number =</span> <span class="dv">10</span>,</a>
<a class="sourceLine" id="cb264-3" data-line-number="3">                   <span class="dt">repeats =</span> <span class="dv">10</span>,</a>
<a class="sourceLine" id="cb264-4" data-line-number="4">                   <span class="dt">seeds =</span> seeds)</a></code></pre></div>
<p>There are two options for choosing the values of <em>k</em> to be evaluated by the <strong>train</strong> function:</p>
<ol style="list-style-type: decimal">
<li>Pass a data.frame of values of <em>k</em> to the <strong>tuneGrid</strong> argument of <strong>train</strong>.</li>
<li>Specify the number of different levels of <em>k</em> using the <strong>tuneLength</strong> function and allow <strong>train</strong> to pick the actual values.</li>
</ol>
<p>We will use the first option, so that we can try the values of <em>k</em> we examined earlier. The vector of values of <em>k</em> we created earlier should be converted into a data.frame.</p>
<div class="sourceCode" id="cb265"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb265-1" data-line-number="1">s &lt;-<span class="st"> </span><span class="kw">data.frame</span>(s)</a>
<a class="sourceLine" id="cb265-2" data-line-number="2"><span class="kw">names</span>(s) &lt;-<span class="st"> &quot;k&quot;</span></a></code></pre></div>
<p>We are now ready to run the cross-validation.</p>
<div class="sourceCode" id="cb266"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb266-1" data-line-number="1">knnFit &lt;-<span class="st"> </span><span class="kw">train</span>(xtrain, <span class="kw">as.factor</span>(ytrain), </a>
<a class="sourceLine" id="cb266-2" data-line-number="2">                <span class="dt">method=</span><span class="st">&quot;knn&quot;</span>,</a>
<a class="sourceLine" id="cb266-3" data-line-number="3">                <span class="dt">tuneGrid=</span>s,</a>
<a class="sourceLine" id="cb266-4" data-line-number="4">                <span class="dt">trControl=</span>tc)</a>
<a class="sourceLine" id="cb266-5" data-line-number="5"></a>
<a class="sourceLine" id="cb266-6" data-line-number="6">knnFit</a></code></pre></div>
<pre><code>## k-Nearest Neighbors 
## 
## 400 samples
##   2 predictor
##   2 classes: &#39;0&#39;, &#39;1&#39; 
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold, repeated 10 times) 
## Summary of sample sizes: 360, 360, 360, 360, 360, 360, ... 
## Resampling results across tuning parameters:
## 
##   k    Accuracy  Kappa  
##     1  0.63575    0.2715
##     2  0.63700    0.2740
##     3  0.67250    0.3450
##     4  0.67700    0.3540
##     5  0.69425    0.3885
##     7  0.71250    0.4250
##     9  0.71525    0.4305
##    12  0.71275    0.4255
##    17  0.72500    0.4500
##    23  0.73175    0.4635
##    32  0.73675    0.4735
##    44  0.73900    0.4780
##    60  0.74925    0.4985
##    83  0.75325    0.5065
##   113  0.73275    0.4655
##   155  0.72700    0.4540
##   213  0.70950    0.4190
##   292  0.69275    0.3855
##   400  0.49900   -0.0020
## 
## Accuracy was used to select the optimal model using the largest value.
## The final value used for the model was k = 83.</code></pre>
<p><strong>Cohen’s Kappa:</strong>
<span class="math display" id="eq:kappa">\[\begin{equation}
  Kappa = \frac{O-E}{1-E}
  \tag{5.1}
\end{equation}\]</span></p>
<p>where <em>O</em> is the observed accuracy and <em>E</em> is the expected accuracy based on the marginal totals of the confusion matrix. Cohen’s Kappa takes values between -1 and 1; a value of zero indicates no agreement between the observed and predicted classes, while a value of one shows perfect concordance of the model prediction and the observed classes. If the prediction is in the opposite direction of the truth, a negative value will be obtained, but large negative values are rare in practice <span class="citation">(Kuhn and Johnson <a href="#ref-Kuhn2013">2013</a>)</span>.</p>
<p>We can plot accuracy (determined from repeated cross-validation) as a function of neighbourhood size.</p>
<div class="sourceCode" id="cb268"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb268-1" data-line-number="1"><span class="kw">plot</span>(knnFit)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:cvAccuracyFunK"></span>
<img src="04-nearest-neighbours_files/figure-html/cvAccuracyFunK-1.png" alt="Accuracy (repeated cross-validation) as a function of neighbourhood size." width="100%" />
<p class="caption">
Figure 5.5: Accuracy (repeated cross-validation) as a function of neighbourhood size.
</p>
</div>
<p>We can also plot other performance metrics, such as Cohen’s Kappa, using the <strong>metric</strong> argument.</p>
<div class="sourceCode" id="cb269"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb269-1" data-line-number="1"><span class="kw">plot</span>(knnFit, <span class="dt">metric=</span><span class="st">&quot;Kappa&quot;</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:cvKappaFunK"></span>
<img src="04-nearest-neighbours_files/figure-html/cvKappaFunK-1.png" alt="Cohen's Kappa (repeated cross-validation) as a function of neighbourhood size." width="100%" />
<p class="caption">
Figure 5.6: Cohen’s Kappa (repeated cross-validation) as a function of neighbourhood size.
</p>
</div>
<p>Let us now evaluate how our classifier performs on the test set.</p>
<div class="sourceCode" id="cb270"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb270-1" data-line-number="1">test_pred &lt;-<span class="st"> </span><span class="kw">predict</span>(knnFit, xtest)</a>
<a class="sourceLine" id="cb270-2" data-line-number="2"><span class="kw">confusionMatrix</span>(test_pred, <span class="kw">as.factor</span>(ytest))</a></code></pre></div>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction   0   1
##          0 154  68
##          1  46 132
##                                          
##                Accuracy : 0.715          
##                  95% CI : (0.668, 0.7588)
##     No Information Rate : 0.5            
##     P-Value [Acc &gt; NIR] : &lt;2e-16         
##                                          
##                   Kappa : 0.43           
##                                          
##  Mcnemar&#39;s Test P-Value : 0.0492         
##                                          
##             Sensitivity : 0.7700         
##             Specificity : 0.6600         
##          Pos Pred Value : 0.6937         
##          Neg Pred Value : 0.7416         
##              Prevalence : 0.5000         
##          Detection Rate : 0.3850         
##    Detection Prevalence : 0.5550         
##       Balanced Accuracy : 0.7150         
##                                          
##        &#39;Positive&#39; Class : 0              
## </code></pre>
<p>Scatterplots with decision boundaries can be plotted using the methods described earlier. First create a grid so we can predict across the full range of our variables V1 and V2:</p>
<div class="sourceCode" id="cb272"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb272-1" data-line-number="1">gridSize &lt;-<span class="st"> </span><span class="dv">150</span> </a>
<a class="sourceLine" id="cb272-2" data-line-number="2">v1limits &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="kw">min</span>(<span class="kw">c</span>(xtrain[,<span class="dv">1</span>],xtest[,<span class="dv">1</span>])),<span class="kw">max</span>(<span class="kw">c</span>(xtrain[,<span class="dv">1</span>],xtest[,<span class="dv">1</span>])))</a>
<a class="sourceLine" id="cb272-3" data-line-number="3">tmpV1 &lt;-<span class="st"> </span><span class="kw">seq</span>(v1limits[<span class="dv">1</span>],v1limits[<span class="dv">2</span>],<span class="dt">len=</span>gridSize)</a>
<a class="sourceLine" id="cb272-4" data-line-number="4">v2limits &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="kw">min</span>(<span class="kw">c</span>(xtrain[,<span class="dv">2</span>],xtest[,<span class="dv">2</span>])),<span class="kw">max</span>(<span class="kw">c</span>(xtrain[,<span class="dv">2</span>],xtest[,<span class="dv">2</span>])))</a>
<a class="sourceLine" id="cb272-5" data-line-number="5">tmpV2 &lt;-<span class="st"> </span><span class="kw">seq</span>(v2limits[<span class="dv">1</span>],v2limits[<span class="dv">2</span>],<span class="dt">len=</span>gridSize)</a>
<a class="sourceLine" id="cb272-6" data-line-number="6">xgrid &lt;-<span class="st"> </span><span class="kw">expand.grid</span>(tmpV1,tmpV2)</a>
<a class="sourceLine" id="cb272-7" data-line-number="7"><span class="kw">names</span>(xgrid) &lt;-<span class="st"> </span><span class="kw">names</span>(xtrain)</a></code></pre></div>
<p>Predict values of all elements of grid.</p>
<div class="sourceCode" id="cb273"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb273-1" data-line-number="1">knn1grid &lt;-<span class="st"> </span><span class="kw">predict</span>(knnFit, xgrid)</a>
<a class="sourceLine" id="cb273-2" data-line-number="2">V3 &lt;-<span class="st"> </span><span class="kw">as.numeric</span>(<span class="kw">as.vector</span>(knn1grid))</a>
<a class="sourceLine" id="cb273-3" data-line-number="3">xgrid &lt;-<span class="st"> </span><span class="kw">cbind</span>(xgrid, V3)</a></code></pre></div>
<p>Plot</p>
<div class="sourceCode" id="cb274"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb274-1" data-line-number="1">point_shapes &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">15</span>,<span class="dv">17</span>)</a>
<a class="sourceLine" id="cb274-2" data-line-number="2">point_colours &lt;-<span class="st"> </span><span class="kw">brewer.pal</span>(<span class="dv">3</span>,<span class="st">&quot;Dark2&quot;</span>)</a>
<a class="sourceLine" id="cb274-3" data-line-number="3">point_size =<span class="st"> </span><span class="dv">2</span></a>
<a class="sourceLine" id="cb274-4" data-line-number="4"></a>
<a class="sourceLine" id="cb274-5" data-line-number="5"><span class="kw">ggplot</span>(xgrid, <span class="kw">aes</span>(V1,V2)) <span class="op">+</span></a>
<a class="sourceLine" id="cb274-6" data-line-number="6"><span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">col=</span>point_colours[knn1grid], <span class="dt">shape=</span><span class="dv">16</span>, <span class="dt">size=</span><span class="fl">0.3</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb274-7" data-line-number="7"><span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">data=</span>xtrain, <span class="kw">aes</span>(V1,V2), <span class="dt">col=</span>point_colours[ytrain<span class="op">+</span><span class="dv">1</span>],</a>
<a class="sourceLine" id="cb274-8" data-line-number="8">             <span class="dt">shape=</span>point_shapes[ytrain<span class="op">+</span><span class="dv">1</span>], <span class="dt">size=</span>point_size) <span class="op">+</span></a>
<a class="sourceLine" id="cb274-9" data-line-number="9"><span class="st">  </span><span class="kw">geom_contour</span>(<span class="dt">data=</span>xgrid, <span class="kw">aes</span>(<span class="dt">x=</span>V1, <span class="dt">y=</span>V2, <span class="dt">z=</span>V3), <span class="dt">breaks=</span><span class="fl">0.5</span>, <span class="dt">col=</span><span class="st">&quot;grey30&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb274-10" data-line-number="10"><span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;train&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb274-11" data-line-number="11"><span class="st">  </span><span class="kw">theme_bw</span>() <span class="op">+</span></a>
<a class="sourceLine" id="cb274-12" data-line-number="12"><span class="st">  </span><span class="kw">theme</span>(<span class="dt">plot.title =</span> <span class="kw">element_text</span>(<span class="dt">size=</span><span class="dv">25</span>, <span class="dt">face=</span><span class="st">&quot;bold&quot;</span>), <span class="dt">axis.text=</span><span class="kw">element_text</span>(<span class="dt">size=</span><span class="dv">15</span>),</a>
<a class="sourceLine" id="cb274-13" data-line-number="13">        <span class="dt">axis.title=</span><span class="kw">element_text</span>(<span class="dt">size=</span><span class="dv">20</span>,<span class="dt">face=</span><span class="st">&quot;bold&quot;</span>))</a>
<a class="sourceLine" id="cb274-14" data-line-number="14"></a>
<a class="sourceLine" id="cb274-15" data-line-number="15"><span class="kw">ggplot</span>(xgrid, <span class="kw">aes</span>(V1,V2)) <span class="op">+</span></a>
<a class="sourceLine" id="cb274-16" data-line-number="16"><span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">col=</span>point_colours[knn1grid], <span class="dt">shape=</span><span class="dv">16</span>, <span class="dt">size=</span><span class="fl">0.3</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb274-17" data-line-number="17"><span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">data=</span>xtest, <span class="kw">aes</span>(V1,V2), <span class="dt">col=</span>point_colours[ytest<span class="op">+</span><span class="dv">1</span>],</a>
<a class="sourceLine" id="cb274-18" data-line-number="18">             <span class="dt">shape=</span>point_shapes[ytrain<span class="op">+</span><span class="dv">1</span>], <span class="dt">size=</span>point_size) <span class="op">+</span></a>
<a class="sourceLine" id="cb274-19" data-line-number="19"><span class="st">  </span><span class="kw">geom_contour</span>(<span class="dt">data=</span>xgrid, <span class="kw">aes</span>(<span class="dt">x=</span>V1, <span class="dt">y=</span>V2, <span class="dt">z=</span>V3), <span class="dt">breaks=</span><span class="fl">0.5</span>, <span class="dt">col=</span><span class="st">&quot;grey30&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb274-20" data-line-number="20"><span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;test&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb274-21" data-line-number="21"><span class="st">  </span><span class="kw">theme_bw</span>() <span class="op">+</span></a>
<a class="sourceLine" id="cb274-22" data-line-number="22"><span class="st">  </span><span class="kw">theme</span>(<span class="dt">plot.title =</span> <span class="kw">element_text</span>(<span class="dt">size=</span><span class="dv">25</span>, <span class="dt">face=</span><span class="st">&quot;bold&quot;</span>), <span class="dt">axis.text=</span><span class="kw">element_text</span>(<span class="dt">size=</span><span class="dv">15</span>),</a>
<a class="sourceLine" id="cb274-23" data-line-number="23">        <span class="dt">axis.title=</span><span class="kw">element_text</span>(<span class="dt">size=</span><span class="dv">20</span>,<span class="dt">face=</span><span class="st">&quot;bold&quot;</span>))</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:simDataBinClassDecisionBoundaryK83"></span>
<img src="04-nearest-neighbours_files/figure-html/simDataBinClassDecisionBoundaryK83-1.png" alt="Binary classification of the simulated training and test sets with _k_=83." width="50%" /><img src="04-nearest-neighbours_files/figure-html/simDataBinClassDecisionBoundaryK83-2.png" alt="Binary classification of the simulated training and test sets with _k_=83." width="50%" />
<p class="caption">
Figure 5.7: Binary classification of the simulated training and test sets with <em>k</em>=83.
</p>
</div>
</div>
</div>
<div id="example-on-the-iris-dataset" class="section level2 hasAnchor">
<h2><span class="header-section-number">5.3</span> Example on the Iris dataset<a href="nearest-neighbours.html#example-on-the-iris-dataset" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>From the iris manual page:</p>
<p>The famous (Fisher’s or Anderson’s) Iris data set, first presented by Fisher in 1936 (<a href="http://archive.ics.uci.edu/ml/datasets/Iris" class="uri">http://archive.ics.uci.edu/ml/datasets/Iris</a>), gives the measurements in centimeters of the variables sepal length and width and petal length and width, respectively, for 50 flowers from each of 3 species of iris. The species are Iris setosa, versicolor, and virginica. One class is linearly separable from the other two; the latter are not linearly separable from each other.
The data base contains the following attributes:
1). sepal length in cm
2). sepal width in cm
3). petal length in cm
4). petal width in cm
5). classes:
- Iris Setosa
- Iris Versicolour
- Iris Virginica</p>
<div class="sourceCode" id="cb275"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb275-1" data-line-number="1"><span class="kw">library</span>(datasets)</a>
<a class="sourceLine" id="cb275-2" data-line-number="2"><span class="kw">library</span>(gridExtra)</a>
<a class="sourceLine" id="cb275-3" data-line-number="3"><span class="kw">library</span>(GGally)</a>
<a class="sourceLine" id="cb275-4" data-line-number="4"><span class="kw">data</span>(iris)      <span class="co">##loads the dataset, which can be accessed under the variable name iris</span></a>
<a class="sourceLine" id="cb275-5" data-line-number="5"><span class="kw">summary</span>(iris)   <span class="co">##presents the 5 figure summary of the dataset</span></a></code></pre></div>
<pre><code>##   Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   
##  Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100  
##  1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300  
##  Median :5.800   Median :3.000   Median :4.350   Median :1.300  
##  Mean   :5.843   Mean   :3.057   Mean   :3.758   Mean   :1.199  
##  3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800  
##  Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500  
##        Species  
##  setosa    :50  
##  versicolor:50  
##  virginica :50  
##                 
##                 
## </code></pre>
<div class="sourceCode" id="cb277"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb277-1" data-line-number="1"><span class="kw">str</span>(iris)       <span class="co">##presents the structure of the iris dataframe</span></a></code></pre></div>
<pre><code>## &#39;data.frame&#39;:    150 obs. of  5 variables:
##  $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...
##  $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...
##  $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...
##  $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...
##  $ Species     : Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 1 1 1 1 1 1 1 1 1 ...</code></pre>
<p>Explore the data: visualize the numerical values using the violin plots.
They are similar to the Box Plots but they allow the illustration of the number of points at a particular value by the width of the shapes. We can also include the marker for the median and a box for the interquartile range.</p>
<div class="sourceCode" id="cb279"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb279-1" data-line-number="1">VpSl &lt;-<span class="st">  </span><span class="kw">ggplot</span>(iris, <span class="kw">aes</span>(Species, Sepal.Length, <span class="dt">fill=</span>Species)) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb279-2" data-line-number="2"><span class="st">        </span><span class="kw">geom_violin</span>(<span class="kw">aes</span>(<span class="dt">color =</span> Species), <span class="dt">trim =</span> T)<span class="op">+</span></a>
<a class="sourceLine" id="cb279-3" data-line-number="3"><span class="st">        </span><span class="kw">scale_y_continuous</span>(<span class="st">&quot;Sepal Length&quot;</span>, <span class="dt">breaks=</span> <span class="kw">seq</span>(<span class="dv">0</span>,<span class="dv">30</span>, <span class="dt">by=</span>.<span class="dv">5</span>))<span class="op">+</span></a>
<a class="sourceLine" id="cb279-4" data-line-number="4"><span class="st">        </span><span class="kw">geom_boxplot</span>(<span class="dt">width=</span><span class="fl">0.1</span>)<span class="op">+</span></a>
<a class="sourceLine" id="cb279-5" data-line-number="5"><span class="st">        </span><span class="kw">theme</span>(<span class="dt">legend.position=</span><span class="st">&quot;none&quot;</span>)</a>
<a class="sourceLine" id="cb279-6" data-line-number="6">VpSw &lt;-<span class="st">  </span><span class="kw">ggplot</span>(iris, <span class="kw">aes</span>(Species, Sepal.Width, <span class="dt">fill=</span>Species)) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb279-7" data-line-number="7"><span class="st">        </span><span class="kw">geom_violin</span>(<span class="kw">aes</span>(<span class="dt">color =</span> Species), <span class="dt">trim =</span> T)<span class="op">+</span></a>
<a class="sourceLine" id="cb279-8" data-line-number="8"><span class="st">        </span><span class="kw">scale_y_continuous</span>(<span class="st">&quot;Sepal Width&quot;</span>, <span class="dt">breaks=</span> <span class="kw">seq</span>(<span class="dv">0</span>,<span class="dv">30</span>, <span class="dt">by=</span>.<span class="dv">5</span>))<span class="op">+</span></a>
<a class="sourceLine" id="cb279-9" data-line-number="9"><span class="st">        </span><span class="kw">geom_boxplot</span>(<span class="dt">width=</span><span class="fl">0.1</span>)<span class="op">+</span></a>
<a class="sourceLine" id="cb279-10" data-line-number="10"><span class="st">        </span><span class="kw">theme</span>(<span class="dt">legend.position=</span><span class="st">&quot;none&quot;</span>)</a>
<a class="sourceLine" id="cb279-11" data-line-number="11">VpPl &lt;-<span class="st">  </span><span class="kw">ggplot</span>(iris, <span class="kw">aes</span>(Species, Petal.Length, <span class="dt">fill=</span>Species)) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb279-12" data-line-number="12"><span class="st">        </span><span class="kw">geom_violin</span>(<span class="kw">aes</span>(<span class="dt">color =</span> Species), <span class="dt">trim =</span> T)<span class="op">+</span></a>
<a class="sourceLine" id="cb279-13" data-line-number="13"><span class="st">        </span><span class="kw">scale_y_continuous</span>(<span class="st">&quot;Petal Length&quot;</span>, <span class="dt">breaks=</span> <span class="kw">seq</span>(<span class="dv">0</span>,<span class="dv">30</span>, <span class="dt">by=</span>.<span class="dv">5</span>))<span class="op">+</span></a>
<a class="sourceLine" id="cb279-14" data-line-number="14"><span class="st">        </span><span class="kw">geom_boxplot</span>(<span class="dt">width=</span><span class="fl">0.1</span>)<span class="op">+</span></a>
<a class="sourceLine" id="cb279-15" data-line-number="15"><span class="st">        </span><span class="kw">theme</span>(<span class="dt">legend.position=</span><span class="st">&quot;none&quot;</span>)</a>
<a class="sourceLine" id="cb279-16" data-line-number="16">VpPw &lt;-<span class="st">  </span><span class="kw">ggplot</span>(iris, <span class="kw">aes</span>(Species, Petal.Width, <span class="dt">fill=</span>Species)) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb279-17" data-line-number="17"><span class="st">        </span><span class="kw">geom_violin</span>(<span class="kw">aes</span>(<span class="dt">color =</span> Species), <span class="dt">trim =</span> T)<span class="op">+</span></a>
<a class="sourceLine" id="cb279-18" data-line-number="18"><span class="st">        </span><span class="kw">scale_y_continuous</span>(<span class="st">&quot;Petal Width&quot;</span>, <span class="dt">breaks=</span> <span class="kw">seq</span>(<span class="dv">0</span>,<span class="dv">30</span>, <span class="dt">by=</span>.<span class="dv">5</span>))<span class="op">+</span></a>
<a class="sourceLine" id="cb279-19" data-line-number="19"><span class="st">        </span><span class="kw">geom_boxplot</span>(<span class="dt">width=</span><span class="fl">0.1</span>)<span class="op">+</span></a>
<a class="sourceLine" id="cb279-20" data-line-number="20"><span class="st">        </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Iris Box Plot&quot;</span>, <span class="dt">x =</span> <span class="st">&quot;Species&quot;</span>)</a>
<a class="sourceLine" id="cb279-21" data-line-number="21"><span class="co"># Plot all visualizations</span></a>
<a class="sourceLine" id="cb279-22" data-line-number="22"><span class="kw">grid.arrange</span>(VpSl  <span class="op">+</span><span class="st"> </span><span class="kw">ggtitle</span>(<span class="st">&quot;&quot;</span>),</a>
<a class="sourceLine" id="cb279-23" data-line-number="23">             VpSw  <span class="op">+</span><span class="st"> </span><span class="kw">ggtitle</span>(<span class="st">&quot;&quot;</span>),</a>
<a class="sourceLine" id="cb279-24" data-line-number="24">             VpPl <span class="op">+</span><span class="st"> </span><span class="kw">ggtitle</span>(<span class="st">&quot;&quot;</span>),</a>
<a class="sourceLine" id="cb279-25" data-line-number="25">             VpPw <span class="op">+</span><span class="st"> </span><span class="kw">ggtitle</span>(<span class="st">&quot;&quot;</span>),</a>
<a class="sourceLine" id="cb279-26" data-line-number="26">             <span class="dt">nrow =</span> <span class="dv">2</span>)</a></code></pre></div>
<p><img src="04-nearest-neighbours_files/figure-html/unnamed-chunk-21-1.png" width="672" /></p>
<div class="sourceCode" id="cb280"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb280-1" data-line-number="1"><span class="kw">ggpairs</span>(iris, ggplot2<span class="op">::</span><span class="kw">aes</span>(<span class="dt">colour =</span> Species, <span class="dt">alpha =</span> <span class="fl">0.4</span>))             </a></code></pre></div>
<p><img src="04-nearest-neighbours_files/figure-html/unnamed-chunk-21-2.png" width="672" /></p>
<p>Divide the Iris dataset into training and test dataset to apply KNN classification. 80% of the data is used for training while the KNN classification is tested on the remaining 20% of the data.</p>
<div class="sourceCode" id="cb281"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb281-1" data-line-number="1">iris[,<span class="dv">1</span><span class="op">:</span><span class="dv">4</span>] &lt;-<span class="st"> </span><span class="kw">scale</span>(iris[,<span class="dv">1</span><span class="op">:</span><span class="dv">4</span>])</a>
<a class="sourceLine" id="cb281-2" data-line-number="2">setosa&lt;-<span class="st"> </span><span class="kw">rbind</span>(iris[iris<span class="op">$</span>Species<span class="op">==</span><span class="st">&quot;setosa&quot;</span>,])</a>
<a class="sourceLine" id="cb281-3" data-line-number="3">versicolor&lt;-<span class="st"> </span><span class="kw">rbind</span>(iris[iris<span class="op">$</span>Species<span class="op">==</span><span class="st">&quot;versicolor&quot;</span>,])</a>
<a class="sourceLine" id="cb281-4" data-line-number="4">virginica&lt;-<span class="st"> </span><span class="kw">rbind</span>(iris[iris<span class="op">$</span>Species<span class="op">==</span><span class="st">&quot;virginica&quot;</span>,])</a>
<a class="sourceLine" id="cb281-5" data-line-number="5"></a>
<a class="sourceLine" id="cb281-6" data-line-number="6">ind &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="dv">1</span><span class="op">:</span><span class="kw">nrow</span>(setosa), <span class="kw">nrow</span>(setosa)<span class="op">*</span><span class="fl">0.8</span>)</a>
<a class="sourceLine" id="cb281-7" data-line-number="7">iris.train&lt;-<span class="st"> </span><span class="kw">rbind</span>(setosa[ind,], versicolor[ind,], virginica[ind,])</a>
<a class="sourceLine" id="cb281-8" data-line-number="8">iris.test&lt;-<span class="st"> </span><span class="kw">rbind</span>(setosa[<span class="op">-</span>ind,], versicolor[<span class="op">-</span>ind,], virginica[<span class="op">-</span>ind,])</a>
<a class="sourceLine" id="cb281-9" data-line-number="9">iris[,<span class="dv">1</span><span class="op">:</span><span class="dv">4</span>] &lt;-<span class="st"> </span><span class="kw">scale</span>(iris[,<span class="dv">1</span><span class="op">:</span><span class="dv">4</span>])</a></code></pre></div>
<p>Then train and evaluate</p>
<div class="sourceCode" id="cb282"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb282-1" data-line-number="1"><span class="kw">library</span>(class)</a>
<a class="sourceLine" id="cb282-2" data-line-number="2"><span class="kw">library</span>(gmodels)</a>
<a class="sourceLine" id="cb282-3" data-line-number="3">error &lt;-<span class="st"> </span><span class="kw">c</span>()</a>
<a class="sourceLine" id="cb282-4" data-line-number="4"><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">15</span>)</a>
<a class="sourceLine" id="cb282-5" data-line-number="5">{</a>
<a class="sourceLine" id="cb282-6" data-line-number="6">  knn.fit &lt;-<span class="st"> </span><span class="kw">knn</span>(<span class="dt">train =</span> iris.train[,<span class="dv">1</span><span class="op">:</span><span class="dv">4</span>], <span class="dt">test =</span> iris.test[,<span class="dv">1</span><span class="op">:</span><span class="dv">4</span>], <span class="dt">cl =</span> iris.train<span class="op">$</span>Species, <span class="dt">k =</span> i)</a>
<a class="sourceLine" id="cb282-7" data-line-number="7">  error[i] =<span class="st"> </span><span class="dv">1</span><span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(knn.fit <span class="op">==</span><span class="st"> </span>iris.test<span class="op">$</span>Species)</a>
<a class="sourceLine" id="cb282-8" data-line-number="8">}</a>
<a class="sourceLine" id="cb282-9" data-line-number="9"></a>
<a class="sourceLine" id="cb282-10" data-line-number="10"><span class="kw">ggplot</span>(<span class="dt">data =</span> <span class="kw">data.frame</span>(error), <span class="kw">aes</span>(<span class="dt">x =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">15</span>, <span class="dt">y =</span> error)) <span class="op">+</span></a>
<a class="sourceLine" id="cb282-11" data-line-number="11"><span class="st">  </span><span class="kw">geom_line</span>(<span class="dt">color =</span> <span class="st">&quot;Blue&quot;</span>)</a></code></pre></div>
<p><img src="04-nearest-neighbours_files/figure-html/unnamed-chunk-23-1.png" width="672" /></p>
<div class="sourceCode" id="cb283"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb283-1" data-line-number="1">iris_test_pred1 &lt;-<span class="st"> </span><span class="kw">knn</span>(<span class="dt">train =</span> iris.train[,<span class="dv">1</span><span class="op">:</span><span class="dv">4</span>], <span class="dt">test =</span> iris.test[,<span class="dv">1</span><span class="op">:</span><span class="dv">4</span>], <span class="dt">cl =</span> iris.train<span class="op">$</span>Species,<span class="dt">k =</span> <span class="dv">7</span>,<span class="dt">prob=</span><span class="ot">TRUE</span>) </a>
<a class="sourceLine" id="cb283-2" data-line-number="2"><span class="kw">table</span>(iris.test<span class="op">$</span>Species,iris_test_pred1)</a></code></pre></div>
<pre><code>##             iris_test_pred1
##              setosa versicolor virginica
##   setosa         10          0         0
##   versicolor      0          9         1
##   virginica       0          1         9</code></pre>
<div class="sourceCode" id="cb285"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb285-1" data-line-number="1"><span class="kw">CrossTable</span>(<span class="dt">x =</span> iris.test<span class="op">$</span>Species, <span class="dt">y =</span> iris_test_pred1,<span class="dt">prop.chisq=</span><span class="ot">FALSE</span>) </a></code></pre></div>
<pre><code>## 
##  
##    Cell Contents
## |-------------------------|
## |                       N |
## |           N / Row Total |
## |           N / Col Total |
## |         N / Table Total |
## |-------------------------|
## 
##  
## Total Observations in Table:  30 
## 
##  
##                   | iris_test_pred1 
## iris.test$Species |     setosa | versicolor |  virginica |  Row Total | 
## ------------------|------------|------------|------------|------------|
##            setosa |         10 |          0 |          0 |         10 | 
##                   |      1.000 |      0.000 |      0.000 |      0.333 | 
##                   |      1.000 |      0.000 |      0.000 |            | 
##                   |      0.333 |      0.000 |      0.000 |            | 
## ------------------|------------|------------|------------|------------|
##        versicolor |          0 |          9 |          1 |         10 | 
##                   |      0.000 |      0.900 |      0.100 |      0.333 | 
##                   |      0.000 |      0.900 |      0.100 |            | 
##                   |      0.000 |      0.300 |      0.033 |            | 
## ------------------|------------|------------|------------|------------|
##         virginica |          0 |          1 |          9 |         10 | 
##                   |      0.000 |      0.100 |      0.900 |      0.333 | 
##                   |      0.000 |      0.100 |      0.900 |            | 
##                   |      0.000 |      0.033 |      0.300 |            | 
## ------------------|------------|------------|------------|------------|
##      Column Total |         10 |         10 |         10 |         30 | 
##                   |      0.333 |      0.333 |      0.333 |            | 
## ------------------|------------|------------|------------|------------|
## 
## </code></pre>
</div>
<div id="knn-cell-segmentation" class="section level2 hasAnchor">
<h2><span class="header-section-number">5.4</span> Classification: cell segmentation<a href="nearest-neighbours.html#knn-cell-segmentation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The simulated data in our previous example were randomly sampled from a normal (Gaussian) distribution and so did not require pre-processing. In practice, data collected in real studies often require transformation and/or filtering. Furthermore, the simulated data contained only two predictors; in practice, you are likely to have many variables. For example, in a gene expression study you might have thousands of variables. When using <em>k</em>-nn for classification or regression, removing variables that are not associated with the outcome of interest may improve the predictive power of the model. The process of choosing the best predictors from the available variables is known as <em>feature selection</em>. For honest estimates of model performance, pre-processing and feature selection should be performed within the loops of the cross validation process.</p>
<div id="cell-segmentation-data-set" class="section level3 hasAnchor">
<h3><span class="header-section-number">5.4.1</span> Cell segmentation data set<a href="nearest-neighbours.html#cell-segmentation-data-set" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Pre-processing and feature selection will be demonstrated using the cell segmentation data of (<span class="citation">Hill et al. (<a href="#ref-Hill2007">2007</a>)</span>). High Content Screening (HCS) automates the collection and analysis of biological images of cultured cells. However, image segmentation algorithms are not perfect and sometimes do not reliably quantitate the morphology of cells. Hill et al. sought to differentiate between well- and poorly-segmented cells based on the morphometric data collected in HCS. If poorly-segmented cells can be automatically detected and eliminated, then the accuracy of studies using HCS will be improved. Hill et al. collected morphometric data on 2019 cells and asked human reviewers to classify the cells as well- or poorly-segmented.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:imageSegmentationHCS"></span>
<img src="images/Hill_2007_cell_segmentation.jpg" alt="Image segmentation in high content screening. Images **b** and **c** are examples of well-segmented cells; **d** and **e** show poor-segmentation. Source: Hill(2007) https://doi.org/10.1186/1471-2105-8-340" width="75%" />
<p class="caption">
Figure 5.8: Image segmentation in high content screening. Images <strong>b</strong> and <strong>c</strong> are examples of well-segmented cells; <strong>d</strong> and <strong>e</strong> show poor-segmentation. Source: Hill(2007) <a href="https://doi.org/10.1186/1471-2105-8-340" class="uri">https://doi.org/10.1186/1471-2105-8-340</a>
</p>
</div>
<p>This data set is one of several included in <a href="http://cran.r-project.org/web/packages/caret/index.html">caret</a>.</p>
<div class="sourceCode" id="cb287"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb287-1" data-line-number="1"><span class="kw">data</span>(segmentationData)</a>
<a class="sourceLine" id="cb287-2" data-line-number="2"><span class="kw">str</span>(segmentationData)</a></code></pre></div>
<pre><code>## &#39;data.frame&#39;:    2019 obs. of  61 variables:
##  $ Cell                   : int  207827637 207932307 207932463 207932470 207932455 207827656 207827659 207827661 207932479 207932480 ...
##  $ Case                   : Factor w/ 2 levels &quot;Test&quot;,&quot;Train&quot;: 1 2 2 2 1 1 1 1 1 1 ...
##  $ Class                  : Factor w/ 2 levels &quot;PS&quot;,&quot;WS&quot;: 1 1 2 1 1 2 2 1 2 2 ...
##  $ AngleCh1               : num  143.25 133.75 106.65 69.15 2.89 ...
##  $ AreaCh1                : int  185 819 431 298 285 172 177 251 495 384 ...
##  $ AvgIntenCh1            : num  15.7 31.9 28 19.5 24.3 ...
##  $ AvgIntenCh2            : num  4.95 206.88 116.32 102.29 112.42 ...
##  $ AvgIntenCh3            : num  9.55 69.92 63.94 28.22 20.47 ...
##  $ AvgIntenCh4            : num  2.21 164.15 106.7 31.03 40.58 ...
##  $ ConvexHullAreaRatioCh1 : num  1.12 1.26 1.05 1.2 1.11 ...
##  $ ConvexHullPerimRatioCh1: num  0.92 0.797 0.935 0.866 0.957 ...
##  $ DiffIntenDensityCh1    : num  29.5 31.9 32.5 26.7 31.6 ...
##  $ DiffIntenDensityCh3    : num  13.8 43.1 36 22.9 21.7 ...
##  $ DiffIntenDensityCh4    : num  6.83 79.31 51.36 26.39 25.03 ...
##  $ EntropyIntenCh1        : num  4.97 6.09 5.88 5.42 5.66 ...
##  $ EntropyIntenCh3        : num  4.37 6.64 6.68 5.44 5.29 ...
##  $ EntropyIntenCh4        : num  2.72 7.88 7.14 5.78 5.24 ...
##  $ EqCircDiamCh1          : num  15.4 32.3 23.4 19.5 19.1 ...
##  $ EqEllipseLWRCh1        : num  3.06 1.56 1.38 3.39 2.74 ...
##  $ EqEllipseOblateVolCh1  : num  337 2233 802 725 608 ...
##  $ EqEllipseProlateVolCh1 : num  110 1433 583 214 222 ...
##  $ EqSphereAreaCh1        : num  742 3279 1727 1195 1140 ...
##  $ EqSphereVolCh1         : num  1901 17654 6751 3884 3621 ...
##  $ FiberAlign2Ch3         : num  1 1.49 1.3 1.22 1.49 ...
##  $ FiberAlign2Ch4         : num  1 1.35 1.52 1.73 1.38 ...
##  $ FiberLengthCh1         : num  27 64.3 21.1 43.1 34.7 ...
##  $ FiberWidthCh1          : num  7.41 13.17 21.14 7.4 8.48 ...
##  $ IntenCoocASMCh3        : num  0.01118 0.02805 0.00686 0.03096 0.02277 ...
##  $ IntenCoocASMCh4        : num  0.05045 0.01259 0.00614 0.01103 0.07969 ...
##  $ IntenCoocContrastCh3   : num  40.75 8.23 14.45 7.3 15.85 ...
##  $ IntenCoocContrastCh4   : num  13.9 6.98 16.7 13.39 3.54 ...
##  $ IntenCoocEntropyCh3    : num  7.2 6.82 7.58 6.31 6.78 ...
##  $ IntenCoocEntropyCh4    : num  5.25 7.1 7.67 7.2 5.5 ...
##  $ IntenCoocMaxCh3        : num  0.0774 0.1532 0.0284 0.1628 0.1274 ...
##  $ IntenCoocMaxCh4        : num  0.172 0.0739 0.0232 0.0775 0.2785 ...
##  $ KurtIntenCh1           : num  -0.6567 -0.2488 -0.2935 0.6259 0.0421 ...
##  $ KurtIntenCh3           : num  -0.608 -0.331 1.051 0.128 0.952 ...
##  $ KurtIntenCh4           : num  0.726 -0.265 0.151 -0.347 -0.195 ...
##  $ LengthCh1              : num  26.2 47.2 28.1 37.9 36 ...
##  $ NeighborAvgDistCh1     : num  370 174 158 206 205 ...
##  $ NeighborMinDistCh1     : num  99.1 30.1 34.9 33.1 27 ...
##  $ NeighborVarDistCh1     : num  128 81.4 90.4 116.9 111 ...
##  $ PerimCh1               : num  68.8 154.9 84.6 101.1 86.5 ...
##  $ ShapeBFRCh1            : num  0.665 0.54 0.724 0.589 0.6 ...
##  $ ShapeLWRCh1            : num  2.46 1.47 1.33 2.83 2.73 ...
##  $ ShapeP2ACh1            : num  1.88 2.26 1.27 2.55 2.02 ...
##  $ SkewIntenCh1           : num  0.455 0.399 0.472 0.882 0.517 ...
##  $ SkewIntenCh3           : num  0.46 0.62 0.971 1 1.177 ...
##  $ SkewIntenCh4           : num  1.233 0.527 0.325 0.604 0.926 ...
##  $ SpotFiberCountCh3      : int  1 4 2 4 1 1 0 2 1 1 ...
##  $ SpotFiberCountCh4      : num  5 12 7 8 8 5 5 8 12 8 ...
##  $ TotalIntenCh1          : int  2781 24964 11552 5545 6603 53779 43950 4401 7593 6512 ...
##  $ TotalIntenCh2          : num  701 160998 47511 28870 30306 ...
##  $ TotalIntenCh3          : int  1690 54675 26344 8042 5569 21234 20929 4136 6488 7503 ...
##  $ TotalIntenCh4          : int  392 128368 43959 8843 11037 57231 46187 373 24325 23162 ...
##  $ VarIntenCh1            : num  12.5 18.8 17.3 13.8 15.4 ...
##  $ VarIntenCh3            : num  7.61 56.72 37.67 30.01 20.5 ...
##  $ VarIntenCh4            : num  2.71 118.39 49.47 24.75 45.45 ...
##  $ WidthCh1               : num  10.6 32.2 21.2 13.4 13.2 ...
##  $ XCentroid              : int  42 215 371 487 283 191 180 373 236 303 ...
##  $ YCentroid              : int  14 347 252 295 159 127 138 181 467 468 ...</code></pre>
<p>The first column of <strong>segmentationData</strong> is a unique identifier for each cell and the second column is a factor indicating how the observations were characterized into training and test sets in the original study; these two variables are irrelevant for the purposes of this demonstration and so can be discarded.</p>
<p>The third column <em>Class</em> contains the class labels: <em>PS</em> (poorly-segmented) and <em>WS</em> (well-segmented). The last two columns are cell centroids and can be ignored. Columns 4-59 are the 58 morphological measurements available to be used as predictors. Let’s put the class labels in a vector and the predictors in their own data.frame.</p>
<div class="sourceCode" id="cb289"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb289-1" data-line-number="1">segClass &lt;-<span class="st"> </span>segmentationData<span class="op">$</span>Class</a>
<a class="sourceLine" id="cb289-2" data-line-number="2">segData &lt;-<span class="st"> </span>segmentationData[,<span class="dv">4</span><span class="op">:</span><span class="dv">59</span>]</a></code></pre></div>
</div>
<div id="data-splitting" class="section level3 hasAnchor">
<h3><span class="header-section-number">5.4.2</span> Data splitting<a href="nearest-neighbours.html#data-splitting" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Before starting analysis we must partition the data into training and test sets, using the <strong>createDataPartition</strong> function in <a href="http://cran.r-project.org/web/packages/caret/index.html">caret</a>.</p>
<div class="sourceCode" id="cb290"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb290-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">42</span>)</a>
<a class="sourceLine" id="cb290-2" data-line-number="2">trainIndex &lt;-<span class="st"> </span><span class="kw">createDataPartition</span>(<span class="dt">y=</span>segClass, <span class="dt">times=</span><span class="dv">1</span>, <span class="dt">p=</span><span class="fl">0.5</span>, <span class="dt">list=</span>F)</a>
<a class="sourceLine" id="cb290-3" data-line-number="3">segDataTrain &lt;-<span class="st"> </span>segData[trainIndex,]</a>
<a class="sourceLine" id="cb290-4" data-line-number="4">segDataTest &lt;-<span class="st"> </span>segData[<span class="op">-</span>trainIndex,]</a>
<a class="sourceLine" id="cb290-5" data-line-number="5">segClassTrain &lt;-<span class="st"> </span>segClass[trainIndex]</a>
<a class="sourceLine" id="cb290-6" data-line-number="6">segClassTest &lt;-<span class="st"> </span>segClass[<span class="op">-</span>trainIndex]</a></code></pre></div>
<p>This results in balanced class distributions within the splits:</p>
<div class="sourceCode" id="cb291"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb291-1" data-line-number="1"><span class="kw">summary</span>(segClassTrain)</a></code></pre></div>
<pre><code>##  PS  WS 
## 650 360</code></pre>
<div class="sourceCode" id="cb293"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb293-1" data-line-number="1"><span class="kw">summary</span>(segClassTest)</a></code></pre></div>
<pre><code>##  PS  WS 
## 650 359</code></pre>
<p><em><strong>N.B. The test set is set aside for now. It will be used only ONCE, to test the final model.</strong></em></p>
</div>
<div id="identification-of-data-quality-issues" class="section level3 hasAnchor">
<h3><span class="header-section-number">5.4.3</span> Identification of data quality issues<a href="nearest-neighbours.html#identification-of-data-quality-issues" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let’s check our training data set for some undesirable characteristics which may impact model performance and should be addressed through pre-processing.</p>
<div id="zero-and-near-zero-variance-predictors" class="section level4 hasAnchor">
<h4><span class="header-section-number">5.4.3.1</span> Zero and near zero-variance predictors<a href="nearest-neighbours.html#zero-and-near-zero-variance-predictors" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The function <strong>nearZeroVar</strong> identifies predictors that have one unique value. It also diagnoses predictors having both of the following characteristics:</p>
<ul>
<li>very few unique values relative to the number of samples</li>
<li>the ratio of the frequency of the most common value to the frequency of the 2nd most common value is large.</li>
</ul>
<p>Such <em>zero and near zero-variance predictors</em> have a deleterious impact on modelling and may lead to unstable fits.</p>
<div class="sourceCode" id="cb295"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb295-1" data-line-number="1">nzv &lt;-<span class="st"> </span><span class="kw">nearZeroVar</span>(segDataTrain, <span class="dt">saveMetrics=</span>T)</a>
<a class="sourceLine" id="cb295-2" data-line-number="2">nzv</a></code></pre></div>
<pre><code>##                         freqRatio percentUnique zeroVar   nzv
## AngleCh1                 1.000000    100.000000   FALSE FALSE
## AreaCh1                  1.083333     38.217822   FALSE FALSE
## AvgIntenCh1              1.000000    100.000000   FALSE FALSE
## AvgIntenCh2              3.000000     99.801980   FALSE FALSE
## AvgIntenCh3              1.000000    100.000000   FALSE FALSE
## AvgIntenCh4              1.000000    100.000000   FALSE FALSE
## ConvexHullAreaRatioCh1   1.000000     98.415842   FALSE FALSE
## ConvexHullPerimRatioCh1  1.000000    100.000000   FALSE FALSE
## DiffIntenDensityCh1      1.000000    100.000000   FALSE FALSE
## DiffIntenDensityCh3      2.000000     99.900990   FALSE FALSE
## DiffIntenDensityCh4      1.000000    100.000000   FALSE FALSE
## EntropyIntenCh1          1.000000    100.000000   FALSE FALSE
## EntropyIntenCh3          1.000000    100.000000   FALSE FALSE
## EntropyIntenCh4          1.000000    100.000000   FALSE FALSE
## EqCircDiamCh1            1.083333     38.217822   FALSE FALSE
## EqEllipseLWRCh1          1.000000    100.000000   FALSE FALSE
## EqEllipseOblateVolCh1    1.000000    100.000000   FALSE FALSE
## EqEllipseProlateVolCh1   1.000000    100.000000   FALSE FALSE
## EqSphereAreaCh1          1.083333     38.217822   FALSE FALSE
## EqSphereVolCh1           1.083333     38.217822   FALSE FALSE
## FiberAlign2Ch3           1.173913     95.247525   FALSE FALSE
## FiberAlign2Ch4           7.000000     93.861386   FALSE FALSE
## FiberLengthCh1           1.000000     93.267327   FALSE FALSE
## FiberWidthCh1            1.000000     93.267327   FALSE FALSE
## IntenCoocASMCh3          1.000000    100.000000   FALSE FALSE
## IntenCoocASMCh4          1.000000    100.000000   FALSE FALSE
## IntenCoocContrastCh3     1.000000    100.000000   FALSE FALSE
## IntenCoocContrastCh4     1.000000    100.000000   FALSE FALSE
## IntenCoocEntropyCh3      1.000000    100.000000   FALSE FALSE
## IntenCoocEntropyCh4      1.000000    100.000000   FALSE FALSE
## IntenCoocMaxCh3          1.333333     93.069307   FALSE FALSE
## IntenCoocMaxCh4          1.250000     94.851485   FALSE FALSE
## KurtIntenCh1             1.000000    100.000000   FALSE FALSE
## KurtIntenCh3             1.000000    100.000000   FALSE FALSE
## KurtIntenCh4             1.000000    100.000000   FALSE FALSE
## LengthCh1                1.000000    100.000000   FALSE FALSE
## NeighborAvgDistCh1       1.000000    100.000000   FALSE FALSE
## NeighborMinDistCh1       1.583333     41.881188   FALSE FALSE
## NeighborVarDistCh1       1.000000    100.000000   FALSE FALSE
## PerimCh1                 1.000000     62.970297   FALSE FALSE
## ShapeBFRCh1              1.000000    100.000000   FALSE FALSE
## ShapeLWRCh1              1.000000    100.000000   FALSE FALSE
## ShapeP2ACh1              1.000000     99.702970   FALSE FALSE
## SkewIntenCh1             1.000000    100.000000   FALSE FALSE
## SkewIntenCh3             1.000000    100.000000   FALSE FALSE
## SkewIntenCh4             1.000000    100.000000   FALSE FALSE
## SpotFiberCountCh3        1.260870      1.089109   FALSE FALSE
## SpotFiberCountCh4        1.092199      2.871287   FALSE FALSE
## TotalIntenCh1            1.000000     98.613861   FALSE FALSE
## TotalIntenCh2            1.000000     99.009901   FALSE FALSE
## TotalIntenCh3            1.000000     99.306931   FALSE FALSE
## TotalIntenCh4            1.000000     99.207921   FALSE FALSE
## VarIntenCh1              1.000000    100.000000   FALSE FALSE
## VarIntenCh3              1.000000    100.000000   FALSE FALSE
## VarIntenCh4              1.000000    100.000000   FALSE FALSE
## WidthCh1                 1.000000    100.000000   FALSE FALSE</code></pre>
</div>
<div id="scaling" class="section level4 hasAnchor">
<h4><span class="header-section-number">5.4.3.2</span> Scaling<a href="nearest-neighbours.html#scaling" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The variables in this data set are on different scales, for example:</p>
<div class="sourceCode" id="cb297"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb297-1" data-line-number="1"><span class="kw">summary</span>(segDataTrain<span class="op">$</span>IntenCoocASMCh4)</a></code></pre></div>
<pre><code>##     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. 
## 0.004514 0.017687 0.049861 0.097286 0.111304 0.867845</code></pre>
<div class="sourceCode" id="cb299"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb299-1" data-line-number="1"><span class="kw">summary</span>(segDataTrain<span class="op">$</span>TotalIntenCh2)</a></code></pre></div>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##       1   12639   50160   53165   73527  363311</code></pre>
<p>In this situation it is important to centre and scale each predictor. A predictor variable is centered by subtracting the mean of the predictor from each value. To scale a predictor variable, each value is divided by its standard deviation. After centring and scaling the predictor variable has a mean of 0 and a standard deviation of 1.</p>
</div>
<div id="skewness" class="section level4 hasAnchor">
<h4><span class="header-section-number">5.4.3.3</span> Skewness<a href="nearest-neighbours.html#skewness" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Many of the predictors in the segmentation data set exhibit skewness, <em>i.e.</em> the distribution of their values is asymmetric, for example:</p>
<div class="sourceCode" id="cb301"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb301-1" data-line-number="1"><span class="kw">qplot</span>(segDataTrain<span class="op">$</span>IntenCoocASMCh3, <span class="dt">binwidth=</span><span class="fl">0.1</span>) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb301-2" data-line-number="2"><span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;IntenCoocASMCh3&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb301-3" data-line-number="3"><span class="st">  </span><span class="kw">theme_bw</span>()</a></code></pre></div>
<pre><code>## Warning: `qplot()` was deprecated in ggplot2 3.4.0.</code></pre>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:segDataSkewness"></span>
<img src="04-nearest-neighbours_files/figure-html/segDataSkewness-1.png" alt="Example of a predictor from the segmentation data set showing skewness." width="75%" />
<p class="caption">
Figure 5.9: Example of a predictor from the segmentation data set showing skewness.
</p>
</div>
<p><a href="http://cran.r-project.org/web/packages/caret/index.html">caret</a> provides various methods for transforming skewed variables to normality, including the Box-Cox <span class="citation">(Box and Cox <a href="#ref-BoxCox">1964</a>)</span> and Yeo-Johnson <span class="citation">(Yeo and Johnson <a href="#ref-YeoJohnson">2000</a>)</span> transformations.</p>
</div>
<div id="correlated-predictors" class="section level4 hasAnchor">
<h4><span class="header-section-number">5.4.3.4</span> Correlated predictors<a href="nearest-neighbours.html#correlated-predictors" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Many of the variables in the segmentation data set are highly correlated.</p>
<p>A correlogram provides a helpful visualization of the patterns of pairwise correlation within the data set.</p>
<div class="sourceCode" id="cb303"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb303-1" data-line-number="1"><span class="kw">library</span>(corrplot)</a>
<a class="sourceLine" id="cb303-2" data-line-number="2">corMat &lt;-<span class="st"> </span><span class="kw">cor</span>(segDataTrain)</a>
<a class="sourceLine" id="cb303-3" data-line-number="3"><span class="kw">corrplot</span>(corMat, <span class="dt">order=</span><span class="st">&quot;hclust&quot;</span>, <span class="dt">tl.cex=</span><span class="fl">0.4</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:segDataCorrelogram"></span>
<img src="04-nearest-neighbours_files/figure-html/segDataCorrelogram-1.png" alt="Correlogram of the segmentation data set." width="75%" />
<p class="caption">
Figure 5.10: Correlogram of the segmentation data set.
</p>
</div>
<p>The <strong>preProcess</strong> function in <a href="http://cran.r-project.org/web/packages/caret/index.html">caret</a> has an option, <strong>corr</strong> to remove highly correlated variables. It considers the absolute values of pair-wise correlations. If two variables are highly correlated, <strong>preProcess</strong> looks at the mean absolute correlation of each variable and removes the variable with the largest mean absolute correlation.</p>
<p>In the case of data-sets comprised of many highly correlated variables, an alternative to removing correlated predictors is the transformation of the entire data set to a lower dimensional space, using a technique such as principal component analysis (PCA). Methods for dimensionality reduction will be explored in chapter <a href="dimensionality-reduction.html#dimensionality-reduction">3</a>.</p>
<!--

```r
highCorr <- findCorrelation(corMat, cutoff=0.75)
length(highCorr)
```

```
## [1] 33
```

```r
segDataTrain <- segDataTrain[,-highCorr]
```
-->
</div>
</div>
<div id="fit-model" class="section level3 hasAnchor">
<h3><span class="header-section-number">5.4.4</span> Fit model<a href="nearest-neighbours.html#fit-model" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<!-- original settings:
set.seed(42)
seeds <- vector(mode = "list", length = 101)
for(i in 1:100) seeds[[i]] <- sample.int(1000, 50)
seeds[[101]] <- sample.int(1000,1)
-->
<p>Generate a list of seeds.</p>
<div class="sourceCode" id="cb304"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb304-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">42</span>)</a>
<a class="sourceLine" id="cb304-2" data-line-number="2">seeds &lt;-<span class="st"> </span><span class="kw">vector</span>(<span class="dt">mode =</span> <span class="st">&quot;list&quot;</span>, <span class="dt">length =</span> <span class="dv">26</span>)</a>
<a class="sourceLine" id="cb304-3" data-line-number="3"><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">25</span>) seeds[[i]] &lt;-<span class="st"> </span><span class="kw">sample.int</span>(<span class="dv">1000</span>, <span class="dv">50</span>)</a>
<a class="sourceLine" id="cb304-4" data-line-number="4">seeds[[<span class="dv">26</span>]] &lt;-<span class="st"> </span><span class="kw">sample.int</span>(<span class="dv">1000</span>,<span class="dv">1</span>)</a></code></pre></div>
<p>Create a list of computational options for resampling. In the interest of speed for this demonstration, we will perform 5-fold cross-validation a total of 5 times. In practice we would use a larger number of folds and repetitions.</p>
<div class="sourceCode" id="cb305"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb305-1" data-line-number="1">train_ctrl &lt;-<span class="st"> </span><span class="kw">trainControl</span>(<span class="dt">method=</span><span class="st">&quot;repeatedcv&quot;</span>,</a>
<a class="sourceLine" id="cb305-2" data-line-number="2">                   <span class="dt">number =</span> <span class="dv">5</span>,</a>
<a class="sourceLine" id="cb305-3" data-line-number="3">                   <span class="dt">repeats =</span> <span class="dv">5</span>,</a>
<a class="sourceLine" id="cb305-4" data-line-number="4">                   <span class="co">#preProcOptions=list(cutoff=0.75),</span></a>
<a class="sourceLine" id="cb305-5" data-line-number="5">                   <span class="dt">seeds =</span> seeds)</a></code></pre></div>
<p>Create a grid of values of <em>k</em> for evaluation.</p>
<div class="sourceCode" id="cb306"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb306-1" data-line-number="1">tuneParam &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">k=</span><span class="kw">seq</span>(<span class="dv">5</span>,<span class="dv">500</span>,<span class="dv">10</span>))</a></code></pre></div>
<p>To deal with the issues of scaling, skewness and highly correlated predictors identified earlier, we need to pre-process the data. We will use the Yeo-Johnson transformation to reduce skewness, because it can deal with the zero values present in some of the predictors. Ideally the pre-processing procedures would be performed within each cross-validation loop, using the following command:</p>
<pre><code>knnFit &lt;- train(segDataTrain, segClassTrain, 
                method=&quot;knn&quot;,
                preProcess = c(&quot;YeoJohnson&quot;, &quot;center&quot;, &quot;scale&quot;, &quot;corr&quot;),
                tuneGrid=tuneParam,
                trControl=train_ctrl)</code></pre>
<p>However, this is time-consuming, so for the purposes of this demonstration we will pre-process the entire training data-set before proceeding with training and cross-validation.</p>
<div class="sourceCode" id="cb308"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb308-1" data-line-number="1">transformations &lt;-<span class="st"> </span><span class="kw">preProcess</span>(segDataTrain, </a>
<a class="sourceLine" id="cb308-2" data-line-number="2">                              <span class="dt">method=</span><span class="kw">c</span>(<span class="st">&quot;YeoJohnson&quot;</span>, <span class="st">&quot;center&quot;</span>, <span class="st">&quot;scale&quot;</span>, <span class="st">&quot;corr&quot;</span>),</a>
<a class="sourceLine" id="cb308-3" data-line-number="3">                              <span class="dt">cutoff=</span><span class="fl">0.75</span>)</a>
<a class="sourceLine" id="cb308-4" data-line-number="4">segDataTrain &lt;-<span class="st"> </span><span class="kw">predict</span>(transformations, segDataTrain)</a></code></pre></div>
<p>The <code>cutoff</code> refers to the correlation coefficient threshold.</p>
<div class="sourceCode" id="cb309"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb309-1" data-line-number="1"><span class="kw">str</span>(segDataTrain)</a></code></pre></div>
<pre><code>## &#39;data.frame&#39;:    1010 obs. of  23 variables:
##  $ AngleCh1               : num  0.854 0.352 -0.383 -0.99 1.565 ...
##  $ ConvexHullPerimRatioCh1: num  -1.314 0.494 -0.416 1.241 1.129 ...
##  $ DiffIntenDensityCh3    : num  -0.3311 -0.5341 -1.0142 0.0933 -0.2125 ...
##  $ EntropyIntenCh1        : num  -0.65 -0.958 -1.694 0.615 0.481 ...
##  $ EqSphereVolCh1         : num  1.84 1.07 0.38 -1.18 -1.09 ...
##  $ FiberAlign2Ch3         : num  0.122 -0.602 -0.907 -0.567 -0.814 ...
##  $ FiberAlign2Ch4         : num  -0.321 0.389 1.297 -0.281 0.486 ...
##  $ FiberWidthCh1          : num  0.859 2.033 -0.767 0.696 0.741 ...
##  $ IntenCoocASMCh3        : num  -0.492 -0.645 -0.471 -0.631 -0.636 ...
##  $ IntenCoocASMCh4        : num  -0.633 -0.681 -0.644 -0.658 -0.665 ...
##  $ IntenCoocContrastCh3   : num  0.00893 0.7834 -0.13895 0.01205 0.61482 ...
##  $ IntenCoocContrastCh4   : num  0.106 1.403 1.057 0.597 1.469 ...
##  $ KurtIntenCh1           : num  -0.316 -0.383 0.51 -0.243 0.242 ...
##  $ KurtIntenCh3           : num  -1.0756 -0.0959 -0.6256 -1.8307 -1.4251 ...
##  $ KurtIntenCh4           : num  0.0297 0.4049 -0.0657 -0.1372 -0.7716 ...
##  $ NeighborMinDistCh1     : num  0.271 0.685 0.535 -1.732 -1.732 ...
##  $ NeighborVarDistCh1     : num  -1.169 -0.745 0.538 -0.482 -0.537 ...
##  $ ShapeBFRCh1            : num  -0.591 1.231 -0.104 0.862 1.453 ...
##  $ ShapeLWRCh1            : num  -0.328 -0.72 1.488 -1.792 -1.678 ...
##  $ SpotFiberCountCh3      : num  1.311 0.304 1.311 -0.453 -1.664 ...
##  $ TotalIntenCh1          : num  0.315 -0.507 -1.423 1.011 0.839 ...
##  $ TotalIntenCh2          : num  1.63 0.21 -0.2 1.09 1 ...
##  $ VarIntenCh4            : num  0.295 -0.554 -1.103 0.807 0.35 ...</code></pre>
<p>Perform cross validation to find best value of <em>k</em>.</p>
<div class="sourceCode" id="cb311"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb311-1" data-line-number="1">knnFit &lt;-<span class="st"> </span><span class="kw">train</span>(segDataTrain, segClassTrain, </a>
<a class="sourceLine" id="cb311-2" data-line-number="2">                <span class="dt">method=</span><span class="st">&quot;knn&quot;</span>,</a>
<a class="sourceLine" id="cb311-3" data-line-number="3">                <span class="dt">tuneGrid=</span>tuneParam,</a>
<a class="sourceLine" id="cb311-4" data-line-number="4">                <span class="dt">trControl=</span>train_ctrl)</a>
<a class="sourceLine" id="cb311-5" data-line-number="5">knnFit</a></code></pre></div>
<pre><code>## k-Nearest Neighbors 
## 
## 1010 samples
##   23 predictor
##    2 classes: &#39;PS&#39;, &#39;WS&#39; 
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold, repeated 5 times) 
## Summary of sample sizes: 808, 808, 808, 808, 808, 808, ... 
## Resampling results across tuning parameters:
## 
##   k    Accuracy   Kappa    
##     5  0.7986139  0.5612788
##    15  0.8071287  0.5834268
##    25  0.8063366  0.5809096
##    35  0.8057426  0.5804170
##    45  0.8011881  0.5707377
##    55  0.8025743  0.5740795
##    65  0.8029703  0.5736852
##    75  0.8021782  0.5708545
##    85  0.7994059  0.5653635
##    95  0.8001980  0.5672397
##   105  0.7992079  0.5650183
##   115  0.8000000  0.5662863
##   125  0.8013861  0.5695369
##   135  0.8011881  0.5680439
##   145  0.8015842  0.5681434
##   155  0.8035644  0.5714389
##   165  0.8025743  0.5688528
##   175  0.8019802  0.5675416
##   185  0.8021782  0.5666971
##   195  0.7982178  0.5572158
##   205  0.7976238  0.5553098
##   215  0.7998020  0.5585649
##   225  0.7978218  0.5534856
##   235  0.7956436  0.5474332
##   245  0.7944554  0.5438308
##   255  0.7946535  0.5432198
##   265  0.7954455  0.5450816
##   275  0.7948515  0.5430274
##   285  0.7944554  0.5414309
##   295  0.7944554  0.5404298
##   305  0.7914851  0.5320417
##   315  0.7885149  0.5241017
##   325  0.7891089  0.5249205
##   335  0.7859406  0.5160910
##   345  0.7839604  0.5095485
##   355  0.7831683  0.5061947
##   365  0.7809901  0.4993268
##   375  0.7762376  0.4846859
##   385  0.7716832  0.4701399
##   395  0.7685149  0.4587635
##   405  0.7613861  0.4371686
##   415  0.7564356  0.4216456
##   425  0.7502970  0.3999603
##   435  0.7433663  0.3768513
##   445  0.7340594  0.3465947
##   455  0.7255446  0.3157445
##   465  0.7156436  0.2808044
##   475  0.7061386  0.2477092
##   485  0.6964356  0.2106821
##   495  0.6857426  0.1679627
## 
## Accuracy was used to select the optimal model using the largest value.
## The final value used for the model was k = 15.</code></pre>
<div class="sourceCode" id="cb313"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb313-1" data-line-number="1"><span class="kw">plot</span>(knnFit)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:cvAccuracySegDataHighCorRem"></span>
<img src="04-nearest-neighbours_files/figure-html/cvAccuracySegDataHighCorRem-1.png" alt="Accuracy (repeated cross-validation) as a function of neighbourhood size for the segmentation training data with highly correlated predictors removed." width="100%" />
<p class="caption">
Figure 5.11: Accuracy (repeated cross-validation) as a function of neighbourhood size for the segmentation training data with highly correlated predictors removed.
</p>
</div>
<p>Let’s retrieve some information on the final model. To see the optimum value of <em>k</em> found during the grid search, run either of the following lines:</p>
<div class="sourceCode" id="cb314"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb314-1" data-line-number="1">knnFit<span class="op">$</span>finalModel<span class="op">$</span>k</a></code></pre></div>
<pre><code>## [1] 15</code></pre>
<div class="sourceCode" id="cb316"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb316-1" data-line-number="1">knnFit<span class="op">$</span>finalModel<span class="op">$</span>tuneValue</a></code></pre></div>
<pre><code>##    k
## 2 15</code></pre>
<p>To find out which variables have been used in the final model, run:</p>
<div class="sourceCode" id="cb318"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb318-1" data-line-number="1">knnFit<span class="op">$</span>finalModel<span class="op">$</span>xNames</a></code></pre></div>
<pre><code>##  [1] &quot;AngleCh1&quot;                &quot;ConvexHullPerimRatioCh1&quot;
##  [3] &quot;DiffIntenDensityCh3&quot;     &quot;EntropyIntenCh1&quot;        
##  [5] &quot;EqSphereVolCh1&quot;          &quot;FiberAlign2Ch3&quot;         
##  [7] &quot;FiberAlign2Ch4&quot;          &quot;FiberWidthCh1&quot;          
##  [9] &quot;IntenCoocASMCh3&quot;         &quot;IntenCoocASMCh4&quot;        
## [11] &quot;IntenCoocContrastCh3&quot;    &quot;IntenCoocContrastCh4&quot;   
## [13] &quot;KurtIntenCh1&quot;            &quot;KurtIntenCh3&quot;           
## [15] &quot;KurtIntenCh4&quot;            &quot;NeighborMinDistCh1&quot;     
## [17] &quot;NeighborVarDistCh1&quot;      &quot;ShapeBFRCh1&quot;            
## [19] &quot;ShapeLWRCh1&quot;             &quot;SpotFiberCountCh3&quot;      
## [21] &quot;TotalIntenCh1&quot;           &quot;TotalIntenCh2&quot;          
## [23] &quot;VarIntenCh4&quot;</code></pre>
<p>Let’s predict our test set using our final model.</p>
<div class="sourceCode" id="cb320"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb320-1" data-line-number="1">segDataTest &lt;-<span class="st"> </span><span class="kw">predict</span>(transformations, segDataTest)</a>
<a class="sourceLine" id="cb320-2" data-line-number="2">test_pred &lt;-<span class="st"> </span><span class="kw">predict</span>(knnFit, segDataTest)</a>
<a class="sourceLine" id="cb320-3" data-line-number="3"><span class="kw">confusionMatrix</span>(test_pred, segClassTest)</a></code></pre></div>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction  PS  WS
##         PS 557  92
##         WS  93 267
##                                           
##                Accuracy : 0.8167          
##                  95% CI : (0.7914, 0.8401)
##     No Information Rate : 0.6442          
##     P-Value [Acc &gt; NIR] : &lt;2e-16          
##                                           
##                   Kappa : 0.6003          
##                                           
##  Mcnemar&#39;s Test P-Value : 1               
##                                           
##             Sensitivity : 0.8569          
##             Specificity : 0.7437          
##          Pos Pred Value : 0.8582          
##          Neg Pred Value : 0.7417          
##              Prevalence : 0.6442          
##          Detection Rate : 0.5520          
##    Detection Prevalence : 0.6432          
##       Balanced Accuracy : 0.8003          
##                                           
##        &#39;Positive&#39; Class : PS              
## </code></pre>
</div>
</div>
<div id="knn-regression" class="section level2 hasAnchor">
<h2><span class="header-section-number">5.5</span> Regression<a href="nearest-neighbours.html#knn-regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><em>k</em>-nn can also be applied to the problem of regression as we will see in the following example. The <strong>BloodBrain</strong> dataset in the <a href="http://cran.r-project.org/web/packages/caret/index.html">caret</a> package contains data on 208 chemical compounds, organized in two objects:</p>
<ul>
<li><strong>logBBB</strong> - a vector of the log ratio of the concentration of a chemical compound in the brain and the concentration in the blood.</li>
<li><strong>bbbDescr</strong> - a data frame of 134 molecular descriptors of the compounds.</li>
</ul>
<p>We’ll start by loading the data.</p>
<div class="sourceCode" id="cb322"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb322-1" data-line-number="1"><span class="kw">data</span>(BloodBrain)</a>
<a class="sourceLine" id="cb322-2" data-line-number="2"><span class="kw">str</span>(bbbDescr)</a></code></pre></div>
<pre><code>## &#39;data.frame&#39;:    208 obs. of  134 variables:
##  $ tpsa                : num  12 49.3 50.5 37.4 37.4 ...
##  $ nbasic              : int  1 0 1 0 1 1 1 1 1 1 ...
##  $ negative            : int  0 0 0 0 0 0 0 0 0 0 ...
##  $ vsa_hyd             : num  167.1 92.6 295.2 319.1 299.7 ...
##  $ a_aro               : int  0 6 15 15 12 11 6 12 12 6 ...
##  $ weight              : num  156 151 366 383 326 ...
##  $ peoe_vsa.0          : num  76.9 38.2 58.1 62.2 74.8 ...
##  $ peoe_vsa.1          : num  43.4 25.5 124.7 124.7 118 ...
##  $ peoe_vsa.2          : num  0 0 21.7 13.2 33 ...
##  $ peoe_vsa.3          : num  0 8.62 8.62 21.79 0 ...
##  $ peoe_vsa.4          : num  0 23.3 17.4 0 0 ...
##  $ peoe_vsa.5          : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ peoe_vsa.6          : num  17.24 0 8.62 8.62 8.62 ...
##  $ peoe_vsa.0.1        : num  18.7 49 83.8 83.8 83.8 ...
##  $ peoe_vsa.1.1        : num  43.5 0 49 68.8 36.8 ...
##  $ peoe_vsa.2.1        : num  0 0 0 0 0 ...
##  $ peoe_vsa.3.1        : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ peoe_vsa.4.1        : num  0 0 5.68 5.68 5.68 ...
##  $ peoe_vsa.5.1        : num  0 13.567 2.504 0 0.137 ...
##  $ peoe_vsa.6.1        : num  0 7.9 2.64 2.64 2.5 ...
##  $ a_acc               : int  0 2 2 2 2 2 2 2 0 2 ...
##  $ a_acid              : int  0 0 0 0 0 0 0 0 0 0 ...
##  $ a_base              : int  1 0 1 1 1 1 1 1 1 1 ...
##  $ vsa_acc             : num  0 13.57 8.19 8.19 8.19 ...
##  $ vsa_acid            : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ vsa_base            : num  5.68 0 0 0 0 ...
##  $ vsa_don             : num  5.68 5.68 5.68 5.68 5.68 ...
##  $ vsa_other           : num  0 28.1 43.6 28.3 19.6 ...
##  $ vsa_pol             : num  0 13.6 0 0 0 ...
##  $ slogp_vsa0          : num  18 25.4 14.1 14.1 14.1 ...
##  $ slogp_vsa1          : num  0 23.3 34.8 34.8 34.8 ...
##  $ slogp_vsa2          : num  3.98 23.86 0 0 0 ...
##  $ slogp_vsa3          : num  0 0 76.2 76.2 76.2 ...
##  $ slogp_vsa4          : num  4.41 0 3.19 3.19 3.19 ...
##  $ slogp_vsa5          : num  32.9 0 9.51 0 0 ...
##  $ slogp_vsa6          : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ slogp_vsa7          : num  0 70.6 148.1 144 140.7 ...
##  $ slogp_vsa8          : num  113.2 0 75.5 75.5 75.5 ...
##  $ slogp_vsa9          : num  33.3 41.3 28.3 55.5 26 ...
##  $ smr_vsa0            : num  0 23.86 12.63 3.12 3.12 ...
##  $ smr_vsa1            : num  18 25.4 27.8 27.8 27.8 ...
##  $ smr_vsa2            : num  4.41 0 0 0 0 ...
##  $ smr_vsa3            : num  3.98 5.24 8.43 8.43 8.43 ...
##  $ smr_vsa4            : num  0 20.8 29.6 21.4 20.3 ...
##  $ smr_vsa5            : num  113.2 70.6 235.1 235.1 234.6 ...
##  $ smr_vsa6            : num  0 5.26 76.25 76.25 76.25 ...
##  $ smr_vsa7            : num  66.2 33.3 0 31.3 0 ...
##  $ tpsa.1              : num  16.6 49.3 51.7 38.6 38.6 ...
##  $ logp.o.w.           : num  2.948 0.889 4.439 5.254 3.8 ...
##  $ frac.anion7.        : num  0 0.001 0 0 0 0 0.001 0 0 0 ...
##  $ frac.cation7.       : num  0.999 0 0.986 0.986 0.986 0.986 0.996 0.946 0.999 0.976 ...
##  $ andrewbind          : num  3.4 -3.3 12.8 12.8 10.3 10 10.4 15.9 12.9 9.5 ...
##  $ rotatablebonds      : int  3 2 8 8 8 8 8 7 4 5 ...
##  $ mlogp               : num  2.5 1.06 4.66 3.82 3.27 ...
##  $ clogp               : num  2.97 0.494 5.137 5.878 4.367 ...
##  $ mw                  : num  155 151 365 382 325 ...
##  $ nocount             : int  1 3 5 4 4 4 4 3 2 4 ...
##  $ hbdnr               : int  1 2 1 1 1 1 2 1 1 0 ...
##  $ rule.of.5violations : int  0 0 1 1 0 0 0 0 1 0 ...
##  $ alert               : int  0 0 0 0 0 0 0 0 0 0 ...
##  $ prx                 : int  0 1 6 2 2 2 1 0 0 4 ...
##  $ ub                  : num  0 3 5.3 5.3 4.2 3.6 3 4.7 4.2 3 ...
##  $ pol                 : int  0 2 3 3 2 2 2 3 4 1 ...
##  $ inthb               : int  0 0 0 0 0 0 1 0 0 0 ...
##  $ adistm              : num  0 395 1365 703 746 ...
##  $ adistd              : num  0 10.9 25.7 10 10.6 ...
##  $ polar_area          : num  21.1 117.4 82.1 65.1 66.2 ...
##  $ nonpolar_area       : num  379 248 638 668 602 ...
##  $ psa_npsa            : num  0.0557 0.4743 0.1287 0.0974 0.11 ...
##  $ tcsa                : num  0.0097 0.0134 0.0111 0.0108 0.0118 0.0111 0.0123 0.0099 0.0106 0.0115 ...
##  $ tcpa                : num  0.1842 0.0417 0.0972 0.1218 0.1186 ...
##  $ tcnp                : num  0.0103 0.0198 0.0125 0.0119 0.013 0.0125 0.0162 0.011 0.0109 0.0122 ...
##  $ ovality             : num  1.1 1.12 1.3 1.3 1.27 ...
##  $ surface_area        : num  400 365 720 733 668 ...
##  $ volume              : num  656 555 1224 1257 1133 ...
##  $ most_negative_charge: num  -0.617 -0.84 -0.801 -0.761 -0.857 ...
##  $ most_positive_charge: num  0.307 0.497 0.541 0.48 0.455 ...
##  $ sum_absolute_charge : num  3.89 4.89 7.98 7.93 7.85 ...
##  $ dipole_moment       : num  1.19 4.21 3.52 3.15 3.27 ...
##  $ homo                : num  -9.67 -8.96 -8.63 -8.56 -8.67 ...
##  $ lumo                : num  3.4038 0.1942 0.0589 -0.2651 0.3149 ...
##  $ hardness            : num  6.54 4.58 4.34 4.15 4.49 ...
##  $ ppsa1               : num  349 223 518 508 509 ...
##  $ ppsa2               : num  679 546 2066 2013 1999 ...
##  $ ppsa3               : num  31 42.3 64 61.7 61.6 ...
##  $ pnsa1               : num  51.1 141.8 202 225.4 158.8 ...
##  $ pnsa2               : num  -99.3 -346.9 -805.9 -894 -623.3 ...
##  $ pnsa3               : num  -10.5 -44 -43.8 -42 -39.8 ...
##  $ fpsa1               : num  0.872 0.611 0.719 0.693 0.762 ...
##  $ fpsa2               : num  1.7 1.5 2.87 2.75 2.99 ...
##  $ fpsa3               : num  0.0774 0.1159 0.0888 0.0842 0.0922 ...
##  $ fnsa1               : num  0.128 0.389 0.281 0.307 0.238 ...
##  $ fnsa2               : num  -0.248 -0.951 -1.12 -1.22 -0.933 ...
##  $ fnsa3               : num  -0.0262 -0.1207 -0.0608 -0.0573 -0.0596 ...
##  $ wpsa1               : num  139.7 81.4 372.7 372.1 340.1 ...
##  $ wpsa2               : num  272 199 1487 1476 1335 ...
##  $ wpsa3               : num  12.4 15.4 46 45.2 41.1 ...
##  $ wnsa1               : num  20.4 51.8 145.4 165.3 106 ...
##  $ wnsa2               : num  -39.8 -126.6 -580.1 -655.3 -416.3 ...
##   [list output truncated]</code></pre>
<div class="sourceCode" id="cb324"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb324-1" data-line-number="1"><span class="kw">str</span>(logBBB)</a></code></pre></div>
<pre><code>##  num [1:208] 1.08 -0.4 0.22 0.14 0.69 0.44 -0.43 1.38 0.75 0.88 ...</code></pre>
<p>Evidently the variables are on different scales which is problematic for <em>k</em>-nn.</p>
<div id="partition-data" class="section level3 hasAnchor">
<h3><span class="header-section-number">5.5.1</span> Partition data<a href="nearest-neighbours.html#partition-data" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Before proceeding the data set must be partitioned into a training and a test set.</p>
<div class="sourceCode" id="cb326"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb326-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">42</span>)</a>
<a class="sourceLine" id="cb326-2" data-line-number="2">trainIndex &lt;-<span class="st"> </span><span class="kw">createDataPartition</span>(<span class="dt">y=</span>logBBB, <span class="dt">times=</span><span class="dv">1</span>, <span class="dt">p=</span><span class="fl">0.8</span>, <span class="dt">list=</span>F)</a>
<a class="sourceLine" id="cb326-3" data-line-number="3">descrTrain &lt;-<span class="st"> </span>bbbDescr[trainIndex,]</a>
<a class="sourceLine" id="cb326-4" data-line-number="4">concRatioTrain &lt;-<span class="st"> </span>logBBB[trainIndex]</a>
<a class="sourceLine" id="cb326-5" data-line-number="5">descrTest &lt;-<span class="st"> </span>bbbDescr[<span class="op">-</span>trainIndex,]</a>
<a class="sourceLine" id="cb326-6" data-line-number="6">concRatioTest &lt;-<span class="st"> </span>logBBB[<span class="op">-</span>trainIndex]</a></code></pre></div>
</div>
<div id="data-pre-processing" class="section level3 hasAnchor">
<h3><span class="header-section-number">5.5.2</span> Data pre-processing<a href="nearest-neighbours.html#data-pre-processing" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Are there any issues with the data that might affect model fitting? Let’s start by considering correlation.</p>
<div class="sourceCode" id="cb327"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb327-1" data-line-number="1">cm &lt;-<span class="st"> </span><span class="kw">cor</span>(descrTrain)</a>
<a class="sourceLine" id="cb327-2" data-line-number="2"><span class="kw">corrplot</span>(cm, <span class="dt">order=</span><span class="st">&quot;hclust&quot;</span>, <span class="dt">tl.pos=</span><span class="st">&quot;n&quot;</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:compoundDescriptorsCorrelogram"></span>
<img src="04-nearest-neighbours_files/figure-html/compoundDescriptorsCorrelogram-1.png" alt="Correlogram of the chemical compound descriptors." width="80%" />
<p class="caption">
Figure 5.12: Correlogram of the chemical compound descriptors.
</p>
</div>
<p>The number of variables exhibiting a pair-wise correlation coefficient above 0.75 can be determined:</p>
<div class="sourceCode" id="cb328"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb328-1" data-line-number="1">highCorr &lt;-<span class="st"> </span><span class="kw">findCorrelation</span>(cm, <span class="dt">cutoff=</span><span class="fl">0.75</span>)</a>
<a class="sourceLine" id="cb328-2" data-line-number="2"><span class="kw">length</span>(highCorr)</a></code></pre></div>
<pre><code>## [1] 65</code></pre>
<p>A check for the presence of missing values:</p>
<div class="sourceCode" id="cb330"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb330-1" data-line-number="1"><span class="kw">anyNA</span>(descrTrain)</a></code></pre></div>
<pre><code>## [1] FALSE</code></pre>
<p>Detection of near zero variance predictors:</p>
<div class="sourceCode" id="cb332"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb332-1" data-line-number="1"><span class="kw">nearZeroVar</span>(descrTrain)</a></code></pre></div>
<pre><code>## [1]  3 16 17 22 25 50 60</code></pre>
<p>We know there are issues with scaling, and the presence of highly correlated predictors and near zero variance predictors. These problems are resolved by pre-processing. First we define the procesing steps.</p>
<div class="sourceCode" id="cb334"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb334-1" data-line-number="1">transformations &lt;-<span class="st"> </span><span class="kw">preProcess</span>(descrTrain,</a>
<a class="sourceLine" id="cb334-2" data-line-number="2">                              <span class="dt">method=</span><span class="kw">c</span>(<span class="st">&quot;center&quot;</span>, <span class="st">&quot;scale&quot;</span>, <span class="st">&quot;corr&quot;</span>, <span class="st">&quot;nzv&quot;</span>),</a>
<a class="sourceLine" id="cb334-3" data-line-number="3">                              <span class="dt">cutoff=</span><span class="fl">0.75</span>)</a></code></pre></div>
<p>Then this transformation can be applied to the compound descriptor data set.</p>
<div class="sourceCode" id="cb335"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb335-1" data-line-number="1">descrTrain &lt;-<span class="st"> </span><span class="kw">predict</span>(transformations, descrTrain)</a></code></pre></div>
</div>
<div id="search-for-optimum-k" class="section level3 hasAnchor">
<h3><span class="header-section-number">5.5.3</span> Search for optimum <em>k</em><a href="nearest-neighbours.html#search-for-optimum-k" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The optimum value of <em>k</em> can be found by cross-validation, following similar methodology to that used to find the best <em>k</em> for classification. We’ll start by generating seeds to make this example reproducible.</p>
<div class="sourceCode" id="cb336"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb336-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">42</span>)</a>
<a class="sourceLine" id="cb336-2" data-line-number="2">seeds &lt;-<span class="st"> </span><span class="kw">vector</span>(<span class="dt">mode =</span> <span class="st">&quot;list&quot;</span>, <span class="dt">length =</span> <span class="dv">26</span>)</a>
<a class="sourceLine" id="cb336-3" data-line-number="3"><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">25</span>) seeds[[i]] &lt;-<span class="st"> </span><span class="kw">sample.int</span>(<span class="dv">1000</span>, <span class="dv">10</span>)</a>
<a class="sourceLine" id="cb336-4" data-line-number="4">seeds[[<span class="dv">26</span>]] &lt;-<span class="st"> </span><span class="kw">sample.int</span>(<span class="dv">1000</span>,<span class="dv">1</span>)</a></code></pre></div>
<p>Ten values of <em>k</em> will be evaluated using 5 repeats of 5-fold cross-validation.</p>
<div class="sourceCode" id="cb337"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb337-1" data-line-number="1">knnTune &lt;-<span class="st"> </span><span class="kw">train</span>(descrTrain,</a>
<a class="sourceLine" id="cb337-2" data-line-number="2">                 concRatioTrain,</a>
<a class="sourceLine" id="cb337-3" data-line-number="3">                 <span class="dt">method=</span><span class="st">&quot;knn&quot;</span>,</a>
<a class="sourceLine" id="cb337-4" data-line-number="4">                 <span class="dt">tuneGrid =</span> <span class="kw">data.frame</span>(<span class="dt">.k=</span><span class="dv">1</span><span class="op">:</span><span class="dv">10</span>),</a>
<a class="sourceLine" id="cb337-5" data-line-number="5">                 <span class="dt">trControl =</span> <span class="kw">trainControl</span>(<span class="dt">method=</span><span class="st">&quot;repeatedcv&quot;</span>,</a>
<a class="sourceLine" id="cb337-6" data-line-number="6">                                          <span class="dt">number =</span> <span class="dv">5</span>,</a>
<a class="sourceLine" id="cb337-7" data-line-number="7">                                          <span class="dt">repeats =</span> <span class="dv">5</span>,</a>
<a class="sourceLine" id="cb337-8" data-line-number="8">                                          <span class="dt">seeds=</span>seeds,</a>
<a class="sourceLine" id="cb337-9" data-line-number="9">                                          <span class="dt">preProcOptions=</span><span class="kw">list</span>(<span class="dt">cutoff=</span><span class="fl">0.75</span>))</a>
<a class="sourceLine" id="cb337-10" data-line-number="10">                 )</a>
<a class="sourceLine" id="cb337-11" data-line-number="11"></a>
<a class="sourceLine" id="cb337-12" data-line-number="12">knnTune</a></code></pre></div>
<pre><code>## k-Nearest Neighbors 
## 
## 168 samples
##  63 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold, repeated 5 times) 
## Summary of sample sizes: 136, 133, 134, 134, 135, 135, ... 
## Resampling results across tuning parameters:
## 
##   k   RMSE       Rsquared   MAE      
##    1  0.6572204  0.4094071  0.4739994
##    2  0.5838867  0.4737129  0.4319682
##    3  0.5869985  0.4632044  0.4377774
##    4  0.6023137  0.4313942  0.4554854
##    5  0.6119249  0.4130384  0.4653676
##    6  0.6134509  0.4086748  0.4710572
##    7  0.6289054  0.3803895  0.4846857
##    8  0.6331586  0.3748235  0.4843006
##    9  0.6358699  0.3744272  0.4835743
##   10  0.6321851  0.3859343  0.4805353
## 
## RMSE was used to select the optimal model using the smallest value.
## The final value used for the model was k = 2.</code></pre>
<p>The Root Mean Squared Error (RMSE) measures the differences between the values predicted by the model and the values actually observed. More specifically, it represents the sample standard deviation of the difference between the predicted values and observed values.</p>
<div class="sourceCode" id="cb339"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb339-1" data-line-number="1"><span class="kw">plot</span>(knnTune)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:rmseFunK"></span>
<img src="04-nearest-neighbours_files/figure-html/rmseFunK-1.png" alt="Root Mean Squared Error as a function of neighbourhood size." width="100%" />
<p class="caption">
Figure 5.13: Root Mean Squared Error as a function of neighbourhood size.
</p>
</div>
</div>
<div id="use-model-to-make-predictions" class="section level3 hasAnchor">
<h3><span class="header-section-number">5.5.4</span> Use model to make predictions<a href="nearest-neighbours.html#use-model-to-make-predictions" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Before attempting to predict the blood/brain concentration ratios of the test samples, the descriptors in the test set must be transformed using the same pre-processing procedure that was applied to the descriptors in the training set.</p>
<div class="sourceCode" id="cb340"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb340-1" data-line-number="1">descrTest &lt;-<span class="st"> </span><span class="kw">predict</span>(transformations, descrTest)</a></code></pre></div>
<p>Use model to predict outcomes (concentration ratios) of the test set.</p>
<div class="sourceCode" id="cb341"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb341-1" data-line-number="1">test_pred &lt;-<span class="st"> </span><span class="kw">predict</span>(knnTune, descrTest)</a></code></pre></div>
<p>Prediction performance can be visualized in a scatterplot.</p>
<div class="sourceCode" id="cb342"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb342-1" data-line-number="1"><span class="kw">qplot</span>(concRatioTest, test_pred) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb342-2" data-line-number="2"><span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;observed&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb342-3" data-line-number="3"><span class="st">  </span><span class="kw">ylab</span>(<span class="st">&quot;predicted&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb342-4" data-line-number="4"><span class="st">  </span><span class="kw">theme_bw</span>()</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:obsPredConcRatios"></span>
<img src="04-nearest-neighbours_files/figure-html/obsPredConcRatios-1.png" alt="Concordance between observed concentration ratios and those predicted by _k_-nn regression." width="80%" />
<p class="caption">
Figure 5.14: Concordance between observed concentration ratios and those predicted by <em>k</em>-nn regression.
</p>
</div>
<p>We can also measure correlation between observed and predicted values.</p>
<div class="sourceCode" id="cb343"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb343-1" data-line-number="1"><span class="kw">cor</span>(concRatioTest, test_pred)</a></code></pre></div>
<pre><code>## [1] 0.6386877</code></pre>
</div>
</div>
<div id="exercises-1" class="section level2 hasAnchor">
<h2><span class="header-section-number">5.6</span> Exercises<a href="nearest-neighbours.html#exercises-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="knnEx1" class="section level3 hasAnchor">
<h3><span class="header-section-number">5.6.1</span> Exercise 1<a href="nearest-neighbours.html#knnEx1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The seeds data set <a href="https://archive.ics.uci.edu/ml/datasets/seeds" class="uri">https://archive.ics.uci.edu/ml/datasets/seeds</a> contains morphological measurements on the kernels of three varieties of wheat: Kama, Rosa and Canadian.</p>
<p>Load the data into your R session using:</p>
<div class="sourceCode" id="cb345"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb345-1" data-line-number="1"><span class="kw">load</span>(<span class="st">&quot;data/wheat_seeds/wheat_seeds.Rda&quot;</span>)</a></code></pre></div>
<p>The data are split into two objects. <strong>morphometrics</strong> is a data.frame containing the morphological measurements:</p>
<div class="sourceCode" id="cb346"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb346-1" data-line-number="1"><span class="kw">str</span>(morphometrics)</a></code></pre></div>
<pre><code>## &#39;data.frame&#39;:    210 obs. of  7 variables:
##  $ area        : num  15.3 14.9 14.3 13.8 16.1 ...
##  $ perimeter   : num  14.8 14.6 14.1 13.9 15 ...
##  $ compactness : num  0.871 0.881 0.905 0.895 0.903 ...
##  $ kernLength  : num  5.76 5.55 5.29 5.32 5.66 ...
##  $ kernWidth   : num  3.31 3.33 3.34 3.38 3.56 ...
##  $ asymCoef    : num  2.22 1.02 2.7 2.26 1.35 ...
##  $ grooveLength: num  5.22 4.96 4.83 4.8 5.17 ...</code></pre>
<p><strong>variety</strong> is a factor containing the corresponding classes:</p>
<div class="sourceCode" id="cb348"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb348-1" data-line-number="1"><span class="kw">str</span>(variety)</a></code></pre></div>
<pre><code>##  Factor w/ 3 levels &quot;Canadian&quot;,&quot;Kama&quot;,..: 2 2 2 2 2 2 2 2 2 2 ...</code></pre>
<p>Your task is to build a <em>k</em>-nn classifier which will predict the variety of wheat from a seeds morphological measurements. You do not need to perform feature selection, but you will want to pre-process the data.</p>
<p>Solutions to exercises can be found in appendix <a href="solutions-nearest-neighbours.html#solutions-nearest-neighbours">D</a>.</p>

</div>
</div>
</div>
<h3><span class="header-section-number">K</span> Solutions for use case 2<a href="use-case-2-solutions.html#use-case-2-solutions" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references">
<div id="ref-BoxCox">
<p>Box, G. E. P., and D. R. Cox. 1964. “An analysis of transformations (with discussion).” <em>Journal of the Royal Statistical Society B</em> 26: 211–52.</p>
</div>
<div id="ref-Hill2007">
<p>Hill, Andrew A., Peter LaPan, Yizheng Li, and Steve Haney. 2007. “Impact of Image Segmentation on High-Content Screening Data Quality for Sk-Br-3 Cells.” <em>BMC Bioinformatics</em> 8 (1): 340. <a href="https://doi.org/10.1186/1471-2105-8-340" class="uri">https://doi.org/10.1186/1471-2105-8-340</a>.</p>
</div>
<div id="ref-Kuhn2013">
<p>Kuhn, Max, and Kjell Johnson. 2013. <em>Applied Predictive Modeling</em>. New York: Springer.</p>
</div>
<div id="ref-YeoJohnson">
<p>Yeo, I. K., and R. Johnson. 2000. “A new family of power transformations to improve normality or symmetry.” <em>Biometrika</em> 87: 954–59.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="clustering.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="svm.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
