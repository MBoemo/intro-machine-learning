<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>7 Decision trees and random forests | An Introduction to Machine Learning</title>
  <meta name="description" content="Course materials for An Introduction to Machine Learning" />
  <meta name="generator" content="bookdown 0.31 and GitBook 2.6.7" />

  <meta property="og:title" content="7 Decision trees and random forests | An Introduction to Machine Learning" />
  <meta property="og:type" content="book" />
  <meta property="og:image" content="/figures/cover_image.png" />
  <meta property="og:description" content="Course materials for An Introduction to Machine Learning" />
  <meta name="github-repo" content="bioinformatics-training/intro-machine-learning-2019" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="7 Decision trees and random forests | An Introduction to Machine Learning" />
  
  <meta name="twitter:description" content="Course materials for An Introduction to Machine Learning" />
  <meta name="twitter:image" content="/figures/cover_image.png" />

<meta name="author" content="Sudhakaran Prabakaran, Matt Wayland and Chris Penfold" />


<meta name="date" content="2023-01-15" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="svm.html"/>
<link rel="next" href="use-case-1.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> About the course</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#overview"><i class="fa fa-check"></i><b>1.1</b> Overview</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#registration"><i class="fa fa-check"></i><b>1.2</b> Registration</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#prerequisites"><i class="fa fa-check"></i><b>1.3</b> Prerequisites</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#github"><i class="fa fa-check"></i><b>1.4</b> Github</a></li>
<li class="chapter" data-level="1.5" data-path="index.html"><a href="index.html#license"><i class="fa fa-check"></i><b>1.5</b> License</a></li>
<li class="chapter" data-level="1.6" data-path="index.html"><a href="index.html#contact"><i class="fa fa-check"></i><b>1.6</b> Contact</a></li>
<li class="chapter" data-level="1.7" data-path="index.html"><a href="index.html#colophon"><i class="fa fa-check"></i><b>1.7</b> Colophon</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>2</b> Introduction</a><ul>
<li class="chapter" data-level="2.1" data-path="intro.html"><a href="intro.html#what-is-machine-learning"><i class="fa fa-check"></i><b>2.1</b> What is machine learning?</a></li>
<li class="chapter" data-level="2.2" data-path="intro.html"><a href="intro.html#aspects-of-ml"><i class="fa fa-check"></i><b>2.2</b> Aspects of ML</a></li>
<li class="chapter" data-level="2.3" data-path="intro.html"><a href="intro.html#what-actually-happened-under-the-hood"><i class="fa fa-check"></i><b>2.3</b> What actually happened under the hood</a></li>
<li class="chapter" data-level="2.4" data-path="intro.html"><a href="intro.html#introduction-to-caret"><i class="fa fa-check"></i><b>2.4</b> Introduction to CARET</a><ul>
<li class="chapter" data-level="2.4.1" data-path="intro.html"><a href="intro.html#preprocessing-with-the-iris-dataset"><i class="fa fa-check"></i><b>2.4.1</b> Preprocessing with the Iris dataset</a></li>
<li class="chapter" data-level="2.4.2" data-path="intro.html"><a href="intro.html#training-different-types-of-models"><i class="fa fa-check"></i><b>2.4.2</b> Training different types of models</a></li>
<li class="chapter" data-level="2.4.3" data-path="intro.html"><a href="intro.html#cross-validation"><i class="fa fa-check"></i><b>2.4.3</b> Cross-validation</a></li>
<li class="chapter" data-level="2.4.4" data-path="intro.html"><a href="intro.html#optimising-hyperparameters"><i class="fa fa-check"></i><b>2.4.4</b> Optimising hyperparameters</a></li>
<li class="chapter" data-level="2.4.5" data-path="intro.html"><a href="intro.html#using-dummy-variables-with-the-sacramento-dataset"><i class="fa fa-check"></i><b>2.4.5</b> Using dummy variables with the Sacramento dataset</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html"><i class="fa fa-check"></i><b>3</b> Dimensionality reduction</a><ul>
<li class="chapter" data-level="3.1" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html#linear-dimensionality-reduction"><i class="fa fa-check"></i><b>3.1</b> Linear Dimensionality Reduction</a><ul>
<li class="chapter" data-level="3.1.1" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html#interpreting-the-principle-component-axes"><i class="fa fa-check"></i><b>3.1.1</b> Interpreting the Principle Component Axes</a></li>
<li class="chapter" data-level="3.1.2" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html#horseshoe-effect"><i class="fa fa-check"></i><b>3.1.2</b> Horseshoe effect</a></li>
<li class="chapter" data-level="3.1.3" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html#pca-analysis-of-mammalian-development"><i class="fa fa-check"></i><b>3.1.3</b> PCA analysis of mammalian development</a></li>
<li class="chapter" data-level="3.1.4" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html#biological-interpretation"><i class="fa fa-check"></i><b>3.1.4</b> Biological interpretation</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html#nonlinear-dimensionality-reduction"><i class="fa fa-check"></i><b>3.2</b> Nonlinear Dimensionality Reduction</a><ul>
<li class="chapter" data-level="3.2.1" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html#nonlinear-warping"><i class="fa fa-check"></i><b>3.2.1</b> Nonlinear warping</a></li>
<li class="chapter" data-level="3.2.2" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html#stochasticity"><i class="fa fa-check"></i><b>3.2.2</b> Stochasticity</a></li>
<li class="chapter" data-level="3.2.3" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html#analysis-of-mammalian-development"><i class="fa fa-check"></i><b>3.2.3</b> Analysis of mammalian development</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html#other-dimensionality-reduction-techniques"><i class="fa fa-check"></i><b>3.3</b> Other dimensionality reduction techniques</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="clustering.html"><a href="clustering.html"><i class="fa fa-check"></i><b>4</b> Clustering</a><ul>
<li class="chapter" data-level="4.1" data-path="clustering.html"><a href="clustering.html#introduction"><i class="fa fa-check"></i><b>4.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2" data-path="clustering.html"><a href="clustering.html#distance-metrics"><i class="fa fa-check"></i><b>4.2</b> Distance metrics</a></li>
<li class="chapter" data-level="4.3" data-path="clustering.html"><a href="clustering.html#hierarchic-agglomerative"><i class="fa fa-check"></i><b>4.3</b> Hierarchic agglomerative</a><ul>
<li class="chapter" data-level="4.3.1" data-path="clustering.html"><a href="clustering.html#linkage-algorithms"><i class="fa fa-check"></i><b>4.3.1</b> Linkage algorithms</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="clustering.html"><a href="clustering.html#k-means"><i class="fa fa-check"></i><b>4.4</b> K-means</a><ul>
<li class="chapter" data-level="4.4.1" data-path="clustering.html"><a href="clustering.html#algorithm"><i class="fa fa-check"></i><b>4.4.1</b> Algorithm</a></li>
<li class="chapter" data-level="4.4.2" data-path="clustering.html"><a href="clustering.html#choosing-initial-cluster-centres"><i class="fa fa-check"></i><b>4.4.2</b> Choosing initial cluster centres</a></li>
<li class="chapter" data-level="4.4.3" data-path="clustering.html"><a href="clustering.html#choosingK"><i class="fa fa-check"></i><b>4.4.3</b> Choosing k</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="clustering.html"><a href="clustering.html#dbscan"><i class="fa fa-check"></i><b>4.5</b> DBSCAN</a><ul>
<li class="chapter" data-level="4.5.1" data-path="clustering.html"><a href="clustering.html#algorithm-1"><i class="fa fa-check"></i><b>4.5.1</b> Algorithm</a></li>
<li class="chapter" data-level="4.5.2" data-path="clustering.html"><a href="clustering.html#implementation-in-r"><i class="fa fa-check"></i><b>4.5.2</b> Implementation in R</a></li>
<li class="chapter" data-level="4.5.3" data-path="clustering.html"><a href="clustering.html#choosing-parameters"><i class="fa fa-check"></i><b>4.5.3</b> Choosing parameters</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="clustering.html"><a href="clustering.html#example-clustering-synthetic-data-sets"><i class="fa fa-check"></i><b>4.6</b> Example: clustering synthetic data sets</a><ul>
<li class="chapter" data-level="4.6.1" data-path="clustering.html"><a href="clustering.html#hierarchic-agglomerative-1"><i class="fa fa-check"></i><b>4.6.1</b> Hierarchic agglomerative</a></li>
<li class="chapter" data-level="4.6.2" data-path="clustering.html"><a href="clustering.html#k-means-1"><i class="fa fa-check"></i><b>4.6.2</b> K-means</a></li>
<li class="chapter" data-level="4.6.3" data-path="clustering.html"><a href="clustering.html#dbscan-1"><i class="fa fa-check"></i><b>4.6.3</b> DBSCAN</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="clustering.html"><a href="clustering.html#evaluating-cluster-quality"><i class="fa fa-check"></i><b>4.7</b> Evaluating cluster quality</a><ul>
<li class="chapter" data-level="4.7.1" data-path="clustering.html"><a href="clustering.html#silhouetteMethod"><i class="fa fa-check"></i><b>4.7.1</b> Silhouette method</a></li>
<li class="chapter" data-level="4.7.2" data-path="clustering.html"><a href="clustering.html#example---k-means-clustering-of-blobs-data-set"><i class="fa fa-check"></i><b>4.7.2</b> Example - k-means clustering of blobs data set</a></li>
<li class="chapter" data-level="4.7.3" data-path="clustering.html"><a href="clustering.html#example---dbscan-clustering-of-noisy-moons"><i class="fa fa-check"></i><b>4.7.3</b> Example - DBSCAN clustering of noisy moons</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="clustering.html"><a href="clustering.html#example-gene-expression-profiling-of-human-tissues"><i class="fa fa-check"></i><b>4.8</b> Example: gene expression profiling of human tissues</a><ul>
<li class="chapter" data-level="4.8.1" data-path="clustering.html"><a href="clustering.html#hierarchic-agglomerative-2"><i class="fa fa-check"></i><b>4.8.1</b> Hierarchic agglomerative</a></li>
<li class="chapter" data-level="4.8.2" data-path="clustering.html"><a href="clustering.html#k-means-2"><i class="fa fa-check"></i><b>4.8.2</b> K-means</a></li>
<li class="chapter" data-level="4.8.3" data-path="clustering.html"><a href="clustering.html#dbscan-2"><i class="fa fa-check"></i><b>4.8.3</b> DBSCAN</a></li>
</ul></li>
<li class="chapter" data-level="4.9" data-path="clustering.html"><a href="clustering.html#exercises"><i class="fa fa-check"></i><b>4.9</b> Exercises</a><ul>
<li class="chapter" data-level="4.9.1" data-path="clustering.html"><a href="clustering.html#clusteringEx1"><i class="fa fa-check"></i><b>4.9.1</b> Exercise 1</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html"><i class="fa fa-check"></i><b>5</b> Nearest neighbours</a><ul>
<li class="chapter" data-level="5.1" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#introduction-1"><i class="fa fa-check"></i><b>5.1</b> Introduction</a></li>
<li class="chapter" data-level="5.2" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#classification-simulated-data"><i class="fa fa-check"></i><b>5.2</b> Classification: simulated data</a><ul>
<li class="chapter" data-level="5.2.1" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#knn-function"><i class="fa fa-check"></i><b>5.2.1</b> knn function</a></li>
<li class="chapter" data-level="5.2.2" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#plotting-decision-boundaries"><i class="fa fa-check"></i><b>5.2.2</b> Plotting decision boundaries</a></li>
<li class="chapter" data-level="5.2.3" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#bias-variance-tradeoff"><i class="fa fa-check"></i><b>5.2.3</b> Bias-variance tradeoff</a></li>
<li class="chapter" data-level="5.2.4" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#choosing-k"><i class="fa fa-check"></i><b>5.2.4</b> Choosing <em>k</em></a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#example-on-the-iris-dataset"><i class="fa fa-check"></i><b>5.3</b> Example on the Iris dataset</a></li>
<li class="chapter" data-level="5.4" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#knn-cell-segmentation"><i class="fa fa-check"></i><b>5.4</b> Classification: cell segmentation</a><ul>
<li class="chapter" data-level="5.4.1" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#cell-segmentation-data-set"><i class="fa fa-check"></i><b>5.4.1</b> Cell segmentation data set</a></li>
<li class="chapter" data-level="5.4.2" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#data-splitting"><i class="fa fa-check"></i><b>5.4.2</b> Data splitting</a></li>
<li class="chapter" data-level="5.4.3" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#identification-of-data-quality-issues"><i class="fa fa-check"></i><b>5.4.3</b> Identification of data quality issues</a></li>
<li class="chapter" data-level="5.4.4" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#fit-model"><i class="fa fa-check"></i><b>5.4.4</b> Fit model</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#knn-regression"><i class="fa fa-check"></i><b>5.5</b> Regression</a><ul>
<li class="chapter" data-level="5.5.1" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#partition-data"><i class="fa fa-check"></i><b>5.5.1</b> Partition data</a></li>
<li class="chapter" data-level="5.5.2" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#data-pre-processing"><i class="fa fa-check"></i><b>5.5.2</b> Data pre-processing</a></li>
<li class="chapter" data-level="5.5.3" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#search-for-optimum-k"><i class="fa fa-check"></i><b>5.5.3</b> Search for optimum <em>k</em></a></li>
<li class="chapter" data-level="5.5.4" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#use-model-to-make-predictions"><i class="fa fa-check"></i><b>5.5.4</b> Use model to make predictions</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#exercises-1"><i class="fa fa-check"></i><b>5.6</b> Exercises</a><ul>
<li class="chapter" data-level="5.6.1" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#knnEx1"><i class="fa fa-check"></i><b>5.6.1</b> Exercise 1</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="svm.html"><a href="svm.html"><i class="fa fa-check"></i><b>6</b> Support vector machines</a><ul>
<li class="chapter" data-level="6.1" data-path="svm.html"><a href="svm.html#introduction-2"><i class="fa fa-check"></i><b>6.1</b> Introduction</a><ul>
<li class="chapter" data-level="6.1.1" data-path="svm.html"><a href="svm.html#maximum-margin-classifier"><i class="fa fa-check"></i><b>6.1.1</b> Maximum margin classifier</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="svm.html"><a href="svm.html#support-vector-classifier"><i class="fa fa-check"></i><b>6.2</b> Support vector classifier</a></li>
<li class="chapter" data-level="6.3" data-path="svm.html"><a href="svm.html#support-vector-machine"><i class="fa fa-check"></i><b>6.3</b> Support Vector Machine</a></li>
<li class="chapter" data-level="6.4" data-path="svm.html"><a href="svm.html#example---training-a-classifier"><i class="fa fa-check"></i><b>6.4</b> Example - training a classifier</a><ul>
<li class="chapter" data-level="6.4.1" data-path="svm.html"><a href="svm.html#setup-environment"><i class="fa fa-check"></i><b>6.4.1</b> Setup environment</a></li>
<li class="chapter" data-level="6.4.2" data-path="svm.html"><a href="svm.html#partition-data-1"><i class="fa fa-check"></i><b>6.4.2</b> Partition data</a></li>
<li class="chapter" data-level="6.4.3" data-path="svm.html"><a href="svm.html#visualize-training-data"><i class="fa fa-check"></i><b>6.4.3</b> Visualize training data</a></li>
<li class="chapter" data-level="6.4.4" data-path="svm.html"><a href="svm.html#prediction-performance-measures"><i class="fa fa-check"></i><b>6.4.4</b> Prediction performance measures</a></li>
<li class="chapter" data-level="6.4.5" data-path="svm.html"><a href="svm.html#plot-decision-boundary"><i class="fa fa-check"></i><b>6.4.5</b> Plot decision boundary</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="svm.html"><a href="svm.html#defining-your-own-model-type-to-use-in-caret"><i class="fa fa-check"></i><b>6.5</b> Defining your own model type to use in caret</a><ul>
<li class="chapter" data-level="6.5.1" data-path="svm.html"><a href="svm.html#model-cross-validation-and-tuning"><i class="fa fa-check"></i><b>6.5.1</b> Model cross-validation and tuning</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="svm.html"><a href="svm.html#iris-example"><i class="fa fa-check"></i><b>6.6</b> Iris example</a></li>
<li class="chapter" data-level="6.7" data-path="svm.html"><a href="svm.html#cell-segmentation-example"><i class="fa fa-check"></i><b>6.7</b> Cell segmentation example</a></li>
<li class="chapter" data-level="6.8" data-path="svm.html"><a href="svm.html#example---regression"><i class="fa fa-check"></i><b>6.8</b> Example - regression</a></li>
<li class="chapter" data-level="6.9" data-path="svm.html"><a href="svm.html#further-reading"><i class="fa fa-check"></i><b>6.9</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="decision-trees.html"><a href="decision-trees.html"><i class="fa fa-check"></i><b>7</b> Decision trees and random forests</a><ul>
<li class="chapter" data-level="7.1" data-path="decision-trees.html"><a href="decision-trees.html#decision-trees-1"><i class="fa fa-check"></i><b>7.1</b> Decision Trees</a></li>
<li class="chapter" data-level="7.2" data-path="decision-trees.html"><a href="decision-trees.html#example-code-with-categorical-data"><i class="fa fa-check"></i><b>7.2</b> Example code with categorical data</a><ul>
<li class="chapter" data-level="7.2.1" data-path="decision-trees.html"><a href="decision-trees.html#loading-packages-and-data"><i class="fa fa-check"></i><b>7.2.1</b> Loading packages and data</a></li>
<li class="chapter" data-level="7.2.2" data-path="decision-trees.html"><a href="decision-trees.html#data-slicing"><i class="fa fa-check"></i><b>7.2.2</b> Data Slicing</a></li>
<li class="chapter" data-level="7.2.3" data-path="decision-trees.html"><a href="decision-trees.html#data-preprocessing"><i class="fa fa-check"></i><b>7.2.3</b> Data Preprocessing</a></li>
<li class="chapter" data-level="7.2.4" data-path="decision-trees.html"><a href="decision-trees.html#training-decisions-trees"><i class="fa fa-check"></i><b>7.2.4</b> Training Decisions Trees</a></li>
<li class="chapter" data-level="7.2.5" data-path="decision-trees.html"><a href="decision-trees.html#plotting-the-decision-trees"><i class="fa fa-check"></i><b>7.2.5</b> Plotting the decision trees</a></li>
<li class="chapter" data-level="7.2.6" data-path="decision-trees.html"><a href="decision-trees.html#prediction"><i class="fa fa-check"></i><b>7.2.6</b> Prediction</a></li>
<li class="chapter" data-level="7.2.7" data-path="decision-trees.html"><a href="decision-trees.html#implementing-decision-trees-directly"><i class="fa fa-check"></i><b>7.2.7</b> Implementing Decision Trees directly</a></li>
<li class="chapter" data-level="7.2.8" data-path="decision-trees.html"><a href="decision-trees.html#iris-example-for-decision-trees"><i class="fa fa-check"></i><b>7.2.8</b> Iris example for Decision Trees</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="decision-trees.html"><a href="decision-trees.html#cell-segmentation-examples"><i class="fa fa-check"></i><b>7.3</b> Cell segmentation examples</a></li>
<li class="chapter" data-level="7.4" data-path="decision-trees.html"><a href="decision-trees.html#regression-example---blood-brain-barrier"><i class="fa fa-check"></i><b>7.4</b> Regression example - Blood Brain Barrier</a><ul>
<li class="chapter" data-level="7.4.1" data-path="decision-trees.html"><a href="decision-trees.html#decision-tree"><i class="fa fa-check"></i><b>7.4.1</b> Decision tree</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="decision-trees.html"><a href="decision-trees.html#random-forest"><i class="fa fa-check"></i><b>7.5</b> Random Forest</a><ul>
<li class="chapter" data-level="7.5.1" data-path="decision-trees.html"><a href="decision-trees.html#iris-example-for-random-forests"><i class="fa fa-check"></i><b>7.5.1</b> Iris example for Random Forests</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="decision-trees.html"><a href="decision-trees.html#cell-segmentation-examples-1"><i class="fa fa-check"></i><b>7.6</b> Cell segmentation examples</a></li>
<li class="chapter" data-level="7.7" data-path="decision-trees.html"><a href="decision-trees.html#regression-example---blood-brain-barrier-1"><i class="fa fa-check"></i><b>7.7</b> Regression example - Blood Brain Barrier</a><ul>
<li class="chapter" data-level="7.7.1" data-path="decision-trees.html"><a href="decision-trees.html#random-forest-1"><i class="fa fa-check"></i><b>7.7.1</b> Random forest</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="use-case-1.html"><a href="use-case-1.html"><i class="fa fa-check"></i><b>8</b> Use case 1</a><ul>
<li class="chapter" data-level="8.1" data-path="use-case-1.html"><a href="use-case-1.html#introduction-3"><i class="fa fa-check"></i><b>8.1</b> Introduction</a></li>
<li class="chapter" data-level="8.2" data-path="use-case-1.html"><a href="use-case-1.html#problem-automated-detection-of-malaria"><i class="fa fa-check"></i><b>8.2</b> Problem: automated detection of malaria</a></li>
<li class="chapter" data-level="8.3" data-path="use-case-1.html"><a href="use-case-1.html#challenges"><i class="fa fa-check"></i><b>8.3</b> Challenges</a></li>
<li class="chapter" data-level="8.4" data-path="use-case-1.html"><a href="use-case-1.html#getting-started"><i class="fa fa-check"></i><b>8.4</b> Getting started</a><ul>
<li class="chapter" data-level="8.4.1" data-path="use-case-1.html"><a href="use-case-1.html#load-data"><i class="fa fa-check"></i><b>8.4.1</b> Load data</a></li>
<li class="chapter" data-level="8.4.2" data-path="use-case-1.html"><a href="use-case-1.html#model-comparison"><i class="fa fa-check"></i><b>8.4.2</b> Model comparison</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="use-case-1.html"><a href="use-case-1.html#solutions"><i class="fa fa-check"></i><b>8.5</b> Solutions</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="linear-models.html"><a href="linear-models.html"><i class="fa fa-check"></i><b>9</b> Linear models and matrix algebra</a><ul>
<li class="chapter" data-level="9.1" data-path="linear-models.html"><a href="linear-models.html#linear-models-1"><i class="fa fa-check"></i><b>9.1</b> Linear models</a></li>
<li class="chapter" data-level="9.2" data-path="linear-models.html"><a href="linear-models.html#matrix-algebra"><i class="fa fa-check"></i><b>9.2</b> Matrix algebra</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="logistic-regression.html"><a href="logistic-regression.html"><i class="fa fa-check"></i><b>10</b> Linear regression and logistic regression</a><ul>
<li class="chapter" data-level="10.1" data-path="logistic-regression.html"><a href="logistic-regression.html#regression"><i class="fa fa-check"></i><b>10.1</b> Regression</a><ul>
<li class="chapter" data-level="10.1.1" data-path="logistic-regression.html"><a href="logistic-regression.html#linear-regression"><i class="fa fa-check"></i><b>10.1.1</b> Linear regression</a></li>
<li class="chapter" data-level="10.1.2" data-path="logistic-regression.html"><a href="logistic-regression.html#polynomial-regression"><i class="fa fa-check"></i><b>10.1.2</b> Polynomial regression</a></li>
<li class="chapter" data-level="10.1.3" data-path="logistic-regression.html"><a href="logistic-regression.html#distributions-of-fits"><i class="fa fa-check"></i><b>10.1.3</b> Distributions of fits</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="logistic-regression.html"><a href="logistic-regression.html#classification"><i class="fa fa-check"></i><b>10.2</b> Classification</a><ul>
<li class="chapter" data-level="10.2.1" data-path="logistic-regression.html"><a href="logistic-regression.html#logistic-regression"><i class="fa fa-check"></i><b>10.2.1</b> Logistic regression</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="logistic-regression.html"><a href="logistic-regression.html#resources"><i class="fa fa-check"></i><b>10.3</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="ann.html"><a href="ann.html"><i class="fa fa-check"></i><b>11</b> Artificial neural networks</a><ul>
<li class="chapter" data-level="11.1" data-path="ann.html"><a href="ann.html#neural-networks"><i class="fa fa-check"></i><b>11.1</b> Neural Networks</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="use-case-2.html"><a href="use-case-2.html"><i class="fa fa-check"></i><b>12</b> Use case 2</a></li>
<li class="chapter" data-level="13" data-path="mlnn.html"><a href="mlnn.html"><i class="fa fa-check"></i><b>13</b> Deep Learning</a><ul>
<li class="chapter" data-level="13.1" data-path="mlnn.html"><a href="mlnn.html#multilayer-neural-networks"><i class="fa fa-check"></i><b>13.1</b> Multilayer Neural Networks</a><ul>
<li class="chapter" data-level="13.1.1" data-path="mlnn.html"><a href="mlnn.html#reading-in-images"><i class="fa fa-check"></i><b>13.1.1</b> Reading in images</a></li>
<li class="chapter" data-level="13.1.2" data-path="mlnn.html"><a href="mlnn.html#constructing-layers-in-kerasr"><i class="fa fa-check"></i><b>13.1.2</b> Constructing layers in kerasR</a></li>
<li class="chapter" data-level="13.1.3" data-path="mlnn.html"><a href="mlnn.html#rick-and-morty-classifier-using-deep-learning"><i class="fa fa-check"></i><b>13.1.3</b> Rick and Morty classifier using Deep Learning</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="mlnn.html"><a href="mlnn.html#convolutional-neural-networks"><i class="fa fa-check"></i><b>13.2</b> Convolutional neural networks</a><ul>
<li class="chapter" data-level="13.2.1" data-path="mlnn.html"><a href="mlnn.html#checking-the-models"><i class="fa fa-check"></i><b>13.2.1</b> Checking the models</a></li>
<li class="chapter" data-level="13.2.2" data-path="mlnn.html"><a href="mlnn.html#asking-more-precise-questions"><i class="fa fa-check"></i><b>13.2.2</b> Asking more precise questions</a></li>
<li class="chapter" data-level="13.2.3" data-path="mlnn.html"><a href="mlnn.html#more-complex-networks"><i class="fa fa-check"></i><b>13.2.3</b> More complex networks</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="mlnn.html"><a href="mlnn.html#further-reading-1"><i class="fa fa-check"></i><b>13.3</b> Further reading</a></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="logistic-regression.html"><a href="logistic-regression.html#resources"><i class="fa fa-check"></i><b>A</b> Resources</a><ul>
<li class="chapter" data-level="A.1" data-path="resources.html"><a href="resources.html"><i class="fa fa-check"></i><b>A.1</b> Python</a></li>
<li class="chapter" data-level="A.2" data-path="resources.html"><a href="resources.html#machine-learning-data-set-repositories"><i class="fa fa-check"></i><b>A.2</b> Machine learning data set repositories</a><ul>
<li class="chapter" data-level="A.2.1" data-path="resources.html"><a href="resources.html#mldata"><i class="fa fa-check"></i><b>A.2.1</b> MLDATA</a></li>
<li class="chapter" data-level="A.2.2" data-path="resources.html"><a href="resources.html#uci-machine-learning-repository"><i class="fa fa-check"></i><b>A.2.2</b> UCI Machine Learning Repository</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="B" data-path="solutions-dimensionality-reduction.html"><a href="solutions-dimensionality-reduction.html"><i class="fa fa-check"></i><b>B</b> Solutions - Dimensionality reduction</a><ul>
<li class="chapter" data-level="B.1" data-path="solutions-dimensionality-reduction.html"><a href="solutions-dimensionality-reduction.html#exercise-1."><i class="fa fa-check"></i><b>B.1</b> Exercise 1.</a></li>
<li class="chapter" data-level="B.2" data-path="solutions-dimensionality-reduction.html"><a href="solutions-dimensionality-reduction.html#exercise-2."><i class="fa fa-check"></i><b>B.2</b> Exercise 2.</a></li>
</ul></li>
<li class="chapter" data-level="C" data-path="solutions-clustering.html"><a href="solutions-clustering.html"><i class="fa fa-check"></i><b>C</b> Solutions ch. 4 - Clustering</a><ul>
<li class="chapter" data-level="C.1" data-path="solutions-clustering.html"><a href="solutions-clustering.html#exercise-1"><i class="fa fa-check"></i><b>C.1</b> Exercise 1</a></li>
</ul></li>
<li class="chapter" data-level="D" data-path="solutions-nearest-neighbours.html"><a href="solutions-nearest-neighbours.html"><i class="fa fa-check"></i><b>D</b> Solutions ch. 7 - Nearest neighbours</a><ul>
<li class="chapter" data-level="D.1" data-path="solutions-nearest-neighbours.html"><a href="solutions-nearest-neighbours.html#exercise-1-1"><i class="fa fa-check"></i><b>D.1</b> Exercise 1</a></li>
</ul></li>
<li class="chapter" data-level="E" data-path="solutions-svm.html"><a href="solutions-svm.html"><i class="fa fa-check"></i><b>E</b> Solutions ch. 6 - Support vector machines</a><ul>
<li class="chapter" data-level="E.1" data-path="solutions-svm.html"><a href="solutions-svm.html#exercise-1-2"><i class="fa fa-check"></i><b>E.1</b> Exercise 1</a></li>
</ul></li>
<li class="chapter" data-level="F" data-path="solutions-decision-trees.html"><a href="solutions-decision-trees.html"><i class="fa fa-check"></i><b>F</b> Solutions ch. 9 - Decision trees and random forests</a><ul>
<li class="chapter" data-level="F.1" data-path="solutions-decision-trees.html"><a href="solutions-decision-trees.html#exercise-1-3"><i class="fa fa-check"></i><b>F.1</b> Exercise 1</a></li>
</ul></li>
<li class="chapter" data-level="G" data-path="use-case-1-solutions.html"><a href="use-case-1-solutions.html"><i class="fa fa-check"></i><b>G</b> Solutions chapter 8 - use case 1</a><ul>
<li class="chapter" data-level="G.1" data-path="use-case-1-solutions.html"><a href="use-case-1-solutions.html#preparation"><i class="fa fa-check"></i><b>G.1</b> Preparation</a><ul>
<li class="chapter" data-level="G.1.1" data-path="use-case-1-solutions.html"><a href="use-case-1-solutions.html#load-required-libraries"><i class="fa fa-check"></i><b>G.1.1</b> Load required libraries</a></li>
<li class="chapter" data-level="G.1.2" data-path="use-case-1-solutions.html"><a href="use-case-1-solutions.html#define-svm-model"><i class="fa fa-check"></i><b>G.1.2</b> Define SVM model</a></li>
<li class="chapter" data-level="G.1.3" data-path="use-case-1-solutions.html"><a href="use-case-1-solutions.html#setup-parallel-processing"><i class="fa fa-check"></i><b>G.1.3</b> Setup parallel processing</a></li>
<li class="chapter" data-level="G.1.4" data-path="use-case-1-solutions.html"><a href="use-case-1-solutions.html#load-data-1"><i class="fa fa-check"></i><b>G.1.4</b> Load data</a></li>
</ul></li>
<li class="chapter" data-level="G.2" data-path="use-case-1-solutions.html"><a href="use-case-1-solutions.html#assess-data-quality"><i class="fa fa-check"></i><b>G.2</b> Assess data quality</a><ul>
<li class="chapter" data-level="G.2.1" data-path="use-case-1-solutions.html"><a href="use-case-1-solutions.html#zero-and-near-zero-variance-predictors-1"><i class="fa fa-check"></i><b>G.2.1</b> Zero and near-zero variance predictors</a></li>
<li class="chapter" data-level="G.2.2" data-path="use-case-1-solutions.html"><a href="use-case-1-solutions.html#are-all-predictors-on-the-same-scale"><i class="fa fa-check"></i><b>G.2.2</b> Are all predictors on the same scale?</a></li>
<li class="chapter" data-level="G.2.3" data-path="use-case-1-solutions.html"><a href="use-case-1-solutions.html#redundancy-from-correlated-variables"><i class="fa fa-check"></i><b>G.2.3</b> Redundancy from correlated variables</a></li>
<li class="chapter" data-level="G.2.4" data-path="use-case-1-solutions.html"><a href="use-case-1-solutions.html#skewness-1"><i class="fa fa-check"></i><b>G.2.4</b> Skewness</a></li>
</ul></li>
<li class="chapter" data-level="G.3" data-path="use-case-1-solutions.html"><a href="use-case-1-solutions.html#infection-status-two-class-problem"><i class="fa fa-check"></i><b>G.3</b> Infection status (two-class problem)</a><ul>
<li class="chapter" data-level="G.3.1" data-path="use-case-1-solutions.html"><a href="use-case-1-solutions.html#model-training-and-parameter-tuning"><i class="fa fa-check"></i><b>G.3.1</b> Model training and parameter tuning</a></li>
<li class="chapter" data-level="G.3.2" data-path="use-case-1-solutions.html"><a href="use-case-1-solutions.html#knn"><i class="fa fa-check"></i><b>G.3.2</b> KNN</a></li>
<li class="chapter" data-level="G.3.3" data-path="use-case-1-solutions.html"><a href="use-case-1-solutions.html#svm-1"><i class="fa fa-check"></i><b>G.3.3</b> SVM</a></li>
<li class="chapter" data-level="G.3.4" data-path="use-case-1-solutions.html"><a href="use-case-1-solutions.html#decision-tree-1"><i class="fa fa-check"></i><b>G.3.4</b> Decision tree</a></li>
<li class="chapter" data-level="G.3.5" data-path="use-case-1-solutions.html"><a href="use-case-1-solutions.html#random-forest-2"><i class="fa fa-check"></i><b>G.3.5</b> Random forest</a></li>
<li class="chapter" data-level="G.3.6" data-path="use-case-1-solutions.html"><a href="use-case-1-solutions.html#compare-models"><i class="fa fa-check"></i><b>G.3.6</b> Compare models</a></li>
<li class="chapter" data-level="G.3.7" data-path="use-case-1-solutions.html"><a href="use-case-1-solutions.html#predict-test-set-using-our-best-model"><i class="fa fa-check"></i><b>G.3.7</b> Predict test set using our best model</a></li>
<li class="chapter" data-level="G.3.8" data-path="use-case-1-solutions.html"><a href="use-case-1-solutions.html#roc-curve"><i class="fa fa-check"></i><b>G.3.8</b> ROC curve</a></li>
</ul></li>
<li class="chapter" data-level="G.4" data-path="use-case-1-solutions.html"><a href="use-case-1-solutions.html#discrimination-of-infective-stages-multi-class-problem"><i class="fa fa-check"></i><b>G.4</b> Discrimination of infective stages (multi-class problem)</a><ul>
<li class="chapter" data-level="G.4.1" data-path="use-case-1-solutions.html"><a href="use-case-1-solutions.html#define-cross-validation-procedure"><i class="fa fa-check"></i><b>G.4.1</b> Define cross-validation procedure</a></li>
<li class="chapter" data-level="G.4.2" data-path="use-case-1-solutions.html"><a href="use-case-1-solutions.html#knn-1"><i class="fa fa-check"></i><b>G.4.2</b> KNN</a></li>
<li class="chapter" data-level="G.4.3" data-path="use-case-1-solutions.html"><a href="use-case-1-solutions.html#svm-2"><i class="fa fa-check"></i><b>G.4.3</b> SVM</a></li>
<li class="chapter" data-level="G.4.4" data-path="use-case-1-solutions.html"><a href="use-case-1-solutions.html#decision-tree-2"><i class="fa fa-check"></i><b>G.4.4</b> Decision tree</a></li>
<li class="chapter" data-level="G.4.5" data-path="use-case-1-solutions.html"><a href="use-case-1-solutions.html#random-forest-3"><i class="fa fa-check"></i><b>G.4.5</b> Random forest</a></li>
<li class="chapter" data-level="G.4.6" data-path="use-case-1-solutions.html"><a href="use-case-1-solutions.html#compare-models-1"><i class="fa fa-check"></i><b>G.4.6</b> Compare models</a></li>
<li class="chapter" data-level="G.4.7" data-path="use-case-1-solutions.html"><a href="use-case-1-solutions.html#predict-test-set-using-our-best-model-1"><i class="fa fa-check"></i><b>G.4.7</b> Predict test set using our best model</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="H" data-path="solutions-linear-models.html"><a href="solutions-linear-models.html"><i class="fa fa-check"></i><b>H</b> Solutions ch. 3 - Linear models and matrix algebra</a><ul>
<li class="chapter" data-level="H.1" data-path="solutions-linear-models.html"><a href="solutions-linear-models.html#example-2"><i class="fa fa-check"></i><b>H.1</b> Example 2</a></li>
<li class="chapter" data-level="H.2" data-path="solutions-linear-models.html"><a href="solutions-linear-models.html#example-2-1"><i class="fa fa-check"></i><b>H.2</b> Example 2</a></li>
</ul></li>
<li class="chapter" data-level="I" data-path="solutions-logistic-regression.html"><a href="solutions-logistic-regression.html"><i class="fa fa-check"></i><b>I</b> Solutions ch. 4 - Linear and non-linear (logistic) regression</a></li>
<li class="chapter" data-level="J" data-path="solutions-ann.html"><a href="solutions-ann.html"><i class="fa fa-check"></i><b>J</b> Solutions ch. 10 - Artificial neural networks</a><ul>
<li class="chapter" data-level="J.1" data-path="solutions-ann.html"><a href="solutions-ann.html#exercise-1-4"><i class="fa fa-check"></i><b>J.1</b> Exercise 1</a></li>
</ul></li>
<li class="chapter" data-level="K" data-path="use-case-2-solutions.html"><a href="use-case-2-solutions.html"><i class="fa fa-check"></i><b>K</b> Solutions for use case 2</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">An Introduction to Machine Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="decision-trees" class="section level1 hasAnchor">
<h1><span class="header-section-number">7</span> Decision trees and random forests<a href="decision-trees.html#decision-trees" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<!-- Sudhakaran -->
<div id="decision-trees-1" class="section level2 hasAnchor">
<h2><span class="header-section-number">7.1</span> Decision Trees<a href="decision-trees.html#decision-trees-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>What is a Decision Tree?</strong></p>
<p>Decision tree or recursive partitioning is a supervised graph based algorithm to represent choices and the results of the choices in the form of a tree.</p>
<p>The nodes in the graph represent an event or choice and it is referred to as a <strong>leaf</strong> and the set of <em>decisions</em> made at the node is reffered to as <strong>branches</strong>.</p>
<p>Decision trees map non-linear relationships and the hierarchical leaves and branches makes a <strong>Tree</strong>.</p>
<p>It is one of the most widely used tool in ML for predictive analytics. Examples of use of decision tress are − predicting an email as spam or not spam, predicting whether a tumor is cancerous or not.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-1"></span>
<img src="images/decision_tree.png" alt="Decision Tree" width="55%" />
<p class="caption">
Figure 7.1: Decision Tree
</p>
</div>
<p><em>Image source: analyticsvidhya.com</em></p>
<p><strong>How does it work?</strong></p>
<p>A model is first created with training data and then a set of validation data is used to verify and improve the model. R has many packages, such as ctree, rpart, tree, and so on, which are used to create and visualize decision trees.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-2"></span>
<img src="images/decision_tree_2.png" alt="Example of a decision Tree" width="90%" />
<p class="caption">
Figure 7.2: Example of a decision Tree
</p>
</div>
<p><em>Image source: analyticsvidhya.com</em></p>
<p><strong>Example of a decision tree</strong><br />
In this problem (Figure 6.2), we need to segregate students who play cricket in their leisure time based on highly significant input variable among all three.</p>
<p>The decision tree algorithm will initially segregate the students based on <strong>all values</strong> of three variable (Gender, Class, and Height) and identify the variable, which creates the best homogeneous sets of students (which are heterogeneous to each other).</p>
<p>In the snapshot above, you can see that variable Gender is able to identify best homogeneous sets compared to the other two variables.</p>
<p>There are a number of decision tree algorithms. We have to choose them based on our dataset. If the dependent variable is categorical, then we have to use a <em>categorical variable decision tree</em>. If the dependent variable is continuous, then we have to use a <em>continuous variable decision tree</em>.</p>
<p>The above example is of the categorical variable decision tree type.</p>
<p><strong>Some simple R code for a decision tree looks like this:</strong></p>
<!-- this chunk will not run because we have not defined any data-->
<div class="sourceCode" id="cb425"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb425-1" data-line-number="1"><span class="kw">library</span>(rpart)</a>
<a class="sourceLine" id="cb425-2" data-line-number="2">x &lt;-<span class="st"> </span><span class="kw">cbind</span>(x_train,y_train) <span class="co">##y_train – represents dependent variable, x_train – represents independent variable</span></a>
<a class="sourceLine" id="cb425-3" data-line-number="3"><span class="co"># grow tree </span></a>
<a class="sourceLine" id="cb425-4" data-line-number="4">fit &lt;-<span class="st"> </span><span class="kw">rpart</span>(y_train <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> x,<span class="dt">method=</span><span class="st">&quot;class&quot;</span>) <span class="co">##x – represents training data</span></a>
<a class="sourceLine" id="cb425-5" data-line-number="5"><span class="kw">summary</span>(fit)</a>
<a class="sourceLine" id="cb425-6" data-line-number="6"><span class="co">#Predict Output </span></a>
<a class="sourceLine" id="cb425-7" data-line-number="7">predicted &lt;-<span class="st"> </span><span class="kw">predict</span>(fit,x_test)</a></code></pre></div>
<p><strong>Terminology related to decision trees</strong></p>
<p><em>Root nodule</em>: the entire population that can get further divided into homogeneous sets</p>
<p><em>Splitting</em>: process of diving a node into two or more sub-nodes</p>
<p><em>Decision node</em>: When a sub-node splits into further sub-nodes</p>
<p><em>Leaf or terminal node</em>: when a node does not split further it is called a terminal node.</p>
<p><em>Pruning</em>: A loose stopping crieteria is used to contruct the tree and then the tree is cut back by removing branches that do not contribute to the generalisation accuracy.</p>
<p><em>Branch</em>: a sub-section of an entire tree</p>
<p><strong>How does a tree decide where to split?</strong></p>
<p>The classification tree searches through each dependent variable to find a single variable that splits the data into two or more groups and this process is repeated until the stopping criteria is invoked.</p>
<p>The decision of making strategic splits heavily affects a tree’s accuracy. The decision criteria is different for classification and regression trees.</p>
<p>Decision trees use multiple algorithms to decide to split a node in two or more sub-nodes. The common goal for these algorithms is the creation of sub-nodes with increased homogeneity. In other words, we can say that purity of the node increases with respect to the target variable. Decision tree splits the nodes on all available variables and then selects the split which results in most homogeneous sub-nodes.</p>
<p><strong>Commonly used algorithms to decide where to split</strong></p>
<p><strong>Gini Index</strong><br />
If we select two items from a population at random then they must be of same class and the probability for this is 1 if population is pure.</p>
<ol style="list-style-type: lower-alpha">
<li>It works with categorical target variable “Success” or “Failure”.<br />
</li>
<li>It performs only Binary splits<br />
</li>
<li>Higher the value of Gini higher the homogeneity.<br />
</li>
<li>CART (Classification and Regression Tree) uses Gini method to create binary splits.</li>
</ol>
<p>Step 1: Calculate Gini for sub-nodes, using formula sum of square of probability for success and failure  <span class="math display">\[p^2+q^2\]</span>.
Step 2: Calculate Gini for split using weighted Gini score of each node of that split.</p>
<p><strong>Chi-Square</strong><br />
It is an algorithm to find out the statistical significance between the differences between sub-nodes and parent node. We measure it by sum of squares of standardized differences between observed and expected frequencies of target variable.</p>
<ol style="list-style-type: lower-alpha">
<li>It works with categorical target variable “Success” or “Failure”.</li>
<li>It can perform two or more splits.</li>
<li>Higher the value of Chi-Square higher the statistical significance of differences between sub-node and Parent node.</li>
<li>Chi-Square of each node is calculated using formula,
Chi-square = <span class="math display">\[\sum(Actual – Expected)^2 / Expected\]</span></li>
</ol>
<p>Steps to Calculate Chi-square for a split:</p>
<ol style="list-style-type: decimal">
<li>Calculate Chi-square for individual node by calculating the deviation for Success and Failure both</li>
<li>Calculated Chi-square of Split using Sum of all Chi-square of success and Failure of each node of the split</li>
</ol>
<p><strong>Information Gain</strong><br />
The more homogeneous something is the less information is needed to describe it and hence it has gained information. Information theory has a measure to define this degree of disorganization in a system and it is known as Entropy. If a sample is completely homogeneous, then the entropy is zero and if it is equally divided (50% – 50%), it has entropy of one.</p>
<p>Entropy can be calculated using formula
<span class="math display">\[Entropy = -plog_2p - qlog_2q\]</span></p>
<p>Where p and q are probability of success and failure</p>
<p><strong>Reduction in Variance</strong></p>
<p>Reduction in variance is an algorithm used for continuous target variables (regression problems). This algorithm uses the standard formula of variance to choose the best split. The split with lower variance is selected as the criteria to split the population:</p>
<p><strong>Advantages of decision tree</strong></p>
<ol style="list-style-type: decimal">
<li>Simple to understand and use<br />
</li>
<li>Algorithms are robust to noisy data<br />
</li>
<li>Useful in data exploration<br />
</li>
<li>decision tree is ‘non parametric’ in nature i.e. does not have any assumptions about the distribution of the variables</li>
</ol>
<p><strong>Disadvantages of decision tree</strong></p>
<p>1.Overfitting is the common disadvantage of decision trees. It is taken care of partially by constraining the model parameter and by pruning.<br />
2. It is not ideal for continuous variables as in it looses information</p>
<p><em>Some parameters used to defining a tree and constrain overfitting</em></p>
<ol style="list-style-type: decimal">
<li>Minimum sample for a node split<br />
</li>
<li>Minimum sample for a terminal node<br />
</li>
<li>Maximum depth of a tree<br />
</li>
<li>Maximum number of terminal nodes<br />
</li>
<li>Maximum features considered for split</li>
</ol>
<p><em>Acknowledgement: some aspects of this explanation can be read from www.analyticsvidhya.com</em></p>
</div>
<div id="example-code-with-categorical-data" class="section level2 hasAnchor">
<h2><span class="header-section-number">7.2</span> Example code with categorical data<a href="decision-trees.html#example-code-with-categorical-data" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We are going to plot a car evaluation data with 7 attributes, 6 as feature attributes and 1 as the target attribute. This is to evaluate what kinds of cars people purchase. All the attributes are categorical. We will try to build a classifier for predicting the Class attribute. The index of target attribute is 7th. For more information about this dataset, see <a href="https://archive.ics.uci.edu/ml/datasets/car+evaluation" class="uri">https://archive.ics.uci.edu/ml/datasets/car+evaluation</a></p>
<div id="loading-packages-and-data" class="section level3 hasAnchor">
<h3><span class="header-section-number">7.2.1</span> Loading packages and data<a href="decision-trees.html#loading-packages-and-data" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>R package <em>caret</em> helps to perform various machine learning tasks including decision tree classification. The <em>rplot.plot</em> package will help to get a visual plot of the decision tree.</p>
<div class="sourceCode" id="cb426"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb426-1" data-line-number="1"><span class="kw">library</span>(caret)</a>
<a class="sourceLine" id="cb426-2" data-line-number="2"><span class="kw">library</span>(rpart.plot)</a>
<a class="sourceLine" id="cb426-3" data-line-number="3"><span class="co">#download.file(url = &quot;https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data&quot;,</span></a>
<a class="sourceLine" id="cb426-4" data-line-number="4"><span class="co">#              destfile = &quot;data/car.data&quot;)</span></a>
<a class="sourceLine" id="cb426-5" data-line-number="5">car.data &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&quot;data/Decision_tree_and_RF/car.data&quot;</span>, <span class="dt">sep =</span> <span class="st">&#39;,&#39;</span>, <span class="dt">header =</span> <span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb426-6" data-line-number="6"><span class="kw">colnames</span>(car.data)=<span class="kw">c</span>(<span class="st">&#39;buying&#39;</span>,<span class="st">&#39;maint&#39;</span>,<span class="st">&#39;doors&#39;</span>,<span class="st">&#39;persons&#39;</span>,<span class="st">&#39;lug_boot&#39;</span>,<span class="st">&#39;safety&#39;</span>,<span class="st">&#39;class&#39;</span>)</a></code></pre></div>
<div class="sourceCode" id="cb427"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb427-1" data-line-number="1"><span class="kw">str</span>(car.data) </a></code></pre></div>
<pre><code>## &#39;data.frame&#39;:    1728 obs. of  7 variables:
##  $ buying  : chr  &quot;vhigh&quot; &quot;vhigh&quot; &quot;vhigh&quot; &quot;vhigh&quot; ...
##  $ maint   : chr  &quot;vhigh&quot; &quot;vhigh&quot; &quot;vhigh&quot; &quot;vhigh&quot; ...
##  $ doors   : chr  &quot;2&quot; &quot;2&quot; &quot;2&quot; &quot;2&quot; ...
##  $ persons : chr  &quot;2&quot; &quot;2&quot; &quot;2&quot; &quot;2&quot; ...
##  $ lug_boot: chr  &quot;small&quot; &quot;small&quot; &quot;small&quot; &quot;med&quot; ...
##  $ safety  : chr  &quot;low&quot; &quot;med&quot; &quot;high&quot; &quot;low&quot; ...
##  $ class   : chr  &quot;unacc&quot; &quot;unacc&quot; &quot;unacc&quot; &quot;unacc&quot; ...</code></pre>
<div class="sourceCode" id="cb429"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb429-1" data-line-number="1"><span class="kw">head</span>(car.data)</a></code></pre></div>
<pre><code>##   buying maint doors persons lug_boot safety class
## 1  vhigh vhigh     2       2    small    low unacc
## 2  vhigh vhigh     2       2    small    med unacc
## 3  vhigh vhigh     2       2    small   high unacc
## 4  vhigh vhigh     2       2      med    low unacc
## 5  vhigh vhigh     2       2      med    med unacc
## 6  vhigh vhigh     2       2      med   high unacc</code></pre>
<p>The output of this will show us that our dataset consists of 1728 observations each with 7 attributes and that all the features are categorical, so normalization of data is not needed.</p>
</div>
<div id="data-slicing" class="section level3 hasAnchor">
<h3><span class="header-section-number">7.2.2</span> Data Slicing<a href="decision-trees.html#data-slicing" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Data slicing is a step to split data into train and test set. Training data set can be used specifically for our model building. Test dataset should not be mixed up while building model. Even during standardization, we should not standardize our test set.</p>
<div class="sourceCode" id="cb431"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb431-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">42</span>)</a>
<a class="sourceLine" id="cb431-2" data-line-number="2">trainTestPartition &lt;-<span class="st"> </span><span class="kw">createDataPartition</span>(<span class="dt">y =</span> car.data<span class="op">$</span>class, <span class="dt">p=</span> <span class="fl">0.7</span>, <span class="dt">list =</span> <span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb431-3" data-line-number="3">car.training &lt;-<span class="st"> </span>car.data[trainTestPartition,]</a>
<a class="sourceLine" id="cb431-4" data-line-number="4">car.testing &lt;-<span class="st"> </span>car.data[<span class="op">-</span>trainTestPartition,]</a></code></pre></div>
<p>The “p” parameter holds a decimal value in the range of 0-1. It’s to show that percentage of the split. We are using p=0.7. It means that data split should be done in 70:30 ratio.</p>
</div>
<div id="data-preprocessing" class="section level3 hasAnchor">
<h3><span class="header-section-number">7.2.3</span> Data Preprocessing<a href="decision-trees.html#data-preprocessing" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>First, check the dimensions of the data.</p>
<div class="sourceCode" id="cb432"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb432-1" data-line-number="1"><span class="kw">dim</span>(car.training); <span class="kw">dim</span>(car.testing);</a></code></pre></div>
<pre><code>## [1] 1211    7</code></pre>
<pre><code>## [1] 517   7</code></pre>
<p>Next, check for missing data.</p>
<div class="sourceCode" id="cb435"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb435-1" data-line-number="1"><span class="kw">anyNA</span>(car.data)</a></code></pre></div>
<pre><code>## [1] FALSE</code></pre>
<p>Make sure none of the variables are zero or near-zero variance.</p>
<div class="sourceCode" id="cb437"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb437-1" data-line-number="1"><span class="kw">nzv</span>(car.data)</a></code></pre></div>
<pre><code>## integer(0)</code></pre>
</div>
<div id="training-decisions-trees" class="section level3 hasAnchor">
<h3><span class="header-section-number">7.2.4</span> Training Decisions Trees<a href="decision-trees.html#training-decisions-trees" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="sourceCode" id="cb439"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb439-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">42</span>)</a>
<a class="sourceLine" id="cb439-2" data-line-number="2">seeds =<span class="st"> </span><span class="kw">vector</span>(<span class="dt">mode=</span><span class="st">&#39;list&#39;</span>,<span class="dt">length=</span><span class="dv">101</span>)</a>
<a class="sourceLine" id="cb439-3" data-line-number="3"><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">100</span>) seeds[[i]] =<span class="st"> </span><span class="kw">sample.int</span>(<span class="dv">1000</span>,<span class="dv">100</span>)</a>
<a class="sourceLine" id="cb439-4" data-line-number="4">seeds[[<span class="dv">101</span>]] =<span class="st"> </span><span class="kw">sample.int</span>(<span class="dv">1000</span>,<span class="dv">1</span>)</a>
<a class="sourceLine" id="cb439-5" data-line-number="5"></a>
<a class="sourceLine" id="cb439-6" data-line-number="6">trctrl &lt;-<span class="st"> </span><span class="kw">trainControl</span>(<span class="dt">method =</span> <span class="st">&quot;repeatedcv&quot;</span>, </a>
<a class="sourceLine" id="cb439-7" data-line-number="7">                       <span class="dt">number =</span> <span class="dv">10</span>, </a>
<a class="sourceLine" id="cb439-8" data-line-number="8">                       <span class="dt">repeats =</span> <span class="dv">10</span>,</a>
<a class="sourceLine" id="cb439-9" data-line-number="9">                       <span class="dt">seeds =</span> seeds)</a></code></pre></div>
<p>Training the Decision Tree classifier with criterion as GINI INDEX</p>
<div class="sourceCode" id="cb440"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb440-1" data-line-number="1">dtree_fit &lt;-<span class="st"> </span><span class="kw">train</span>(class <span class="op">~</span>., <span class="dt">data =</span> car.training, <span class="dt">method =</span> <span class="st">&quot;rpart&quot;</span>,</a>
<a class="sourceLine" id="cb440-2" data-line-number="2">                   <span class="dt">parms =</span> <span class="kw">list</span>(<span class="dt">split =</span> <span class="st">&quot;gini&quot;</span>),</a>
<a class="sourceLine" id="cb440-3" data-line-number="3">                   <span class="dt">trControl=</span>trctrl,</a>
<a class="sourceLine" id="cb440-4" data-line-number="4">                   <span class="dt">tuneLength =</span> <span class="dv">10</span>)</a>
<a class="sourceLine" id="cb440-5" data-line-number="5">dtree_fit</a></code></pre></div>
<pre><code>## CART 
## 
## 1211 samples
##    6 predictor
##    4 classes: &#39;acc&#39;, &#39;good&#39;, &#39;unacc&#39;, &#39;vgood&#39; 
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold, repeated 10 times) 
## Summary of sample sizes: 1090, 1090, 1092, 1089, 1090, 1089, ... 
## Resampling results across tuning parameters:
## 
##   cp           Accuracy   Kappa    
##   0.006868132  0.8561955  0.6905404
##   0.008241758  0.8533826  0.6840524
##   0.009615385  0.8517310  0.6780120
##   0.012362637  0.8424055  0.6518579
##   0.013736264  0.8387772  0.6442497
##   0.016483516  0.8282234  0.6206633
##   0.019230769  0.8208613  0.6059427
##   0.024725275  0.8072310  0.5764531
##   0.052197802  0.7977251  0.5671482
##   0.072802198  0.7512436  0.3541499
## 
## Accuracy was used to select the optimal model using the largest value.
## The final value used for the model was cp = 0.006868132.</code></pre>
<p>Training the Decision Tree classifier with criterion as INFORMATION GAIN</p>
<div class="sourceCode" id="cb442"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb442-1" data-line-number="1">dtree_fit_information &lt;-<span class="st"> </span><span class="kw">train</span>(class <span class="op">~</span>., <span class="dt">data =</span> car.training, <span class="dt">method =</span> <span class="st">&quot;rpart&quot;</span>,</a>
<a class="sourceLine" id="cb442-2" data-line-number="2">                               <span class="dt">parms =</span> <span class="kw">list</span>(<span class="dt">split =</span> <span class="st">&quot;information&quot;</span>),</a>
<a class="sourceLine" id="cb442-3" data-line-number="3">                               <span class="dt">trControl=</span>trctrl,</a>
<a class="sourceLine" id="cb442-4" data-line-number="4">                               <span class="dt">tuneLength =</span> <span class="dv">10</span>)</a>
<a class="sourceLine" id="cb442-5" data-line-number="5">dtree_fit_information</a></code></pre></div>
<pre><code>## CART 
## 
## 1211 samples
##    6 predictor
##    4 classes: &#39;acc&#39;, &#39;good&#39;, &#39;unacc&#39;, &#39;vgood&#39; 
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold, repeated 10 times) 
## Summary of sample sizes: 1091, 1091, 1090, 1090, 1089, 1090, ... 
## Resampling results across tuning parameters:
## 
##   cp           Accuracy   Kappa    
##   0.006868132  0.8538494  0.6879006
##   0.008241758  0.8537695  0.6860030
##   0.009615385  0.8529559  0.6831290
##   0.012362637  0.8359565  0.6411485
##   0.013736264  0.8294294  0.6264449
##   0.016483516  0.8252267  0.6172837
##   0.019230769  0.8179598  0.6014662
##   0.024725275  0.7972610  0.5351600
##   0.052197802  0.7738585  0.4786880
##   0.072802198  0.7330169  0.2661896
## 
## Accuracy was used to select the optimal model using the largest value.
## The final value used for the model was cp = 0.006868132.</code></pre>
<p>In both cases, the same cp value is chosen but we see different accuracy and kappa values.</p>
</div>
<div id="plotting-the-decision-trees" class="section level3 hasAnchor">
<h3><span class="header-section-number">7.2.5</span> Plotting the decision trees<a href="decision-trees.html#plotting-the-decision-trees" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="sourceCode" id="cb444"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb444-1" data-line-number="1"><span class="kw">prp</span>(dtree_fit<span class="op">$</span>finalModel, <span class="dt">box.palette =</span> <span class="st">&quot;Reds&quot;</span>, <span class="dt">tweak =</span> <span class="fl">1.2</span>) <span class="co">##Gini tree</span></a></code></pre></div>
<p><img src="06-decision-trees_files/figure-html/plot%20tree%20dtree%20cars-1.png" width="672" /></p>
<div class="sourceCode" id="cb445"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb445-1" data-line-number="1"><span class="kw">prp</span>(dtree_fit_information<span class="op">$</span>finalModel, <span class="dt">box.palette =</span> <span class="st">&quot;Blues&quot;</span>, <span class="dt">tweak =</span> <span class="fl">1.2</span>) <span class="co">##Information tree</span></a></code></pre></div>
<p><img src="06-decision-trees_files/figure-html/plot%20tree%20dtree%20cars-2.png" width="672" /></p>
<p>We also see different trees from the two <em>split</em> parameters.</p>
</div>
<div id="prediction" class="section level3 hasAnchor">
<h3><span class="header-section-number">7.2.6</span> Prediction<a href="decision-trees.html#prediction" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The model is trained with cp = 0.006868132. cp is the complexity parameter for our dtree. We are ready to predict classes for our test set. We can use predict() method. Let’s try to predict target variable for test set’s 1st record.</p>
<div class="sourceCode" id="cb446"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb446-1" data-line-number="1">car.testing[<span class="dv">1</span>,]</a></code></pre></div>
<pre><code>##   buying maint doors persons lug_boot safety class
## 2  vhigh vhigh     2       2    small    med unacc</code></pre>
<div class="sourceCode" id="cb448"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb448-1" data-line-number="1"><span class="kw">predict</span>(dtree_fit, <span class="dt">newdata =</span> car.testing[<span class="dv">1</span>,])</a></code></pre></div>
<pre><code>## [1] unacc
## Levels: acc good unacc vgood</code></pre>
<div class="sourceCode" id="cb450"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb450-1" data-line-number="1"><span class="kw">predict</span>(dtree_fit_information, <span class="dt">newdata =</span> car.testing[<span class="dv">1</span>,])</a></code></pre></div>
<pre><code>## [1] unacc
## Levels: acc good unacc vgood</code></pre>
<p>In both models, the 1st record is predicted as unacc. Now, we predict target variable for the whole test set.</p>
<div class="sourceCode" id="cb452"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb452-1" data-line-number="1">test_pred &lt;-<span class="st"> </span><span class="kw">predict</span>(dtree_fit, <span class="dt">newdata =</span> car.testing)</a>
<a class="sourceLine" id="cb452-2" data-line-number="2"><span class="kw">confusionMatrix</span>(test_pred, <span class="kw">as.factor</span>(car.testing<span class="op">$</span>class) )</a></code></pre></div>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction acc good unacc vgood
##      acc    80    8    20     8
##      good    6    4     2     0
##      unacc  22    1   341     0
##      vgood   7    7     0    11
## 
## Overall Statistics
##                                           
##                Accuracy : 0.8433          
##                  95% CI : (0.8091, 0.8736)
##     No Information Rate : 0.7021          
##     P-Value [Acc &gt; NIR] : 6.824e-14       
##                                           
##                   Kappa : 0.6542          
##                                           
##  Mcnemar&#39;s Test P-Value : NA              
## 
## Statistics by Class:
## 
##                      Class: acc Class: good Class: unacc Class: vgood
## Sensitivity              0.6957    0.200000       0.9394      0.57895
## Specificity              0.9104    0.983903       0.8506      0.97189
## Pos Pred Value           0.6897    0.333333       0.9368      0.44000
## Neg Pred Value           0.9127    0.968317       0.8562      0.98374
## Prevalence               0.2224    0.038685       0.7021      0.03675
## Detection Rate           0.1547    0.007737       0.6596      0.02128
## Detection Prevalence     0.2244    0.023211       0.7041      0.04836
## Balanced Accuracy        0.8030    0.591952       0.8950      0.77542</code></pre>
<div class="sourceCode" id="cb454"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb454-1" data-line-number="1">test_pred_information &lt;-<span class="st"> </span><span class="kw">predict</span>(dtree_fit_information, <span class="dt">newdata =</span> car.testing)</a>
<a class="sourceLine" id="cb454-2" data-line-number="2"><span class="kw">confusionMatrix</span>(test_pred_information, <span class="kw">as.factor</span>(car.testing<span class="op">$</span>class) )</a></code></pre></div>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction acc good unacc vgood
##      acc    88    6    26     8
##      good    6    6     2     0
##      unacc  17    1   335     0
##      vgood   4    7     0    11
## 
## Overall Statistics
##                                           
##                Accuracy : 0.8511          
##                  95% CI : (0.8174, 0.8806)
##     No Information Rate : 0.7021          
##     P-Value [Acc &gt; NIR] : 2.18e-15        
##                                           
##                   Kappa : 0.6783          
##                                           
##  Mcnemar&#39;s Test P-Value : NA              
## 
## Statistics by Class:
## 
##                      Class: acc Class: good Class: unacc Class: vgood
## Sensitivity              0.7652     0.30000       0.9229      0.57895
## Specificity              0.9005     0.98390       0.8831      0.97791
## Pos Pred Value           0.6875     0.42857       0.9490      0.50000
## Neg Pred Value           0.9306     0.97217       0.8293      0.98384
## Prevalence               0.2224     0.03868       0.7021      0.03675
## Detection Rate           0.1702     0.01161       0.6480      0.02128
## Detection Prevalence     0.2476     0.02708       0.6828      0.04255
## Balanced Accuracy        0.8329     0.64195       0.9030      0.77843</code></pre>
<p>Although the 1st record was predicted the same in the Gini and information versions of the decision tree model, we see differences across the testing set.</p>
</div>
<div id="implementing-decision-trees-directly" class="section level3 hasAnchor">
<h3><span class="header-section-number">7.2.7</span> Implementing Decision Trees directly<a href="decision-trees.html#implementing-decision-trees-directly" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Both <em>rpart</em> and <em>C.50</em> are widely-used packages for Decision Trees. If you want to use the packages directly then you can implement them like this:</p>
<p><strong>rpart</strong></p>
<div class="sourceCode" id="cb456"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb456-1" data-line-number="1">rpart_dtree=<span class="kw">rpart</span>(class<span class="op">~</span>.,</a>
<a class="sourceLine" id="cb456-2" data-line-number="2">                  car.training,</a>
<a class="sourceLine" id="cb456-3" data-line-number="3">                  <span class="dt">parms =</span> <span class="kw">list</span>(<span class="dt">split=</span><span class="st">&quot;gini&quot;</span>),</a>
<a class="sourceLine" id="cb456-4" data-line-number="4">                  <span class="dt">cp=</span><span class="fl">0.006868132</span>)</a>
<a class="sourceLine" id="cb456-5" data-line-number="5"><span class="kw">prp</span>(rpart_dtree,<span class="dt">box.palette =</span> <span class="st">&#39;Reds&#39;</span>,<span class="dt">tweak=</span><span class="fl">1.2</span>)</a></code></pre></div>
<p><img src="06-decision-trees_files/figure-html/rpart%20direct%20gini-1.png" width="672" /></p>
<div class="sourceCode" id="cb457"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb457-1" data-line-number="1">rpart_test_pred &lt;-<span class="st"> </span><span class="kw">predict</span>(rpart_dtree, <span class="dt">newdata =</span> car.testing,<span class="dt">type =</span> <span class="st">&#39;class&#39;</span>)</a>
<a class="sourceLine" id="cb457-2" data-line-number="2"><span class="kw">confusionMatrix</span>(rpart_test_pred, <span class="kw">as.factor</span>(car.testing<span class="op">$</span>class) )</a></code></pre></div>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction acc good unacc vgood
##      acc   109    0    11     6
##      good    3   16     0     0
##      unacc   2    0   352     0
##      vgood   1    4     0    13
## 
## Overall Statistics
##                                           
##                Accuracy : 0.9478          
##                  95% CI : (0.9249, 0.9653)
##     No Information Rate : 0.7021          
##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16       
##                                           
##                   Kappa : 0.887           
##                                           
##  Mcnemar&#39;s Test P-Value : NA              
## 
## Statistics by Class:
## 
##                      Class: acc Class: good Class: unacc Class: vgood
## Sensitivity              0.9478     0.80000       0.9697      0.68421
## Specificity              0.9577     0.99396       0.9870      0.98996
## Pos Pred Value           0.8651     0.84211       0.9944      0.72222
## Neg Pred Value           0.9847     0.99197       0.9325      0.98798
## Prevalence               0.2224     0.03868       0.7021      0.03675
## Detection Rate           0.2108     0.03095       0.6809      0.02515
## Detection Prevalence     0.2437     0.03675       0.6847      0.03482
## Balanced Accuracy        0.9528     0.89698       0.9784      0.83709</code></pre>
<div class="sourceCode" id="cb459"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb459-1" data-line-number="1">rpart_dtree_information=<span class="kw">rpart</span>(class<span class="op">~</span>.,</a>
<a class="sourceLine" id="cb459-2" data-line-number="2">                              car.training,</a>
<a class="sourceLine" id="cb459-3" data-line-number="3">                              <span class="dt">parms =</span> <span class="kw">list</span>(<span class="dt">split=</span><span class="st">&quot;information&quot;</span>),</a>
<a class="sourceLine" id="cb459-4" data-line-number="4">                              <span class="dt">cp=</span><span class="fl">0.005494505</span>)</a>
<a class="sourceLine" id="cb459-5" data-line-number="5"><span class="kw">prp</span>(rpart_dtree_information,<span class="dt">box.palette =</span> <span class="st">&#39;Blues&#39;</span>,<span class="dt">tweak=</span><span class="fl">1.2</span>)</a></code></pre></div>
<p><img src="06-decision-trees_files/figure-html/rpart%20direct%20information-1.png" width="672" /></p>
<div class="sourceCode" id="cb460"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb460-1" data-line-number="1">rpart_test_pred_information &lt;-<span class="st"> </span><span class="kw">predict</span>(rpart_dtree_information, <span class="dt">newdata =</span> car.testing,<span class="dt">type =</span> <span class="st">&#39;class&#39;</span>)</a>
<a class="sourceLine" id="cb460-2" data-line-number="2"><span class="kw">confusionMatrix</span>(rpart_test_pred_information, <span class="kw">as.factor</span>(car.testing<span class="op">$</span>class) )</a></code></pre></div>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction acc good unacc vgood
##      acc   108    4     8     6
##      good    1   12     0     0
##      unacc   5    0   355     0
##      vgood   1    4     0    13
## 
## Overall Statistics
##                                           
##                Accuracy : 0.9439          
##                  95% CI : (0.9204, 0.9621)
##     No Information Rate : 0.7021          
##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16       
##                                           
##                   Kappa : 0.8766          
##                                           
##  Mcnemar&#39;s Test P-Value : NA              
## 
## Statistics by Class:
## 
##                      Class: acc Class: good Class: unacc Class: vgood
## Sensitivity              0.9391     0.60000       0.9780      0.68421
## Specificity              0.9552     0.99799       0.9675      0.98996
## Pos Pred Value           0.8571     0.92308       0.9861      0.72222
## Neg Pred Value           0.9821     0.98413       0.9490      0.98798
## Prevalence               0.2224     0.03868       0.7021      0.03675
## Detection Rate           0.2089     0.02321       0.6867      0.02515
## Detection Prevalence     0.2437     0.02515       0.6963      0.03482
## Balanced Accuracy        0.9472     0.79899       0.9727      0.83709</code></pre>
<p><strong>C.50</strong></p>
<div class="sourceCode" id="cb462"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb462-1" data-line-number="1"><span class="kw">library</span>(C50)</a>
<a class="sourceLine" id="cb462-2" data-line-number="2">car.training.factors=<span class="st"> </span><span class="kw">as.data.frame</span>(<span class="kw">lapply</span>(car.training,as.factor))</a>
<a class="sourceLine" id="cb462-3" data-line-number="3">car.testing.factors=<span class="st"> </span><span class="kw">as.data.frame</span>(<span class="kw">lapply</span>(car.testing,as.factor))</a>
<a class="sourceLine" id="cb462-4" data-line-number="4">c50_dtree &lt;-<span class="st"> </span><span class="kw">C5.0</span>(<span class="dt">x =</span> car.training.factors[, <span class="dv">1</span><span class="op">:</span><span class="dv">6</span>], <span class="dt">y =</span> car.training.factors<span class="op">$</span>class)</a>
<a class="sourceLine" id="cb462-5" data-line-number="5"><span class="kw">summary</span>(c50_dtree)</a></code></pre></div>
<pre><code>## 
## Call:
## C5.0.default(x = car.training.factors[, 1:6], y = car.training.factors$class)
## 
## 
## C5.0 [Release 2.07 GPL Edition]      Sun Jan 15 21:32:04 2023
## -------------------------------
## 
## Class specified by attribute `outcome&#39;
## 
## Read 1211 cases (7 attributes) from undefined.data
## 
## Decision tree:
## 
## persons = 2: unacc (409)
## persons in {4,more}:
## :...safety = low: unacc (261)
##     safety in {high,med}:
##     :...buying in {low,med}:
##         :...maint in {high,vhigh}:
##         :   :...lug_boot = big:
##         :   :   :...buying = med: acc (23)
##         :   :   :   buying = low:
##         :   :   :   :...safety = med: acc (12)
##         :   :   :       safety = high:
##         :   :   :       :...maint = high: vgood (6)
##         :   :   :           maint = vhigh: acc (6)
##         :   :   lug_boot in {med,small}:
##         :   :   :...safety = high: acc (41/4)
##         :   :       safety = med:
##         :   :       :...lug_boot = med:
##         :   :           :...doors = 2: unacc (6/1)
##         :   :           :   doors in {4,5more}: acc (13)
##         :   :           :   doors = 3:
##         :   :           :   :...persons = 4: unacc (3/1)
##         :   :           :       persons = more: acc (4)
##         :   :           lug_boot = small:
##         :   :           :...buying = med: unacc (13)
##         :   :               buying = low:
##         :   :               :...maint = high: acc (5)
##         :   :                   maint = vhigh: unacc (6)
##         :   maint in {low,med}:
##         :   :...safety = med:
##         :       :...lug_boot = small:
##         :       :   :...doors in {3,4,5more}: acc (15)
##         :       :   :   doors = 2:
##         :       :   :   :...persons = 4: acc (3)
##         :       :   :       persons = more: unacc (4)
##         :       :   lug_boot in {big,med}:
##         :       :   :...maint = med:
##         :       :       :...buying = low: good (14/3)
##         :       :       :   buying = med: acc (11)
##         :       :       maint = low:
##         :       :       :...lug_boot = big: good (12)
##         :       :           lug_boot = med:
##         :       :           :...doors in {2,3}: acc (6/1)
##         :       :               doors in {4,5more}: good (7)
##         :       safety = high:
##         :       :...lug_boot = small:
##         :           :...doors = 2:
##         :           :   :...persons = 4: acc (2/1)
##         :           :   :   persons = more: unacc (4)
##         :           :   doors in {3,4,5more}:
##         :           :   :...buying = low: good (9)
##         :           :       buying = med:
##         :           :       :...maint = low: good (3)
##         :           :           maint = med: acc (4)
##         :           lug_boot in {big,med}:
##         :           :...lug_boot = big: vgood (23)
##         :               lug_boot = med:
##         :               :...doors = 2:
##         :                   :...buying = low: good (2)
##         :                   :   buying = med: acc (3/1)
##         :                   doors in {3,4,5more}:
##         :                   :...persons = more: vgood (11)
##         :                       persons = 4:
##         :                       :...doors = 3: good (2)
##         :                           doors in {4,5more}: vgood (5)
##         buying in {high,vhigh}:
##         :...maint = vhigh: unacc (65)
##             maint in {high,low,med}:
##             :...lug_boot = small:
##                 :...safety = med: unacc (32)
##                 :   safety = high:
##                 :   :...maint in {low,med}: acc (19/3)
##                 :       maint = high:
##                 :       :...buying = high: acc (5/1)
##                 :           buying = vhigh: unacc (5)
##                 lug_boot in {big,med}:
##                 :...maint = high:
##                     :...buying = high: acc (25/3)
##                     :   buying = vhigh: unacc (23)
##                     maint in {low,med}:
##                     :...lug_boot = big: acc (47)
##                         lug_boot = med:
##                         :...safety = high: acc (20)
##                             safety = med:
##                             :...doors in {4,5more}: acc (10)
##                                 doors in {2,3}:
##                                 :...persons = 4: unacc (6)
##                                     persons = more:
##                                     :...doors = 2: unacc (2)
##                                         doors = 3: acc (4)
## 
## 
## Evaluation on training data (1211 cases):
## 
##      Decision Tree   
##    ----------------  
##    Size      Errors  
## 
##      46   19( 1.6%)   &lt;&lt;
## 
## 
##     (a)   (b)   (c)   (d)    &lt;-classified as
##    ----  ----  ----  ----
##     264     3     2          (a): class acc
##       3    46                (b): class good
##      10         837          (c): class unacc
##       1                45    (d): class vgood
## 
## 
##  Attribute usage:
## 
##  100.00% persons
##   66.23% safety
##   44.67% buying
##   44.67% maint
##   39.31% lug_boot
##   10.57% doors
## 
## 
## Time: 0.0 secs</code></pre>
<div class="sourceCode" id="cb464"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb464-1" data-line-number="1">c50_test_pred &lt;-<span class="st"> </span><span class="kw">predict</span>(c50_dtree, <span class="dt">newdata =</span> car.testing.factors,<span class="dt">type =</span> <span class="st">&#39;class&#39;</span>)</a>
<a class="sourceLine" id="cb464-2" data-line-number="2"><span class="kw">confusionMatrix</span>(c50_test_pred, <span class="kw">as.factor</span>(car.testing.factors<span class="op">$</span>class) )</a></code></pre></div>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction acc good unacc vgood
##      acc   113    4     3     4
##      good    1   16     0     0
##      unacc   1    0   360     0
##      vgood   0    0     0    15
## 
## Overall Statistics
##                                           
##                Accuracy : 0.9749          
##                  95% CI : (0.9574, 0.9865)
##     No Information Rate : 0.7021          
##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16       
##                                           
##                   Kappa : 0.9446          
##                                           
##  Mcnemar&#39;s Test P-Value : NA              
## 
## Statistics by Class:
## 
##                      Class: acc Class: good Class: unacc Class: vgood
## Sensitivity              0.9826     0.80000       0.9917      0.78947
## Specificity              0.9726     0.99799       0.9935      1.00000
## Pos Pred Value           0.9113     0.94118       0.9972      1.00000
## Neg Pred Value           0.9949     0.99200       0.9808      0.99203
## Prevalence               0.2224     0.03868       0.7021      0.03675
## Detection Rate           0.2186     0.03095       0.6963      0.02901
## Detection Prevalence     0.2398     0.03288       0.6983      0.02901
## Balanced Accuracy        0.9776     0.89899       0.9926      0.89474</code></pre>
</div>
<div id="iris-example-for-decision-trees" class="section level3 hasAnchor">
<h3><span class="header-section-number">7.2.8</span> Iris example for Decision Trees<a href="decision-trees.html#iris-example-for-decision-trees" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We can use the same pipeline for the Iris dataset but we need to remember to scale, centre and check for highly correlation variables and skewness because now we have numeric predictors.</p>
<div class="sourceCode" id="cb466"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb466-1" data-line-number="1"><span class="kw">library</span>(datasets)</a>
<a class="sourceLine" id="cb466-2" data-line-number="2"><span class="kw">data</span>(iris) <span class="co">##loads the dataset, which can be accessed under the variable name iris</span></a>
<a class="sourceLine" id="cb466-3" data-line-number="3"><span class="co">## ?iris opens the documentation for the dataset</span></a>
<a class="sourceLine" id="cb466-4" data-line-number="4"><span class="kw">summary</span>(iris) <span class="co">##presents the 5 figure summary of the dataset</span></a></code></pre></div>
<pre><code>##   Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   
##  Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100  
##  1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300  
##  Median :5.800   Median :3.000   Median :4.350   Median :1.300  
##  Mean   :5.843   Mean   :3.057   Mean   :3.758   Mean   :1.199  
##  3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800  
##  Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500  
##        Species  
##  setosa    :50  
##  versicolor:50  
##  virginica :50  
##                 
##                 
## </code></pre>
<div class="sourceCode" id="cb468"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb468-1" data-line-number="1"><span class="kw">str</span>(iris) <span class="co">##presents the structure of the iris dataframe</span></a></code></pre></div>
<pre><code>## &#39;data.frame&#39;:    150 obs. of  5 variables:
##  $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...
##  $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...
##  $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...
##  $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...
##  $ Species     : Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 1 1 1 1 1 1 1 1 1 ...</code></pre>
<div class="sourceCode" id="cb470"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb470-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">42</span>)</a>
<a class="sourceLine" id="cb470-2" data-line-number="2">trainTestPartition&lt;-<span class="kw">createDataPartition</span>(<span class="dt">y=</span>iris<span class="op">$</span>Species, <span class="co">#the class label, caret ensures an even split of classes</span></a>
<a class="sourceLine" id="cb470-3" data-line-number="3">                                        <span class="dt">p=</span><span class="fl">0.7</span>, <span class="co">#proportion of samples assigned to train</span></a>
<a class="sourceLine" id="cb470-4" data-line-number="4">                                        <span class="dt">list=</span><span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb470-5" data-line-number="5"><span class="kw">str</span>(trainTestPartition)</a></code></pre></div>
<pre><code>##  int [1:105, 1] 1 2 3 4 5 7 8 10 11 12 ...
##  - attr(*, &quot;dimnames&quot;)=List of 2
##   ..$ : NULL
##   ..$ : chr &quot;Resample1&quot;</code></pre>
<div class="sourceCode" id="cb472"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb472-1" data-line-number="1">iris.training &lt;-<span class="st"> </span>iris[ trainTestPartition,] <span class="co">#take the corresponding rows for training</span></a>
<a class="sourceLine" id="cb472-2" data-line-number="2">iris.testing  &lt;-<span class="st"> </span>iris[<span class="op">-</span>trainTestPartition,] <span class="co">#take the corresponding rows for testing by removing training rows</span></a></code></pre></div>
<div class="sourceCode" id="cb473"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb473-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">42</span>)</a>
<a class="sourceLine" id="cb473-2" data-line-number="2">seeds =<span class="st"> </span><span class="kw">vector</span>(<span class="dt">mode =</span> <span class="st">&#39;list&#39;</span>, <span class="dt">length =</span> <span class="dv">101</span>) <span class="co">#you need length #folds*#repeats + 1 so 10*10 + 1 here</span></a>
<a class="sourceLine" id="cb473-3" data-line-number="3"><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">100</span>) seeds[[i]] =<span class="st"> </span><span class="kw">sample.int</span>(<span class="dv">1000</span>,<span class="dv">10</span>)</a>
<a class="sourceLine" id="cb473-4" data-line-number="4">seeds[[<span class="dv">101</span>]] =<span class="st"> </span><span class="kw">sample.int</span>(<span class="dv">1000</span>,<span class="dv">1</span>)</a>
<a class="sourceLine" id="cb473-5" data-line-number="5"></a>
<a class="sourceLine" id="cb473-6" data-line-number="6">train_ctrl_seed_repeated &lt;-<span class="st"> </span><span class="kw">trainControl</span>(<span class="dt">method=</span><span class="st">&#39;repeatedcv&#39;</span>,</a>
<a class="sourceLine" id="cb473-7" data-line-number="7">                              <span class="dt">number=</span><span class="dv">10</span>, <span class="co">#number of folds</span></a>
<a class="sourceLine" id="cb473-8" data-line-number="8">                              <span class="dt">repeats=</span><span class="dv">10</span>, <span class="co">#number of times to repeat cross-validation</span></a>
<a class="sourceLine" id="cb473-9" data-line-number="9">                              <span class="dt">seeds=</span>seeds)</a>
<a class="sourceLine" id="cb473-10" data-line-number="10">iris_dtree &lt;-<span class="st"> </span><span class="kw">train</span>(</a>
<a class="sourceLine" id="cb473-11" data-line-number="11">                  Species <span class="op">~</span><span class="st"> </span>.,</a>
<a class="sourceLine" id="cb473-12" data-line-number="12">                  <span class="dt">data =</span> iris.training,</a>
<a class="sourceLine" id="cb473-13" data-line-number="13">                  <span class="dt">method =</span> <span class="st">&quot;rpart&quot;</span>,</a>
<a class="sourceLine" id="cb473-14" data-line-number="14">                  <span class="dt">parms =</span> <span class="kw">list</span>(<span class="dt">split =</span> <span class="st">&quot;gini&quot;</span>),</a>
<a class="sourceLine" id="cb473-15" data-line-number="15">                  <span class="dt">preProc =</span> <span class="kw">c</span>(<span class="st">&quot;corr&quot;</span>,<span class="st">&quot;nzv&quot;</span>,<span class="st">&quot;center&quot;</span>, <span class="st">&quot;scale&quot;</span>,<span class="st">&quot;BoxCox&quot;</span>),</a>
<a class="sourceLine" id="cb473-16" data-line-number="16">                  <span class="dt">tuneLength=</span><span class="dv">10</span>,</a>
<a class="sourceLine" id="cb473-17" data-line-number="17">                  <span class="dt">trControl =</span> train_ctrl_seed_repeated</a>
<a class="sourceLine" id="cb473-18" data-line-number="18">)</a>
<a class="sourceLine" id="cb473-19" data-line-number="19">iris_dtree</a></code></pre></div>
<pre><code>## CART 
## 
## 105 samples
##   4 predictor
##   3 classes: &#39;setosa&#39;, &#39;versicolor&#39;, &#39;virginica&#39; 
## 
## Pre-processing: centered (3), scaled (3), Box-Cox transformation (3), remove (1) 
## Resampling: Cross-Validated (10 fold, repeated 10 times) 
## Summary of sample sizes: 94, 95, 94, 93, 95, 95, ... 
## Resampling results across tuning parameters:
## 
##   cp          Accuracy   Kappa     
##   0.00000000  0.9713081  0.95637362
##   0.05555556  0.9713081  0.95637362
##   0.11111111  0.9713081  0.95637362
##   0.16666667  0.9713081  0.95637362
##   0.22222222  0.9713081  0.95637362
##   0.27777778  0.9713081  0.95637362
##   0.33333333  0.9713081  0.95637362
##   0.38888889  0.9713081  0.95637362
##   0.44444444  0.9113081  0.87065933
##   0.50000000  0.3666970  0.09857143
## 
## Accuracy was used to select the optimal model using the largest value.
## The final value used for the model was cp = 0.3888889.</code></pre>
<div class="sourceCode" id="cb475"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb475-1" data-line-number="1"><span class="kw">prp</span>(iris_dtree<span class="op">$</span>finalModel,<span class="dt">box.palette =</span> <span class="st">&#39;Reds&#39;</span>,<span class="dt">tweak=</span><span class="fl">1.2</span>)</a></code></pre></div>
<p><img src="06-decision-trees_files/figure-html/train%20gini%20dtree%20iris-1.png" width="672" /></p>
<div class="sourceCode" id="cb476"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb476-1" data-line-number="1">iris_information_predict_train &lt;-<span class="st"> </span><span class="kw">predict</span>(iris_dtree,iris.training,<span class="dt">type=</span><span class="st">&#39;raw&#39;</span>)</a>
<a class="sourceLine" id="cb476-2" data-line-number="2"><span class="kw">confusionMatrix</span>(iris_information_predict_train,iris.training<span class="op">$</span>Species)</a></code></pre></div>
<pre><code>## Confusion Matrix and Statistics
## 
##             Reference
## Prediction   setosa versicolor virginica
##   setosa         35          0         0
##   versicolor      0         34         2
##   virginica       0          1        33
## 
## Overall Statistics
##                                           
##                Accuracy : 0.9714          
##                  95% CI : (0.9188, 0.9941)
##     No Information Rate : 0.3333          
##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16       
##                                           
##                   Kappa : 0.9571          
##                                           
##  Mcnemar&#39;s Test P-Value : NA              
## 
## Statistics by Class:
## 
##                      Class: setosa Class: versicolor Class: virginica
## Sensitivity                 1.0000            0.9714           0.9429
## Specificity                 1.0000            0.9714           0.9857
## Pos Pred Value              1.0000            0.9444           0.9706
## Neg Pred Value              1.0000            0.9855           0.9718
## Prevalence                  0.3333            0.3333           0.3333
## Detection Rate              0.3333            0.3238           0.3143
## Detection Prevalence        0.3333            0.3429           0.3238
## Balanced Accuracy           1.0000            0.9714           0.9643</code></pre>
<div class="sourceCode" id="cb478"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb478-1" data-line-number="1">iris_gini_predict &lt;-<span class="st"> </span><span class="kw">predict</span>(iris_dtree,iris.testing,<span class="dt">type=</span><span class="st">&#39;raw&#39;</span>)</a>
<a class="sourceLine" id="cb478-2" data-line-number="2"><span class="kw">confusionMatrix</span>(iris_gini_predict,iris.testing<span class="op">$</span>Species)</a></code></pre></div>
<pre><code>## Confusion Matrix and Statistics
## 
##             Reference
## Prediction   setosa versicolor virginica
##   setosa         15          0         0
##   versicolor      0         14         2
##   virginica       0          1        13
## 
## Overall Statistics
##                                          
##                Accuracy : 0.9333         
##                  95% CI : (0.8173, 0.986)
##     No Information Rate : 0.3333         
##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16      
##                                          
##                   Kappa : 0.9            
##                                          
##  Mcnemar&#39;s Test P-Value : NA             
## 
## Statistics by Class:
## 
##                      Class: setosa Class: versicolor Class: virginica
## Sensitivity                 1.0000            0.9333           0.8667
## Specificity                 1.0000            0.9333           0.9667
## Pos Pred Value              1.0000            0.8750           0.9286
## Neg Pred Value              1.0000            0.9655           0.9355
## Prevalence                  0.3333            0.3333           0.3333
## Detection Rate              0.3333            0.3111           0.2889
## Detection Prevalence        0.3333            0.3556           0.3111
## Balanced Accuracy           1.0000            0.9333           0.9167</code></pre>
<div class="sourceCode" id="cb480"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb480-1" data-line-number="1">iris_dtree_information &lt;-<span class="st"> </span><span class="kw">train</span>(</a>
<a class="sourceLine" id="cb480-2" data-line-number="2">                          Species <span class="op">~</span><span class="st"> </span>.,</a>
<a class="sourceLine" id="cb480-3" data-line-number="3">                          <span class="dt">data =</span> iris.training,</a>
<a class="sourceLine" id="cb480-4" data-line-number="4">                          <span class="dt">method =</span> <span class="st">&quot;rpart&quot;</span>,</a>
<a class="sourceLine" id="cb480-5" data-line-number="5">                          <span class="dt">parms =</span> <span class="kw">list</span>(<span class="dt">split =</span> <span class="st">&quot;information&quot;</span>),</a>
<a class="sourceLine" id="cb480-6" data-line-number="6">                          <span class="dt">preProc =</span> <span class="kw">c</span>(<span class="st">&quot;corr&quot;</span>,<span class="st">&quot;nzv&quot;</span>,<span class="st">&quot;center&quot;</span>, <span class="st">&quot;scale&quot;</span>,<span class="st">&quot;BoxCox&quot;</span>),</a>
<a class="sourceLine" id="cb480-7" data-line-number="7">                          <span class="dt">tuneLength=</span><span class="dv">10</span>,</a>
<a class="sourceLine" id="cb480-8" data-line-number="8">                          <span class="dt">trControl =</span> train_ctrl_seed_repeated</a>
<a class="sourceLine" id="cb480-9" data-line-number="9">)</a>
<a class="sourceLine" id="cb480-10" data-line-number="10"></a>
<a class="sourceLine" id="cb480-11" data-line-number="11">iris_dtree_information</a></code></pre></div>
<pre><code>## CART 
## 
## 105 samples
##   4 predictor
##   3 classes: &#39;setosa&#39;, &#39;versicolor&#39;, &#39;virginica&#39; 
## 
## Pre-processing: centered (3), scaled (3), Box-Cox transformation (3), remove (1) 
## Resampling: Cross-Validated (10 fold, repeated 10 times) 
## Summary of sample sizes: 95, 94, 95, 94, 94, 95, ... 
## Resampling results across tuning parameters:
## 
##   cp          Accuracy   Kappa    
##   0.00000000  0.9710354  0.9561669
##   0.05555556  0.9710354  0.9561669
##   0.11111111  0.9710354  0.9561669
##   0.16666667  0.9710354  0.9561669
##   0.22222222  0.9710354  0.9561669
##   0.27777778  0.9710354  0.9561669
##   0.33333333  0.9710354  0.9561669
##   0.38888889  0.9710354  0.9561669
##   0.44444444  0.8950354  0.8475954
##   0.50000000  0.3718182  0.1071429
## 
## Accuracy was used to select the optimal model using the largest value.
## The final value used for the model was cp = 0.3888889.</code></pre>
<div class="sourceCode" id="cb482"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb482-1" data-line-number="1"><span class="kw">prp</span>(iris_dtree_information<span class="op">$</span>finalModel, <span class="dt">box.palette =</span> <span class="st">&#39;Blues&#39;</span>, <span class="dt">tweak=</span><span class="fl">1.2</span>)</a></code></pre></div>
<p><img src="06-decision-trees_files/figure-html/train%20information%20dtree%20iris-1.png" width="672" /></p>
<div class="sourceCode" id="cb483"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb483-1" data-line-number="1">iris_information_predict_train=<span class="kw">predict</span>(iris_dtree_information, iris.training, <span class="dt">type =</span> <span class="st">&#39;raw&#39;</span>)</a>
<a class="sourceLine" id="cb483-2" data-line-number="2"><span class="kw">confusionMatrix</span>(iris_information_predict_train, iris.training<span class="op">$</span>Species)</a></code></pre></div>
<pre><code>## Confusion Matrix and Statistics
## 
##             Reference
## Prediction   setosa versicolor virginica
##   setosa         35          0         0
##   versicolor      0         34         2
##   virginica       0          1        33
## 
## Overall Statistics
##                                           
##                Accuracy : 0.9714          
##                  95% CI : (0.9188, 0.9941)
##     No Information Rate : 0.3333          
##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16       
##                                           
##                   Kappa : 0.9571          
##                                           
##  Mcnemar&#39;s Test P-Value : NA              
## 
## Statistics by Class:
## 
##                      Class: setosa Class: versicolor Class: virginica
## Sensitivity                 1.0000            0.9714           0.9429
## Specificity                 1.0000            0.9714           0.9857
## Pos Pred Value              1.0000            0.9444           0.9706
## Neg Pred Value              1.0000            0.9855           0.9718
## Prevalence                  0.3333            0.3333           0.3333
## Detection Rate              0.3333            0.3238           0.3143
## Detection Prevalence        0.3333            0.3429           0.3238
## Balanced Accuracy           1.0000            0.9714           0.9643</code></pre>
<div class="sourceCode" id="cb485"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb485-1" data-line-number="1">iris_information_predict=<span class="kw">predict</span>(iris_dtree_information, iris.testing, <span class="dt">type =</span> <span class="st">&#39;raw&#39;</span>)</a>
<a class="sourceLine" id="cb485-2" data-line-number="2"><span class="kw">confusionMatrix</span>(iris_information_predict, iris.testing<span class="op">$</span>Species)</a></code></pre></div>
<pre><code>## Confusion Matrix and Statistics
## 
##             Reference
## Prediction   setosa versicolor virginica
##   setosa         15          0         0
##   versicolor      0         14         2
##   virginica       0          1        13
## 
## Overall Statistics
##                                          
##                Accuracy : 0.9333         
##                  95% CI : (0.8173, 0.986)
##     No Information Rate : 0.3333         
##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16      
##                                          
##                   Kappa : 0.9            
##                                          
##  Mcnemar&#39;s Test P-Value : NA             
## 
## Statistics by Class:
## 
##                      Class: setosa Class: versicolor Class: virginica
## Sensitivity                 1.0000            0.9333           0.8667
## Specificity                 1.0000            0.9333           0.9667
## Pos Pred Value              1.0000            0.8750           0.9286
## Neg Pred Value              1.0000            0.9655           0.9355
## Prevalence                  0.3333            0.3333           0.3333
## Detection Rate              0.3333            0.3111           0.2889
## Detection Prevalence        0.3333            0.3556           0.3111
## Balanced Accuracy           1.0000            0.9333           0.9167</code></pre>
<p>In this case, the output models are the same for the information and gini split parameter choices but this is a very simple model which does not capture the patterns in the data fully.</p>
</div>
</div>
<div id="cell-segmentation-examples" class="section level2 hasAnchor">
<h2><span class="header-section-number">7.3</span> Cell segmentation examples<a href="decision-trees.html#cell-segmentation-examples" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Load required libraries</p>
<div class="sourceCode" id="cb487"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb487-1" data-line-number="1"><span class="kw">library</span>(caret)</a>
<a class="sourceLine" id="cb487-2" data-line-number="2"><span class="kw">library</span>(pROC)</a>
<a class="sourceLine" id="cb487-3" data-line-number="3"><span class="kw">library</span>(e1071)</a></code></pre></div>
<p>Load data</p>
<div class="sourceCode" id="cb488"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb488-1" data-line-number="1"><span class="kw">data</span>(segmentationData)</a>
<a class="sourceLine" id="cb488-2" data-line-number="2">segClass &lt;-<span class="st"> </span>segmentationData<span class="op">$</span>Class</a>
<a class="sourceLine" id="cb488-3" data-line-number="3">segData &lt;-<span class="st"> </span>segmentationData[,<span class="dv">4</span><span class="op">:</span><span class="dv">59</span>] <span class="co">##Extract predictors from segmentationData</span></a></code></pre></div>
<p>Partition data</p>
<div class="sourceCode" id="cb489"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb489-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">42</span>)</a>
<a class="sourceLine" id="cb489-2" data-line-number="2">trainIndex &lt;-<span class="st"> </span><span class="kw">createDataPartition</span>(<span class="dt">y=</span>segClass, <span class="dt">times=</span><span class="dv">1</span>, <span class="dt">p=</span><span class="fl">0.5</span>, <span class="dt">list=</span>F)</a>
<a class="sourceLine" id="cb489-3" data-line-number="3">segDataTrain &lt;-<span class="st"> </span>segData[trainIndex,]</a>
<a class="sourceLine" id="cb489-4" data-line-number="4">segDataTest &lt;-<span class="st"> </span>segData[<span class="op">-</span>trainIndex,]</a>
<a class="sourceLine" id="cb489-5" data-line-number="5">segClassTrain &lt;-<span class="st"> </span>segClass[trainIndex]</a>
<a class="sourceLine" id="cb489-6" data-line-number="6">segClassTest &lt;-<span class="st"> </span>segClass[<span class="op">-</span>trainIndex]</a></code></pre></div>
<p>Set seeds for reproducibility (optional). We will be trying 9 values of the tuning parameter with 5 repeats of 10 fold cross-validation, so we need the following list of seeds.</p>
<div class="sourceCode" id="cb490"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb490-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">42</span>)</a>
<a class="sourceLine" id="cb490-2" data-line-number="2">seeds &lt;-<span class="st"> </span><span class="kw">vector</span>(<span class="dt">mode =</span> <span class="st">&quot;list&quot;</span>, <span class="dt">length =</span> <span class="dv">51</span>)</a>
<a class="sourceLine" id="cb490-3" data-line-number="3"><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">50</span>) seeds[[i]] &lt;-<span class="st"> </span><span class="kw">sample.int</span>(<span class="dv">1000</span>, <span class="dv">9</span>)</a>
<a class="sourceLine" id="cb490-4" data-line-number="4">seeds[[<span class="dv">51</span>]] &lt;-<span class="st"> </span><span class="kw">sample.int</span>(<span class="dv">1000</span>,<span class="dv">1</span>)</a></code></pre></div>
<p>We will pass the twoClassSummary function into model training through <strong>trainControl</strong>. Additionally we would like the model to predict class probabilities so that we can calculate the ROC curve, so we use the <strong>classProbs</strong> option.</p>
<div class="sourceCode" id="cb491"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb491-1" data-line-number="1">cvCtrl &lt;-<span class="st"> </span><span class="kw">trainControl</span>(<span class="dt">method =</span> <span class="st">&quot;repeatedcv&quot;</span>, </a>
<a class="sourceLine" id="cb491-2" data-line-number="2">                       <span class="dt">repeats =</span> <span class="dv">5</span>,</a>
<a class="sourceLine" id="cb491-3" data-line-number="3">                       <span class="dt">number =</span> <span class="dv">10</span>,</a>
<a class="sourceLine" id="cb491-4" data-line-number="4">                       <span class="dt">summaryFunction =</span> twoClassSummary,</a>
<a class="sourceLine" id="cb491-5" data-line-number="5">                       <span class="dt">classProbs =</span> <span class="ot">TRUE</span>,</a>
<a class="sourceLine" id="cb491-6" data-line-number="6">                       <span class="dt">seeds=</span>seeds)</a></code></pre></div>
<div class="sourceCode" id="cb492"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb492-1" data-line-number="1">dtreeTune &lt;-<span class="st"> </span><span class="kw">train</span>(<span class="dt">x =</span> segDataTrain,</a>
<a class="sourceLine" id="cb492-2" data-line-number="2">                   <span class="dt">y =</span> segClassTrain,</a>
<a class="sourceLine" id="cb492-3" data-line-number="3">                   <span class="dt">method =</span> <span class="st">&#39;rpart&#39;</span>,</a>
<a class="sourceLine" id="cb492-4" data-line-number="4">                   <span class="dt">tuneLength =</span> <span class="dv">9</span>,</a>
<a class="sourceLine" id="cb492-5" data-line-number="5">                   <span class="dt">preProc =</span> <span class="kw">c</span>(<span class="st">&quot;center&quot;</span>, <span class="st">&quot;scale&quot;</span>),</a>
<a class="sourceLine" id="cb492-6" data-line-number="6">                   <span class="dt">metric =</span> <span class="st">&quot;ROC&quot;</span>,</a>
<a class="sourceLine" id="cb492-7" data-line-number="7">                   <span class="dt">trControl =</span> cvCtrl)</a>
<a class="sourceLine" id="cb492-8" data-line-number="8"></a>
<a class="sourceLine" id="cb492-9" data-line-number="9">dtreeTune</a></code></pre></div>
<pre><code>## CART 
## 
## 1010 samples
##   56 predictor
##    2 classes: &#39;PS&#39;, &#39;WS&#39; 
## 
## Pre-processing: centered (56), scaled (56) 
## Resampling: Cross-Validated (10 fold, repeated 5 times) 
## Summary of sample sizes: 909, 909, 909, 909, 909, 909, ... 
## Resampling results across tuning parameters:
## 
##   cp          ROC        Sens       Spec     
##   0.01018519  0.8294744  0.8347692  0.6738889
##   0.01250000  0.8296239  0.8347692  0.6827778
##   0.01388889  0.8300726  0.8326154  0.6883333
##   0.01805556  0.8229060  0.8276923  0.6833333
##   0.03333333  0.8249615  0.8187692  0.6888889
##   0.03611111  0.8247436  0.8212308  0.6822222
##   0.06111111  0.8200256  0.8292308  0.6533333
##   0.12777778  0.7979402  0.7664615  0.7116667
##   0.26666667  0.6813034  0.7353846  0.6272222
## 
## ROC was used to select the optimal model using the largest value.
## The final value used for the model was cp = 0.01388889.</code></pre>
<div class="sourceCode" id="cb494"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb494-1" data-line-number="1"><span class="kw">prp</span>(dtreeTune<span class="op">$</span>finalModel)</a></code></pre></div>
<p><img src="06-decision-trees_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<p>Decision tree accuracy profile</p>
<div class="sourceCode" id="cb495"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb495-1" data-line-number="1"><span class="kw">plot</span>(dtreeTune, <span class="dt">metric =</span> <span class="st">&quot;ROC&quot;</span>, <span class="dt">scales =</span> <span class="kw">list</span>(<span class="dt">x =</span> <span class="kw">list</span>(<span class="dt">log =</span><span class="dv">2</span>)))</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:dtreeAccuracyProfileCellSegment"></span>
<img src="06-decision-trees_files/figure-html/dtreeAccuracyProfileCellSegment-1.png" alt="dtree accuracy profile." width="80%" />
<p class="caption">
Figure 7.3: dtree accuracy profile.
</p>
</div>
<p>Test set results</p>
<div class="sourceCode" id="cb496"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb496-1" data-line-number="1"><span class="co">#segDataTest &lt;- predict(transformations, segDataTest)</span></a>
<a class="sourceLine" id="cb496-2" data-line-number="2">dtreePred &lt;-<span class="st"> </span><span class="kw">predict</span>(dtreeTune, segDataTest)</a>
<a class="sourceLine" id="cb496-3" data-line-number="3"><span class="kw">confusionMatrix</span>(dtreePred, segClassTest)</a></code></pre></div>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction  PS  WS
##         PS 548 107
##         WS 102 252
##                                           
##                Accuracy : 0.7929          
##                  95% CI : (0.7665, 0.8175)
##     No Information Rate : 0.6442          
##     P-Value [Acc &gt; NIR] : &lt;2e-16          
##                                           
##                   Kappa : 0.5467          
##                                           
##  Mcnemar&#39;s Test P-Value : 0.782           
##                                           
##             Sensitivity : 0.8431          
##             Specificity : 0.7019          
##          Pos Pred Value : 0.8366          
##          Neg Pred Value : 0.7119          
##              Prevalence : 0.6442          
##          Detection Rate : 0.5431          
##    Detection Prevalence : 0.6492          
##       Balanced Accuracy : 0.7725          
##                                           
##        &#39;Positive&#39; Class : PS              
## </code></pre>
<p>Get predicted class probabilities</p>
<div class="sourceCode" id="cb498"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb498-1" data-line-number="1">dtreeProbs &lt;-<span class="st"> </span><span class="kw">predict</span>(dtreeTune, segDataTest, <span class="dt">type=</span><span class="st">&quot;prob&quot;</span>)</a>
<a class="sourceLine" id="cb498-2" data-line-number="2"><span class="kw">head</span>(dtreeProbs)</a></code></pre></div>
<pre><code>##           PS         WS
## 1  0.9509346 0.04906542
## 5  0.9509346 0.04906542
## 8  0.9509346 0.04906542
## 9  0.9509346 0.04906542
## 10 0.9509346 0.04906542
## 12 0.9509346 0.04906542</code></pre>
<p>Build a ROC curve</p>
<div class="sourceCode" id="cb500"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb500-1" data-line-number="1">dtreeROC &lt;-<span class="st"> </span><span class="kw">roc</span>(segClassTest, dtreeProbs[,<span class="st">&quot;PS&quot;</span>])</a></code></pre></div>
<pre><code>## Setting levels: control = PS, case = WS</code></pre>
<pre><code>## Setting direction: controls &gt; cases</code></pre>
<div class="sourceCode" id="cb503"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb503-1" data-line-number="1"><span class="kw">auc</span>(dtreeROC)</a></code></pre></div>
<pre><code>## Area under the curve: 0.8374</code></pre>
<p>Plot ROC curve.</p>
<div class="sourceCode" id="cb505"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb505-1" data-line-number="1"><span class="kw">plot</span>(dtreeROC, <span class="dt">type =</span> <span class="st">&quot;S&quot;</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:dtreeROCcurveCellSegment"></span>
<img src="06-decision-trees_files/figure-html/dtreeROCcurveCellSegment-1.png" alt="dtree ROC curve for cell segmentation data set." width="80%" />
<p class="caption">
Figure 7.4: dtree ROC curve for cell segmentation data set.
</p>
</div>
<p>Calculate area under ROC curve</p>
<div class="sourceCode" id="cb506"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb506-1" data-line-number="1"><span class="kw">auc</span>(dtreeROC)</a></code></pre></div>
<pre><code>## Area under the curve: 0.8374</code></pre>
</div>
<div id="regression-example---blood-brain-barrier" class="section level2 hasAnchor">
<h2><span class="header-section-number">7.4</span> Regression example - Blood Brain Barrier<a href="decision-trees.html#regression-example---blood-brain-barrier" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>This example serves to demonstrate the use of decision trees and random forests in regression, but perhaps more importantly, it highlights the power and flexibility of the <a href="http://cran.r-project.org/web/packages/caret/index.html">caret</a> package. Earlier we used <em>k</em>-NN for a regression analysis of the <strong>BloodBrain</strong> dataset (see section 04-nearest-neighbours.Rmd). We will repeat the regression analysis, but this time we will fit a decision tree. Remarkably, a re-run of this analysis using a completely different type of model, requires changes to only two lines of code.</p>
<p>The pre-processing steps and generation of seeds are identical, therefore if the data were still in memory, we could skip this next block of code:</p>
<div class="sourceCode" id="cb508"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb508-1" data-line-number="1"><span class="kw">data</span>(BloodBrain)</a>
<a class="sourceLine" id="cb508-2" data-line-number="2"></a>
<a class="sourceLine" id="cb508-3" data-line-number="3"><span class="kw">set.seed</span>(<span class="dv">42</span>)</a>
<a class="sourceLine" id="cb508-4" data-line-number="4">trainIndex &lt;-<span class="st"> </span><span class="kw">createDataPartition</span>(<span class="dt">y=</span>logBBB, <span class="dt">times=</span><span class="dv">1</span>, <span class="dt">p=</span><span class="fl">0.8</span>, <span class="dt">list=</span>F)</a>
<a class="sourceLine" id="cb508-5" data-line-number="5">descrTrain &lt;-<span class="st"> </span>bbbDescr[trainIndex,]</a>
<a class="sourceLine" id="cb508-6" data-line-number="6">concRatioTrain &lt;-<span class="st"> </span>logBBB[trainIndex]</a>
<a class="sourceLine" id="cb508-7" data-line-number="7">descrTest &lt;-<span class="st"> </span>bbbDescr[<span class="op">-</span>trainIndex,]</a>
<a class="sourceLine" id="cb508-8" data-line-number="8">concRatioTest &lt;-<span class="st"> </span>logBBB[<span class="op">-</span>trainIndex]</a>
<a class="sourceLine" id="cb508-9" data-line-number="9"></a>
<a class="sourceLine" id="cb508-10" data-line-number="10">transformations &lt;-<span class="st"> </span><span class="kw">preProcess</span>(descrTrain,</a>
<a class="sourceLine" id="cb508-11" data-line-number="11">                              <span class="dt">method=</span><span class="kw">c</span>(<span class="st">&quot;center&quot;</span>, <span class="st">&quot;scale&quot;</span>, <span class="st">&quot;corr&quot;</span>, <span class="st">&quot;nzv&quot;</span>),</a>
<a class="sourceLine" id="cb508-12" data-line-number="12">                              <span class="dt">cutoff=</span><span class="fl">0.75</span>)</a>
<a class="sourceLine" id="cb508-13" data-line-number="13">descrTrain &lt;-<span class="st"> </span><span class="kw">predict</span>(transformations, descrTrain)</a>
<a class="sourceLine" id="cb508-14" data-line-number="14"></a>
<a class="sourceLine" id="cb508-15" data-line-number="15"><span class="kw">set.seed</span>(<span class="dv">42</span>)</a>
<a class="sourceLine" id="cb508-16" data-line-number="16">seeds &lt;-<span class="st"> </span><span class="kw">vector</span>(<span class="dt">mode =</span> <span class="st">&quot;list&quot;</span>, <span class="dt">length =</span> <span class="dv">26</span>)</a>
<a class="sourceLine" id="cb508-17" data-line-number="17"><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">25</span>) seeds[[i]] &lt;-<span class="st"> </span><span class="kw">sample.int</span>(<span class="dv">1000</span>, <span class="dv">50</span>)</a>
<a class="sourceLine" id="cb508-18" data-line-number="18">seeds[[<span class="dv">26</span>]] &lt;-<span class="st"> </span><span class="kw">sample.int</span>(<span class="dv">1000</span>,<span class="dv">1</span>)</a></code></pre></div>
<div id="decision-tree" class="section level3 hasAnchor">
<h3><span class="header-section-number">7.4.1</span> Decision tree<a href="decision-trees.html#decision-tree" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In the arguments to the <code>train</code> function we change <code>method</code> from <code>knn</code> to <code>rpart</code>. The <code>tunegrid</code> parameter is replaced with <code>tuneLength = 9</code>. Now we are ready to fit an decision tree model.</p>
<div class="sourceCode" id="cb509"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb509-1" data-line-number="1">dtTune &lt;-<span class="st"> </span><span class="kw">train</span>(descrTrain,</a>
<a class="sourceLine" id="cb509-2" data-line-number="2">                 concRatioTrain,</a>
<a class="sourceLine" id="cb509-3" data-line-number="3">                 <span class="dt">method=</span><span class="st">&#39;rpart&#39;</span>,</a>
<a class="sourceLine" id="cb509-4" data-line-number="4">                 <span class="dt">tuneLength =</span> <span class="dv">9</span>,</a>
<a class="sourceLine" id="cb509-5" data-line-number="5">                 <span class="dt">trControl =</span> <span class="kw">trainControl</span>(<span class="dt">method=</span><span class="st">&quot;repeatedcv&quot;</span>,</a>
<a class="sourceLine" id="cb509-6" data-line-number="6">                                          <span class="dt">number =</span> <span class="dv">5</span>,</a>
<a class="sourceLine" id="cb509-7" data-line-number="7">                                          <span class="dt">repeats =</span> <span class="dv">5</span>,</a>
<a class="sourceLine" id="cb509-8" data-line-number="8">                                          <span class="dt">seeds=</span>seeds</a>
<a class="sourceLine" id="cb509-9" data-line-number="9">                                          )</a>
<a class="sourceLine" id="cb509-10" data-line-number="10">)</a></code></pre></div>
<pre><code>## Warning in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo, :
## There were missing values in resampled performance measures.</code></pre>
<div class="sourceCode" id="cb511"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb511-1" data-line-number="1">dtTune</a></code></pre></div>
<pre><code>## CART 
## 
## 168 samples
##  63 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold, repeated 5 times) 
## Summary of sample sizes: 136, 134, 134, 134, 134, 134, ... 
## Resampling results across tuning parameters:
## 
##   cp          RMSE       Rsquared   MAE      
##   0.01806336  0.6814627  0.3379575  0.5261921
##   0.02477565  0.6835957  0.3318366  0.5269945
##   0.03155690  0.6849950  0.3219426  0.5252247
##   0.05241187  0.6955772  0.2937793  0.5323978
##   0.05465950  0.6969888  0.2907385  0.5346962
##   0.06435226  0.7004593  0.2773218  0.5352713
##   0.09019924  0.7109988  0.2554855  0.5449032
##   0.13642255  0.7351868  0.1863806  0.5697312
##   0.27502543  0.7531179  0.1609250  0.5912603
## 
## RMSE was used to select the optimal model using the smallest value.
## The final value used for the model was cp = 0.01806336.</code></pre>
<div class="sourceCode" id="cb513"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb513-1" data-line-number="1"><span class="kw">plot</span>(dtTune)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:rmseCordt"></span>
<img src="06-decision-trees_files/figure-html/rmseCordt-1.png" alt="Root Mean Squared Error as a function of cost." width="100%" />
<p class="caption">
Figure 7.5: Root Mean Squared Error as a function of cost.
</p>
</div>
<p>Use model to predict outcomes, after first pre-processing the test set.</p>
<div class="sourceCode" id="cb514"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb514-1" data-line-number="1">descrTest &lt;-<span class="st"> </span><span class="kw">predict</span>(transformations, descrTest)</a>
<a class="sourceLine" id="cb514-2" data-line-number="2">test_pred &lt;-<span class="st"> </span><span class="kw">predict</span>(dtTune, descrTest)</a></code></pre></div>
<p>Prediction performance can be visualized in a scatterplot.</p>
<div class="sourceCode" id="cb515"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb515-1" data-line-number="1"><span class="kw">qplot</span>(concRatioTest, test_pred) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb515-2" data-line-number="2"><span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;observed&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb515-3" data-line-number="3"><span class="st">  </span><span class="kw">ylab</span>(<span class="st">&quot;predicted&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb515-4" data-line-number="4"><span class="st">  </span><span class="kw">theme_bw</span>()</a></code></pre></div>
<pre><code>## Warning: `qplot()` was deprecated in ggplot2 3.4.0.</code></pre>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:obsPredConcRatiosdt"></span>
<img src="06-decision-trees_files/figure-html/obsPredConcRatiosdt-1.png" alt="Concordance between observed concentration ratios and those predicted by decision tree." width="80%" />
<p class="caption">
Figure 7.6: Concordance between observed concentration ratios and those predicted by decision tree.
</p>
</div>
<p>We can also measure correlation between observed and predicted values.</p>
<div class="sourceCode" id="cb517"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb517-1" data-line-number="1"><span class="kw">cor</span>(concRatioTest, test_pred)</a></code></pre></div>
<pre><code>## [1] 0.5496074</code></pre>
</div>
</div>
<div id="random-forest" class="section level2 hasAnchor">
<h2><span class="header-section-number">7.5</span> Random Forest<a href="decision-trees.html#random-forest" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>What is a Random Forest?</strong></p>
<p>It is a kind of ensemble learning method that combines a set of weak models to form a powerful model. In the process it reduces dimensionality, removes outliers, treats missing values, and more importantly it is both a regression and classification machine learning approach.</p>
<p><strong>How does it work?</strong></p>
<p>In Random Forest, multiple trees are grown as opposed to a single tree in a decision tree model. Assume number of cases in the training set is N. Then, sample of these N cases is taken at random but with replacement. This sample will be the training set for growing the tree. Each tree is grown to the largest extent possible and without pruning.</p>
<p>To classify a new object based on attributes, each tree gives a classification i.e. “votes” for that class. The forest chooses the classification having the most votes (over all the trees in the forest) and in case of regression, it takes the average of outputs by different trees.</p>
<p><strong>Key differences between decision trees and random forest</strong></p>
<p>Decision trees proceed by searching for a split on every variable in every node random forest searches for a split only on one variable in a node - the variable that has the largest association with the target among all other explanatory variables but only on a subset of randomly selected explanatory variables that is tested for that node. At every node a new list is selected.</p>
<p>Therefore, eligible variable set will be different from node to node but the important ones will eventually be “voted in” based on their success in predicting the target variable.</p>
<p>This random selection of explanatory variables at each node and which are different at each tree is known as bagging. For each tree the ratio between bagging and out of bagging is 60/40.</p>
<p>The important thing to note is that the trees are themselves not interpreted but they are used to collectively rank the importance of each variable.</p>
<p><strong>Example Random Forest code for binary classification</strong></p>
<p><em>Loading randomForest library</em></p>
<div class="sourceCode" id="cb519"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb519-1" data-line-number="1"><span class="kw">library</span>(randomForest)</a></code></pre></div>
<p>We will again use the car data which we used for the Decision Tree example.</p>
<div class="sourceCode" id="cb520"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb520-1" data-line-number="1">car.data &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&quot;data/Decision_tree_and_RF/car.data&quot;</span>, <span class="dt">sep =</span> <span class="st">&#39;,&#39;</span>, <span class="dt">header =</span> <span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb520-2" data-line-number="2"><span class="kw">colnames</span>(car.data)=<span class="kw">c</span>(<span class="st">&#39;buying&#39;</span>,<span class="st">&#39;maint&#39;</span>,<span class="st">&#39;doors&#39;</span>,<span class="st">&#39;persons&#39;</span>,<span class="st">&#39;lug_boot&#39;</span>,<span class="st">&#39;safety&#39;</span>,<span class="st">&#39;class&#39;</span>)</a>
<a class="sourceLine" id="cb520-3" data-line-number="3"><span class="kw">set.seed</span>(<span class="dv">42</span>)</a>
<a class="sourceLine" id="cb520-4" data-line-number="4">trainTestPartition &lt;-<span class="st"> </span><span class="kw">createDataPartition</span>(<span class="dt">y =</span> car.data<span class="op">$</span>class, <span class="dt">p=</span> <span class="fl">0.7</span>, <span class="dt">list =</span> <span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb520-5" data-line-number="5">car.training &lt;-<span class="st"> </span>car.data[trainTestPartition,]</a>
<a class="sourceLine" id="cb520-6" data-line-number="6">car.testing &lt;-<span class="st"> </span>car.data[<span class="op">-</span>trainTestPartition,]</a></code></pre></div>
<p>Input dataset has 20 independent variables and a target variable. The target variable y is binary.</p>
<p><em>Building Random Forest model</em></p>
<p>We will build 500 decision trees using randomForest.</p>
<div class="sourceCode" id="cb521"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb521-1" data-line-number="1">car.training.factors &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(<span class="kw">lapply</span>(car.training,as.factor))</a>
<a class="sourceLine" id="cb521-2" data-line-number="2">car_rf_direct &lt;-<span class="st"> </span><span class="kw">randomForest</span>(class<span class="op">~</span>.,</a>
<a class="sourceLine" id="cb521-3" data-line-number="3">                              car.training.factors,</a>
<a class="sourceLine" id="cb521-4" data-line-number="4">                              <span class="dt">ntree =</span> <span class="dv">500</span>,</a>
<a class="sourceLine" id="cb521-5" data-line-number="5">                              <span class="dt">importance =</span> T)</a>
<a class="sourceLine" id="cb521-6" data-line-number="6">error.rates &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(car_rf_direct<span class="op">$</span>err.rate)</a>
<a class="sourceLine" id="cb521-7" data-line-number="7">error.rates<span class="op">$</span>ntree &lt;-<span class="st"> </span><span class="kw">as.numeric</span>(<span class="kw">rownames</span>(error.rates))</a>
<a class="sourceLine" id="cb521-8" data-line-number="8">error.rates.melt &lt;-<span class="st"> </span>reshape2<span class="op">::</span><span class="kw">melt</span>(error.rates, <span class="dt">id.vars =</span> <span class="kw">c</span>(<span class="st">&#39;ntree&#39;</span>))</a>
<a class="sourceLine" id="cb521-9" data-line-number="9"><span class="kw">ggplot</span>(error.rates.melt,<span class="kw">aes</span>(<span class="dt">x =</span> ntree,<span class="dt">y =</span> value,<span class="dt">color =</span> variable)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_line</span>()</a></code></pre></div>
<p><img src="06-decision-trees_files/figure-html/ntree%20rf-1.png" width="672" /></p>
<p>500 decision trees or a forest has been built using the Random Forest algorithm based learning. We can plot the error rate across decision trees. The plot seems to indicate that after around 200 decision trees, there is not a significant reduction in error rate.</p>
<div class="sourceCode" id="cb522"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb522-1" data-line-number="1"><span class="co"># Variable Importance Plot</span></a>
<a class="sourceLine" id="cb522-2" data-line-number="2"><span class="kw">varImpPlot</span>(car_rf_direct,</a>
<a class="sourceLine" id="cb522-3" data-line-number="3">           <span class="dt">sort =</span> T,</a>
<a class="sourceLine" id="cb522-4" data-line-number="4">           <span class="dt">main=</span><span class="st">&quot;Variable Importance&quot;</span>)</a></code></pre></div>
<p><img src="06-decision-trees_files/figure-html/varimp%20cars-1.png" width="672" /></p>
<p>Variable importance plot is also a useful tool and can be plotted using varImpPlot function. The variables plotted based on Model Accuracy and Gini value. We can also get a table with decreasing order of importance based on a measure (1 for model accuracy and 2 node impurity)</p>
<div class="sourceCode" id="cb523"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb523-1" data-line-number="1"><span class="co"># Variable Importance Table</span></a>
<a class="sourceLine" id="cb523-2" data-line-number="2"><span class="kw">importance</span>(car_rf_direct, <span class="dt">type =</span> <span class="dv">2</span>)</a></code></pre></div>
<pre><code>##          MeanDecreaseGini
## buying           70.63157
## maint            64.29906
## doors            25.46582
## persons         125.29397
## lug_boot         41.00580
## safety          146.57007</code></pre>
<div class="sourceCode" id="cb525"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb525-1" data-line-number="1"><span class="kw">importance</span>(car_rf_direct, <span class="dt">type =</span> <span class="dv">1</span>)</a></code></pre></div>
<pre><code>##          MeanDecreaseAccuracy
## buying              92.540316
## maint               78.131096
## doors                1.037205
## persons            138.170418
## lug_boot            61.063570
## safety             146.872070</code></pre>
<p>We can train a random forest with caret instead of directly using the randomForest package.</p>
<div class="sourceCode" id="cb527"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb527-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">42</span>)</a>
<a class="sourceLine" id="cb527-2" data-line-number="2">seeds =<span class="st"> </span><span class="kw">vector</span>(<span class="dt">mode=</span><span class="st">&#39;list&#39;</span>,<span class="dt">length=</span><span class="dv">101</span>)</a>
<a class="sourceLine" id="cb527-3" data-line-number="3"><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">100</span>) seeds[[i]] =<span class="st"> </span><span class="kw">sample.int</span>(<span class="dv">1000</span>,<span class="dv">100</span>)</a>
<a class="sourceLine" id="cb527-4" data-line-number="4">seeds[[<span class="dv">101</span>]] =<span class="st"> </span><span class="kw">sample.int</span>(<span class="dv">1000</span>,<span class="dv">1</span>)</a>
<a class="sourceLine" id="cb527-5" data-line-number="5"></a>
<a class="sourceLine" id="cb527-6" data-line-number="6">trctrl &lt;-<span class="st"> </span><span class="kw">trainControl</span>(<span class="dt">method =</span> <span class="st">&quot;repeatedcv&quot;</span>, </a>
<a class="sourceLine" id="cb527-7" data-line-number="7">                       <span class="dt">number =</span> <span class="dv">10</span>, </a>
<a class="sourceLine" id="cb527-8" data-line-number="8">                       <span class="dt">repeats =</span> <span class="dv">10</span>,</a>
<a class="sourceLine" id="cb527-9" data-line-number="9">                       <span class="dt">seeds =</span> seeds)</a>
<a class="sourceLine" id="cb527-10" data-line-number="10"></a>
<a class="sourceLine" id="cb527-11" data-line-number="11">car_rf_fit &lt;-<span class="st"> </span><span class="kw">train</span>(class <span class="op">~</span>., <span class="dt">data =</span> car.training.factors, <span class="dt">method =</span> <span class="st">&quot;rf&quot;</span>,</a>
<a class="sourceLine" id="cb527-12" data-line-number="12">                   <span class="dt">trControl=</span>trctrl,</a>
<a class="sourceLine" id="cb527-13" data-line-number="13">                   <span class="dt">ntree =</span> <span class="dv">10</span>,</a>
<a class="sourceLine" id="cb527-14" data-line-number="14">                   <span class="dt">tuneLength =</span> <span class="dv">10</span>)</a>
<a class="sourceLine" id="cb527-15" data-line-number="15">car_rf_fit</a></code></pre></div>
<pre><code>## Random Forest 
## 
## 1211 samples
##    6 predictor
##    4 classes: &#39;acc&#39;, &#39;good&#39;, &#39;unacc&#39;, &#39;vgood&#39; 
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold, repeated 10 times) 
## Summary of sample sizes: 1090, 1090, 1092, 1089, 1090, 1089, ... 
## Resampling results across tuning parameters:
## 
##   mtry  Accuracy   Kappa    
##    2    0.7606898  0.3281620
##    3    0.8393261  0.6198352
##    4    0.8708673  0.7120605
##    6    0.9000768  0.7812695
##    7    0.9037939  0.7900722
##    9    0.9098089  0.8029032
##   10    0.9147816  0.8137806
##   12    0.9144339  0.8124692
##   13    0.9141305  0.8119262
##   15    0.9137051  0.8107910
## 
## Accuracy was used to select the optimal model using the largest value.
## The final value used for the model was mtry = 10.</code></pre>
<div class="sourceCode" id="cb529"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb529-1" data-line-number="1">car_rf_fit<span class="op">$</span>finalModel</a></code></pre></div>
<pre><code>## 
## Call:
##  randomForest(x = x, y = y, ntree = 10, mtry = param$mtry) 
##                Type of random forest: classification
##                      Number of trees: 10
## No. of variables tried at each split: 10
## 
##         OOB estimate of  error rate: 10.73%
## Confusion matrix:
##       acc good unacc vgood class.error
## acc   214   12    37     4  0.19850187
## good   21   23     1     4  0.53061224
## unacc  28    1   811     0  0.03452381
## vgood  12    9     0    25  0.45652174</code></pre>
<div class="sourceCode" id="cb531"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb531-1" data-line-number="1">rf_test_pred &lt;-<span class="st"> </span><span class="kw">predict</span>(car_rf_fit, <span class="dt">newdata =</span> car.testing.factors)</a>
<a class="sourceLine" id="cb531-2" data-line-number="2"><span class="kw">confusionMatrix</span>(rf_test_pred, car.testing.factors<span class="op">$</span>class )</a></code></pre></div>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction acc good unacc vgood
##      acc    93    2     8     1
##      good    9   14     0     0
##      unacc  12    1   355     0
##      vgood   1    3     0    18
## 
## Overall Statistics
##                                           
##                Accuracy : 0.9284          
##                  95% CI : (0.9027, 0.9491)
##     No Information Rate : 0.7021          
##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16       
##                                           
##                   Kappa : 0.8417          
##                                           
##  Mcnemar&#39;s Test P-Value : NA              
## 
## Statistics by Class:
## 
##                      Class: acc Class: good Class: unacc Class: vgood
## Sensitivity              0.8087     0.70000       0.9780      0.94737
## Specificity              0.9726     0.98189       0.9156      0.99197
## Pos Pred Value           0.8942     0.60870       0.9647      0.81818
## Neg Pred Value           0.9467     0.98785       0.9463      0.99798
## Prevalence               0.2224     0.03868       0.7021      0.03675
## Detection Rate           0.1799     0.02708       0.6867      0.03482
## Detection Prevalence     0.2012     0.04449       0.7118      0.04255
## Balanced Accuracy        0.8907     0.84095       0.9468      0.96967</code></pre>
<p>This performance here is better than decision trees and is much better balanced across the different classes.</p>
<div class="sourceCode" id="cb533"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb533-1" data-line-number="1"><span class="co"># Variable Importance Plot</span></a>
<a class="sourceLine" id="cb533-2" data-line-number="2"><span class="kw">varImpPlot</span>(car_rf_fit<span class="op">$</span>finalModel,</a>
<a class="sourceLine" id="cb533-3" data-line-number="3">           <span class="dt">sort =</span> T,</a>
<a class="sourceLine" id="cb533-4" data-line-number="4">           <span class="dt">main=</span><span class="st">&quot;Variable Importance&quot;</span>)</a></code></pre></div>
<p><img src="06-decision-trees_files/figure-html/varimp%20cars%20caret-1.png" width="672" /></p>
<p>As you can see from the variable importance plot, caret automatically converts categorical variables to dummy variables. This is because it facilitates many types of models, some of which cannot handle categorical variables.</p>
<div id="iris-example-for-random-forests" class="section level3 hasAnchor">
<h3><span class="header-section-number">7.5.1</span> Iris example for Random Forests<a href="decision-trees.html#iris-example-for-random-forests" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We can use the same pipeline for the Iris dataset but we need to remember to scale, centre and check for highly correlation variables and skewness because now we have numeric predictors.</p>
<div class="sourceCode" id="cb534"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb534-1" data-line-number="1"><span class="kw">library</span>(datasets)</a>
<a class="sourceLine" id="cb534-2" data-line-number="2"><span class="kw">data</span>(iris) <span class="co">##loads the dataset, which can be accessed under the variable name iris</span></a>
<a class="sourceLine" id="cb534-3" data-line-number="3"><span class="co">## ?iris opens the documentation for the dataset</span></a>
<a class="sourceLine" id="cb534-4" data-line-number="4"><span class="kw">summary</span>(iris) <span class="co">##presents the 5 figure summary of the dataset</span></a></code></pre></div>
<pre><code>##   Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   
##  Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100  
##  1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300  
##  Median :5.800   Median :3.000   Median :4.350   Median :1.300  
##  Mean   :5.843   Mean   :3.057   Mean   :3.758   Mean   :1.199  
##  3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800  
##  Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500  
##        Species  
##  setosa    :50  
##  versicolor:50  
##  virginica :50  
##                 
##                 
## </code></pre>
<div class="sourceCode" id="cb536"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb536-1" data-line-number="1"><span class="kw">str</span>(iris) <span class="co">##presents the structure of the iris dataframe</span></a></code></pre></div>
<pre><code>## &#39;data.frame&#39;:    150 obs. of  5 variables:
##  $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...
##  $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...
##  $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...
##  $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...
##  $ Species     : Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 1 1 1 1 1 1 1 1 1 ...</code></pre>
<div class="sourceCode" id="cb538"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb538-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">42</span>)</a>
<a class="sourceLine" id="cb538-2" data-line-number="2">trainTestPartition &lt;-<span class="st"> </span><span class="kw">createDataPartition</span>(<span class="dt">y =</span> iris<span class="op">$</span>Species, <span class="co">#the class label, caret ensures an even split of classes</span></a>
<a class="sourceLine" id="cb538-3" data-line-number="3">                                        <span class="dt">p =</span> <span class="fl">0.7</span>, <span class="co">#proportion of samples assigned to train</span></a>
<a class="sourceLine" id="cb538-4" data-line-number="4">                                        <span class="dt">list=</span><span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb538-5" data-line-number="5"><span class="kw">str</span>(trainTestPartition)</a></code></pre></div>
<pre><code>##  int [1:105, 1] 1 2 3 4 5 7 8 10 11 12 ...
##  - attr(*, &quot;dimnames&quot;)=List of 2
##   ..$ : NULL
##   ..$ : chr &quot;Resample1&quot;</code></pre>
<div class="sourceCode" id="cb540"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb540-1" data-line-number="1">iris.training &lt;-<span class="st"> </span>iris[ trainTestPartition,] <span class="co">#take the corresponding rows for training</span></a>
<a class="sourceLine" id="cb540-2" data-line-number="2">iris.testing  &lt;-<span class="st"> </span>iris[<span class="op">-</span>trainTestPartition,] <span class="co">#take the corresponding rows for testing by removing training rows</span></a></code></pre></div>
<div class="sourceCode" id="cb541"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb541-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">42</span>)</a>
<a class="sourceLine" id="cb541-2" data-line-number="2">seeds =<span class="st"> </span><span class="kw">vector</span>(<span class="dt">mode =</span> <span class="st">&#39;list&#39;</span>,<span class="dt">length=</span><span class="dv">101</span>)</a>
<a class="sourceLine" id="cb541-3" data-line-number="3"><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">100</span>) seeds[[i]] =<span class="st"> </span><span class="kw">sample.int</span>(<span class="dv">1000</span>,<span class="dv">10</span>)</a>
<a class="sourceLine" id="cb541-4" data-line-number="4">seeds[[<span class="dv">101</span>]] =<span class="st"> </span><span class="kw">sample.int</span>(<span class="dv">1000</span>,<span class="dv">1</span>)</a>
<a class="sourceLine" id="cb541-5" data-line-number="5"></a>
<a class="sourceLine" id="cb541-6" data-line-number="6">train_ctrl_seed_repeated =<span class="st"> </span><span class="kw">trainControl</span>(<span class="dt">method =</span> <span class="st">&#39;repeatedcv&#39;</span>,</a>
<a class="sourceLine" id="cb541-7" data-line-number="7">                              <span class="dt">number =</span> <span class="dv">10</span>, <span class="co">#number of folds</span></a>
<a class="sourceLine" id="cb541-8" data-line-number="8">                              <span class="dt">repeats =</span> <span class="dv">10</span>, <span class="co">#number of times to repeat cross-validation</span></a>
<a class="sourceLine" id="cb541-9" data-line-number="9">                              <span class="dt">seeds =</span> seeds)</a>
<a class="sourceLine" id="cb541-10" data-line-number="10">iris_rf &lt;-<span class="st"> </span><span class="kw">train</span>(</a>
<a class="sourceLine" id="cb541-11" data-line-number="11">                Species <span class="op">~</span><span class="st"> </span>.,</a>
<a class="sourceLine" id="cb541-12" data-line-number="12">                <span class="dt">data =</span> iris.training,</a>
<a class="sourceLine" id="cb541-13" data-line-number="13">                <span class="dt">method =</span> <span class="st">&quot;rf&quot;</span>,</a>
<a class="sourceLine" id="cb541-14" data-line-number="14">                <span class="dt">preProc =</span> <span class="kw">c</span>(<span class="st">&quot;corr&quot;</span>,<span class="st">&quot;nzv&quot;</span>,<span class="st">&quot;center&quot;</span>, <span class="st">&quot;scale&quot;</span>,<span class="st">&quot;BoxCox&quot;</span>)</a>
<a class="sourceLine" id="cb541-15" data-line-number="15">)</a></code></pre></div>
<pre><code>## Warning in randomForest.default(x, y, mtry = param$mtry, ...): invalid mtry:
## reset to within valid range

## Warning in randomForest.default(x, y, mtry = param$mtry, ...): invalid mtry:
## reset to within valid range

## Warning in randomForest.default(x, y, mtry = param$mtry, ...): invalid mtry:
## reset to within valid range

## Warning in randomForest.default(x, y, mtry = param$mtry, ...): invalid mtry:
## reset to within valid range

## Warning in randomForest.default(x, y, mtry = param$mtry, ...): invalid mtry:
## reset to within valid range

## Warning in randomForest.default(x, y, mtry = param$mtry, ...): invalid mtry:
## reset to within valid range

## Warning in randomForest.default(x, y, mtry = param$mtry, ...): invalid mtry:
## reset to within valid range

## Warning in randomForest.default(x, y, mtry = param$mtry, ...): invalid mtry:
## reset to within valid range

## Warning in randomForest.default(x, y, mtry = param$mtry, ...): invalid mtry:
## reset to within valid range

## Warning in randomForest.default(x, y, mtry = param$mtry, ...): invalid mtry:
## reset to within valid range

## Warning in randomForest.default(x, y, mtry = param$mtry, ...): invalid mtry:
## reset to within valid range

## Warning in randomForest.default(x, y, mtry = param$mtry, ...): invalid mtry:
## reset to within valid range

## Warning in randomForest.default(x, y, mtry = param$mtry, ...): invalid mtry:
## reset to within valid range

## Warning in randomForest.default(x, y, mtry = param$mtry, ...): invalid mtry:
## reset to within valid range

## Warning in randomForest.default(x, y, mtry = param$mtry, ...): invalid mtry:
## reset to within valid range

## Warning in randomForest.default(x, y, mtry = param$mtry, ...): invalid mtry:
## reset to within valid range

## Warning in randomForest.default(x, y, mtry = param$mtry, ...): invalid mtry:
## reset to within valid range

## Warning in randomForest.default(x, y, mtry = param$mtry, ...): invalid mtry:
## reset to within valid range

## Warning in randomForest.default(x, y, mtry = param$mtry, ...): invalid mtry:
## reset to within valid range

## Warning in randomForest.default(x, y, mtry = param$mtry, ...): invalid mtry:
## reset to within valid range

## Warning in randomForest.default(x, y, mtry = param$mtry, ...): invalid mtry:
## reset to within valid range

## Warning in randomForest.default(x, y, mtry = param$mtry, ...): invalid mtry:
## reset to within valid range

## Warning in randomForest.default(x, y, mtry = param$mtry, ...): invalid mtry:
## reset to within valid range

## Warning in randomForest.default(x, y, mtry = param$mtry, ...): invalid mtry:
## reset to within valid range

## Warning in randomForest.default(x, y, mtry = param$mtry, ...): invalid mtry:
## reset to within valid range</code></pre>
<div class="sourceCode" id="cb543"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb543-1" data-line-number="1">iris_rf</a></code></pre></div>
<pre><code>## Random Forest 
## 
## 105 samples
##   4 predictor
##   3 classes: &#39;setosa&#39;, &#39;versicolor&#39;, &#39;virginica&#39; 
## 
## Pre-processing: centered (3), scaled (3), Box-Cox transformation (3), remove (1) 
## Resampling: Bootstrapped (25 reps) 
## Summary of sample sizes: 105, 105, 105, 105, 105, 105, ... 
## Resampling results across tuning parameters:
## 
##   mtry  Accuracy   Kappa    
##   2     0.9571862  0.9351275
##   3     0.9532560  0.9291124
##   4     0.9521131  0.9274814
## 
## Accuracy was used to select the optimal model using the largest value.
## The final value used for the model was mtry = 2.</code></pre>
<div class="sourceCode" id="cb545"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb545-1" data-line-number="1">iris_predict_train =<span class="st"> </span><span class="kw">predict</span>(iris_rf, iris.training,<span class="dt">type =</span> <span class="st">&#39;raw&#39;</span>)</a>
<a class="sourceLine" id="cb545-2" data-line-number="2"><span class="kw">confusionMatrix</span>(iris_predict_train, iris.training<span class="op">$</span>Species)</a></code></pre></div>
<pre><code>## Confusion Matrix and Statistics
## 
##             Reference
## Prediction   setosa versicolor virginica
##   setosa         35          0         0
##   versicolor      0         35         0
##   virginica       0          0        35
## 
## Overall Statistics
##                                      
##                Accuracy : 1          
##                  95% CI : (0.9655, 1)
##     No Information Rate : 0.3333     
##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16  
##                                      
##                   Kappa : 1          
##                                      
##  Mcnemar&#39;s Test P-Value : NA         
## 
## Statistics by Class:
## 
##                      Class: setosa Class: versicolor Class: virginica
## Sensitivity                 1.0000            1.0000           1.0000
## Specificity                 1.0000            1.0000           1.0000
## Pos Pred Value              1.0000            1.0000           1.0000
## Neg Pred Value              1.0000            1.0000           1.0000
## Prevalence                  0.3333            0.3333           0.3333
## Detection Rate              0.3333            0.3333           0.3333
## Detection Prevalence        0.3333            0.3333           0.3333
## Balanced Accuracy           1.0000            1.0000           1.0000</code></pre>
<div class="sourceCode" id="cb547"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb547-1" data-line-number="1">iris_predict_test =<span class="st"> </span><span class="kw">predict</span>(iris_rf, iris.testing,<span class="dt">type =</span> <span class="st">&#39;raw&#39;</span>)</a>
<a class="sourceLine" id="cb547-2" data-line-number="2"><span class="kw">confusionMatrix</span>(iris_predict_test, iris.testing<span class="op">$</span>Species)</a></code></pre></div>
<pre><code>## Confusion Matrix and Statistics
## 
##             Reference
## Prediction   setosa versicolor virginica
##   setosa         15          0         0
##   versicolor      0         14         2
##   virginica       0          1        13
## 
## Overall Statistics
##                                          
##                Accuracy : 0.9333         
##                  95% CI : (0.8173, 0.986)
##     No Information Rate : 0.3333         
##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16      
##                                          
##                   Kappa : 0.9            
##                                          
##  Mcnemar&#39;s Test P-Value : NA             
## 
## Statistics by Class:
## 
##                      Class: setosa Class: versicolor Class: virginica
## Sensitivity                 1.0000            0.9333           0.8667
## Specificity                 1.0000            0.9333           0.9667
## Pos Pred Value              1.0000            0.8750           0.9286
## Neg Pred Value              1.0000            0.9655           0.9355
## Prevalence                  0.3333            0.3333           0.3333
## Detection Rate              0.3333            0.3111           0.2889
## Detection Prevalence        0.3333            0.3556           0.3111
## Balanced Accuracy           1.0000            0.9333           0.9167</code></pre>
<p>In this case, the model predicts the training data perfectly but gives the same confusion matrix on the testing data as for decision trees.</p>
<div class="sourceCode" id="cb549"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb549-1" data-line-number="1"><span class="kw">importance</span>(iris_rf<span class="op">$</span>finalModel)</a></code></pre></div>
<pre><code>##              MeanDecreaseGini
## Sepal.Length        15.162537
## Sepal.Width          4.701492
## Petal.Width         49.264228</code></pre>
<div class="sourceCode" id="cb551"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb551-1" data-line-number="1"><span class="kw">varImpPlot</span>(iris_rf<span class="op">$</span>finalModel)</a></code></pre></div>
<p><img src="06-decision-trees_files/figure-html/varimp%20iris-1.png" width="672" /></p>
</div>
</div>
<div id="cell-segmentation-examples-1" class="section level2 hasAnchor">
<h2><span class="header-section-number">7.6</span> Cell segmentation examples<a href="decision-trees.html#cell-segmentation-examples-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Load required libraries</p>
<div class="sourceCode" id="cb552"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb552-1" data-line-number="1"><span class="kw">library</span>(caret)</a>
<a class="sourceLine" id="cb552-2" data-line-number="2"><span class="kw">library</span>(pROC)</a>
<a class="sourceLine" id="cb552-3" data-line-number="3"><span class="kw">library</span>(e1071)</a></code></pre></div>
<p>Load data</p>
<div class="sourceCode" id="cb553"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb553-1" data-line-number="1"><span class="kw">data</span>(segmentationData)</a>
<a class="sourceLine" id="cb553-2" data-line-number="2">segClass &lt;-<span class="st"> </span>segmentationData<span class="op">$</span>Class</a>
<a class="sourceLine" id="cb553-3" data-line-number="3">segData &lt;-<span class="st"> </span>segmentationData[,<span class="dv">4</span><span class="op">:</span><span class="dv">59</span>] <span class="co">##Extract predictors from segmentationData</span></a></code></pre></div>
<p>Partition data</p>
<div class="sourceCode" id="cb554"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb554-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">42</span>)</a>
<a class="sourceLine" id="cb554-2" data-line-number="2">trainIndex &lt;-<span class="st"> </span><span class="kw">createDataPartition</span>(<span class="dt">y=</span>segClass, <span class="dt">times=</span><span class="dv">1</span>, <span class="dt">p=</span><span class="fl">0.5</span>, <span class="dt">list=</span>F)</a>
<a class="sourceLine" id="cb554-3" data-line-number="3">segDataTrain &lt;-<span class="st"> </span>segData[trainIndex,]</a>
<a class="sourceLine" id="cb554-4" data-line-number="4">segDataTest &lt;-<span class="st"> </span>segData[<span class="op">-</span>trainIndex,]</a>
<a class="sourceLine" id="cb554-5" data-line-number="5">segClassTrain &lt;-<span class="st"> </span>segClass[trainIndex]</a>
<a class="sourceLine" id="cb554-6" data-line-number="6">segClassTest &lt;-<span class="st"> </span>segClass[<span class="op">-</span>trainIndex]</a></code></pre></div>
<p>Set seeds for reproducibility (optional). We will be trying 9 values of the tuning parameter with 5 repeats of 10 fold cross-validation, so we need the following list of seeds.</p>
<div class="sourceCode" id="cb555"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb555-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">42</span>)</a>
<a class="sourceLine" id="cb555-2" data-line-number="2">seeds &lt;-<span class="st"> </span><span class="kw">vector</span>(<span class="dt">mode =</span> <span class="st">&quot;list&quot;</span>, <span class="dt">length =</span> <span class="dv">51</span>)</a>
<a class="sourceLine" id="cb555-3" data-line-number="3"><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">50</span>) seeds[[i]] &lt;-<span class="st"> </span><span class="kw">sample.int</span>(<span class="dv">1000</span>, <span class="dv">9</span>)</a>
<a class="sourceLine" id="cb555-4" data-line-number="4">seeds[[<span class="dv">51</span>]] &lt;-<span class="st"> </span><span class="kw">sample.int</span>(<span class="dv">1000</span>,<span class="dv">1</span>)</a></code></pre></div>
<p>We will pass the twoClassSummary function into model training through <strong>trainControl</strong>. Additionally we would like the model to predict class probabilities so that we can calculate the ROC curve, so we use the <strong>classProbs</strong> option.</p>
<div class="sourceCode" id="cb556"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb556-1" data-line-number="1">cvCtrl &lt;-<span class="st"> </span><span class="kw">trainControl</span>(<span class="dt">method =</span> <span class="st">&quot;repeatedcv&quot;</span>, </a>
<a class="sourceLine" id="cb556-2" data-line-number="2">                       <span class="dt">repeats =</span> <span class="dv">5</span>,</a>
<a class="sourceLine" id="cb556-3" data-line-number="3">                       <span class="dt">number =</span> <span class="dv">10</span>,</a>
<a class="sourceLine" id="cb556-4" data-line-number="4">                       <span class="dt">summaryFunction =</span> twoClassSummary,</a>
<a class="sourceLine" id="cb556-5" data-line-number="5">                       <span class="dt">classProbs =</span> <span class="ot">TRUE</span>,</a>
<a class="sourceLine" id="cb556-6" data-line-number="6">                       <span class="dt">seeds=</span>seeds)</a></code></pre></div>
<div class="sourceCode" id="cb557"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb557-1" data-line-number="1">rfTune &lt;-<span class="st"> </span><span class="kw">train</span>(<span class="dt">x =</span> segDataTrain,</a>
<a class="sourceLine" id="cb557-2" data-line-number="2">                   <span class="dt">y =</span> segClassTrain,</a>
<a class="sourceLine" id="cb557-3" data-line-number="3">                   <span class="dt">method =</span> <span class="st">&#39;rf&#39;</span>,</a>
<a class="sourceLine" id="cb557-4" data-line-number="4">                   <span class="dt">tuneLength =</span> <span class="dv">9</span>,</a>
<a class="sourceLine" id="cb557-5" data-line-number="5">                   <span class="dt">preProc =</span> <span class="kw">c</span>(<span class="st">&quot;center&quot;</span>, <span class="st">&quot;scale&quot;</span>),</a>
<a class="sourceLine" id="cb557-6" data-line-number="6">                   <span class="dt">metric =</span> <span class="st">&quot;ROC&quot;</span>,</a>
<a class="sourceLine" id="cb557-7" data-line-number="7">                   <span class="dt">trControl =</span> cvCtrl)</a>
<a class="sourceLine" id="cb557-8" data-line-number="8"></a>
<a class="sourceLine" id="cb557-9" data-line-number="9">rfTune</a></code></pre></div>
<pre><code>## Random Forest 
## 
## 1010 samples
##   56 predictor
##    2 classes: &#39;PS&#39;, &#39;WS&#39; 
## 
## Pre-processing: centered (56), scaled (56) 
## Resampling: Cross-Validated (10 fold, repeated 5 times) 
## Summary of sample sizes: 909, 909, 909, 909, 909, 909, ... 
## Resampling results across tuning parameters:
## 
##   mtry  ROC        Sens       Spec     
##    2    0.8934786  0.8806154  0.7011111
##    8    0.8935940  0.8658462  0.7322222
##   15    0.8929573  0.8600000  0.7372222
##   22    0.8911923  0.8560000  0.7461111
##   29    0.8883675  0.8526154  0.7405556
##   35    0.8864829  0.8498462  0.7488889
##   42    0.8858932  0.8464615  0.7394444
##   49    0.8843248  0.8498462  0.7416667
##   56    0.8828632  0.8458462  0.7444444
## 
## ROC was used to select the optimal model using the largest value.
## The final value used for the model was mtry = 8.</code></pre>
<div class="sourceCode" id="cb559"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb559-1" data-line-number="1"><span class="kw">plot</span>(rfTune, <span class="dt">metric =</span> <span class="st">&quot;ROC&quot;</span>, <span class="dt">scales =</span> <span class="kw">list</span>(<span class="dt">x =</span> <span class="kw">list</span>(<span class="dt">log =</span><span class="dv">2</span>)))</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:rfAccuracyProfileCellSegment"></span>
<img src="06-decision-trees_files/figure-html/rfAccuracyProfileCellSegment-1.png" alt="rf accuracy profile." width="80%" />
<p class="caption">
Figure 7.7: rf accuracy profile.
</p>
</div>
<p>Test set results</p>
<div class="sourceCode" id="cb560"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb560-1" data-line-number="1"><span class="co">#segDataTest &lt;- predict(transformations, segDataTest)</span></a>
<a class="sourceLine" id="cb560-2" data-line-number="2">rfPred &lt;-<span class="st"> </span><span class="kw">predict</span>(rfTune, segDataTest)</a>
<a class="sourceLine" id="cb560-3" data-line-number="3"><span class="kw">confusionMatrix</span>(rfPred, segClassTest)</a></code></pre></div>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction  PS  WS
##         PS 576  95
##         WS  74 264
##                                         
##                Accuracy : 0.8325        
##                  95% CI : (0.808, 0.855)
##     No Information Rate : 0.6442        
##     P-Value [Acc &gt; NIR] : &lt;2e-16        
##                                         
##                   Kappa : 0.6298        
##                                         
##  Mcnemar&#39;s Test P-Value : 0.1239        
##                                         
##             Sensitivity : 0.8862        
##             Specificity : 0.7354        
##          Pos Pred Value : 0.8584        
##          Neg Pred Value : 0.7811        
##              Prevalence : 0.6442        
##          Detection Rate : 0.5709        
##    Detection Prevalence : 0.6650        
##       Balanced Accuracy : 0.8108        
##                                         
##        &#39;Positive&#39; Class : PS            
## </code></pre>
<p>Get predicted class probabilities</p>
<div class="sourceCode" id="cb562"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb562-1" data-line-number="1">rfProbs &lt;-<span class="st"> </span><span class="kw">predict</span>(rfTune, segDataTest, <span class="dt">type=</span><span class="st">&quot;prob&quot;</span>)</a>
<a class="sourceLine" id="cb562-2" data-line-number="2"><span class="kw">head</span>(rfProbs)</a></code></pre></div>
<pre><code>##       PS    WS
## 1  0.892 0.108
## 5  0.926 0.074
## 8  0.944 0.056
## 9  0.620 0.380
## 10 0.870 0.130
## 12 0.730 0.270</code></pre>
<p>Build a ROC curve</p>
<div class="sourceCode" id="cb564"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb564-1" data-line-number="1">rfROC &lt;-<span class="st"> </span><span class="kw">roc</span>(segClassTest, rfProbs[,<span class="st">&quot;PS&quot;</span>])</a></code></pre></div>
<pre><code>## Setting levels: control = PS, case = WS</code></pre>
<pre><code>## Setting direction: controls &gt; cases</code></pre>
<div class="sourceCode" id="cb567"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb567-1" data-line-number="1"><span class="kw">auc</span>(rfROC)</a></code></pre></div>
<pre><code>## Area under the curve: 0.9023</code></pre>
<p>Plot ROC curve.</p>
<div class="sourceCode" id="cb569"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb569-1" data-line-number="1"><span class="kw">plot</span>(rfROC, <span class="dt">type =</span> <span class="st">&quot;S&quot;</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:rfROCcurveCellSegment"></span>
<img src="06-decision-trees_files/figure-html/rfROCcurveCellSegment-1.png" alt="rf ROC curve for cell segmentation data set." width="80%" />
<p class="caption">
Figure 7.8: rf ROC curve for cell segmentation data set.
</p>
</div>
<p>Calculate area under ROC curve</p>
<div class="sourceCode" id="cb570"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb570-1" data-line-number="1"><span class="kw">auc</span>(rfROC)</a></code></pre></div>
<pre><code>## Area under the curve: 0.9023</code></pre>
</div>
<div id="regression-example---blood-brain-barrier-1" class="section level2 hasAnchor">
<h2><span class="header-section-number">7.7</span> Regression example - Blood Brain Barrier<a href="decision-trees.html#regression-example---blood-brain-barrier-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The pre-processing steps and generation of seeds are identical, therefore if the data were still in memory, we could skip this next block of code:</p>
<div class="sourceCode" id="cb572"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb572-1" data-line-number="1"><span class="kw">data</span>(BloodBrain)</a>
<a class="sourceLine" id="cb572-2" data-line-number="2"></a>
<a class="sourceLine" id="cb572-3" data-line-number="3"><span class="kw">set.seed</span>(<span class="dv">42</span>)</a>
<a class="sourceLine" id="cb572-4" data-line-number="4">trainIndex &lt;-<span class="st"> </span><span class="kw">createDataPartition</span>(<span class="dt">y=</span>logBBB, <span class="dt">times=</span><span class="dv">1</span>, <span class="dt">p=</span><span class="fl">0.8</span>, <span class="dt">list=</span>F)</a>
<a class="sourceLine" id="cb572-5" data-line-number="5">descrTrain &lt;-<span class="st"> </span>bbbDescr[trainIndex,]</a>
<a class="sourceLine" id="cb572-6" data-line-number="6">concRatioTrain &lt;-<span class="st"> </span>logBBB[trainIndex]</a>
<a class="sourceLine" id="cb572-7" data-line-number="7">descrTest &lt;-<span class="st"> </span>bbbDescr[<span class="op">-</span>trainIndex,]</a>
<a class="sourceLine" id="cb572-8" data-line-number="8">concRatioTest &lt;-<span class="st"> </span>logBBB[<span class="op">-</span>trainIndex]</a>
<a class="sourceLine" id="cb572-9" data-line-number="9"></a>
<a class="sourceLine" id="cb572-10" data-line-number="10">transformations &lt;-<span class="st"> </span><span class="kw">preProcess</span>(descrTrain,</a>
<a class="sourceLine" id="cb572-11" data-line-number="11">                              <span class="dt">method=</span><span class="kw">c</span>(<span class="st">&quot;center&quot;</span>, <span class="st">&quot;scale&quot;</span>, <span class="st">&quot;corr&quot;</span>, <span class="st">&quot;nzv&quot;</span>),</a>
<a class="sourceLine" id="cb572-12" data-line-number="12">                              <span class="dt">cutoff=</span><span class="fl">0.75</span>)</a>
<a class="sourceLine" id="cb572-13" data-line-number="13">descrTrain &lt;-<span class="st"> </span><span class="kw">predict</span>(transformations, descrTrain)</a>
<a class="sourceLine" id="cb572-14" data-line-number="14"></a>
<a class="sourceLine" id="cb572-15" data-line-number="15"><span class="kw">set.seed</span>(<span class="dv">42</span>)</a>
<a class="sourceLine" id="cb572-16" data-line-number="16">seeds &lt;-<span class="st"> </span><span class="kw">vector</span>(<span class="dt">mode =</span> <span class="st">&quot;list&quot;</span>, <span class="dt">length =</span> <span class="dv">26</span>)</a>
<a class="sourceLine" id="cb572-17" data-line-number="17"><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">25</span>) seeds[[i]] &lt;-<span class="st"> </span><span class="kw">sample.int</span>(<span class="dv">1000</span>, <span class="dv">50</span>)</a>
<a class="sourceLine" id="cb572-18" data-line-number="18">seeds[[<span class="dv">26</span>]] &lt;-<span class="st"> </span><span class="kw">sample.int</span>(<span class="dv">1000</span>,<span class="dv">1</span>)</a></code></pre></div>
<div id="random-forest-1" class="section level3 hasAnchor">
<h3><span class="header-section-number">7.7.1</span> Random forest<a href="decision-trees.html#random-forest-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="sourceCode" id="cb573"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb573-1" data-line-number="1">rfTune &lt;-<span class="st"> </span><span class="kw">train</span>(descrTrain,</a>
<a class="sourceLine" id="cb573-2" data-line-number="2">                 concRatioTrain,</a>
<a class="sourceLine" id="cb573-3" data-line-number="3">                 <span class="dt">method=</span><span class="st">&#39;rf&#39;</span>,</a>
<a class="sourceLine" id="cb573-4" data-line-number="4">                 <span class="dt">tuneLength =</span> <span class="dv">9</span>,</a>
<a class="sourceLine" id="cb573-5" data-line-number="5">                 <span class="dt">trControl =</span> <span class="kw">trainControl</span>(<span class="dt">method=</span><span class="st">&quot;repeatedcv&quot;</span>,</a>
<a class="sourceLine" id="cb573-6" data-line-number="6">                                          <span class="dt">number =</span> <span class="dv">5</span>,</a>
<a class="sourceLine" id="cb573-7" data-line-number="7">                                          <span class="dt">repeats =</span> <span class="dv">5</span>,</a>
<a class="sourceLine" id="cb573-8" data-line-number="8">                                          <span class="dt">seeds=</span>seeds</a>
<a class="sourceLine" id="cb573-9" data-line-number="9">                                          )</a>
<a class="sourceLine" id="cb573-10" data-line-number="10">)</a>
<a class="sourceLine" id="cb573-11" data-line-number="11"></a>
<a class="sourceLine" id="cb573-12" data-line-number="12">rfTune</a></code></pre></div>
<pre><code>## Random Forest 
## 
## 168 samples
##  63 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold, repeated 5 times) 
## Summary of sample sizes: 136, 134, 134, 134, 134, 134, ... 
## Resampling results across tuning parameters:
## 
##   mtry  RMSE       Rsquared   MAE      
##    2    0.5623974  0.5480514  0.4273841
##    9    0.5371936  0.5705485  0.4097465
##   17    0.5328204  0.5754905  0.4061802
##   24    0.5318392  0.5746695  0.4045764
##   32    0.5342083  0.5684221  0.4064105
##   40    0.5348686  0.5670075  0.4061708
##   47    0.5353506  0.5655544  0.4059083
##   55    0.5376848  0.5604832  0.4083446
##   63    0.5391429  0.5582140  0.4089364
## 
## RMSE was used to select the optimal model using the smallest value.
## The final value used for the model was mtry = 24.</code></pre>
<div class="sourceCode" id="cb575"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb575-1" data-line-number="1"><span class="kw">plot</span>(rfTune)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:rmseCorrf"></span>
<img src="06-decision-trees_files/figure-html/rmseCorrf-1.png" alt="Root Mean Squared Error as a function of cost." width="100%" />
<p class="caption">
Figure 7.9: Root Mean Squared Error as a function of cost.
</p>
</div>
<p>Use model to predict outcomes, after first pre-processing the test set.</p>
<div class="sourceCode" id="cb576"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb576-1" data-line-number="1">descrTest &lt;-<span class="st"> </span><span class="kw">predict</span>(transformations, descrTest)</a>
<a class="sourceLine" id="cb576-2" data-line-number="2">test_pred &lt;-<span class="st"> </span><span class="kw">predict</span>(rfTune, descrTest)</a></code></pre></div>
<p>Prediction performance can be visualized in a scatterplot.</p>
<div class="sourceCode" id="cb577"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb577-1" data-line-number="1"><span class="kw">qplot</span>(concRatioTest, test_pred) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb577-2" data-line-number="2"><span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;observed&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb577-3" data-line-number="3"><span class="st">  </span><span class="kw">ylab</span>(<span class="st">&quot;predicted&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb577-4" data-line-number="4"><span class="st">  </span><span class="kw">theme_bw</span>()</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:obsPredConcRatiosrf"></span>
<img src="06-decision-trees_files/figure-html/obsPredConcRatiosrf-1.png" alt="Concordance between observed concentration ratios and those predicted by random forest." width="80%" />
<p class="caption">
Figure 7.10: Concordance between observed concentration ratios and those predicted by random forest.
</p>
</div>
<p>We can also measure correlation between observed and predicted values.</p>
<div class="sourceCode" id="cb578"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb578-1" data-line-number="1"><span class="kw">cor</span>(concRatioTest, test_pred)</a></code></pre></div>
<pre><code>## [1] 0.7278505</code></pre>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="svm.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="use-case-1.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
